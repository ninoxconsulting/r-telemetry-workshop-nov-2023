[
  {
    "objectID": "04-spatial-data-viz.html",
    "href": "04-spatial-data-viz.html",
    "title": "Visualization of Spatial Data",
    "section": "",
    "text": "Quick recap of plot()\nPretty (and useful) maps with ggplot2\nInteractive maps with mapview/leaflet"
  },
  {
    "objectID": "04-spatial-data-viz.html#outline",
    "href": "04-spatial-data-viz.html#outline",
    "title": "Visualization of Spatial Data",
    "section": "",
    "text": "Quick recap of plot()\nPretty (and useful) maps with ggplot2\nInteractive maps with mapview/leaflet"
  },
  {
    "objectID": "02-intro-spatial-data.html#outline",
    "href": "02-intro-spatial-data.html#outline",
    "title": "Introduction to Spatial Data with R",
    "section": "Outline",
    "text": "Outline\n\nIntroduction to Simple Features and the sf package\nCoordinate Reference Systems\nReading various spatial data formats into R\nBasic operations with spatial data"
  },
  {
    "objectID": "02-intro-spatial-data.html#learning-objectives",
    "href": "02-intro-spatial-data.html#learning-objectives",
    "title": "Introduction to Spatial Data with R",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nThe “simple features” representation of vector data\nUse and understand sf objects in R\nBasic understanding of Coordinate Reference Systems (CRS)\nUse mapview and sf to preview spatial data\nHow to do basic operations with spatial data using sf"
  },
  {
    "objectID": "02-intro-spatial-data.html#vector-simple-features",
    "href": "02-intro-spatial-data.html#vector-simple-features",
    "title": "Introduction to Spatial Data with R",
    "section": "Vector: Simple Features",
    "text": "Vector: Simple Features\n\n\nThe sf R package1\nReplaces\n\nsp\nrgdal\nrgeos\n\n\n2\n\n\n\n\nSimple Features is a standard specification (Open Geospatial Consortium) - agreed-upon way to represent vector spatial data\nrepresent all common vector geometry types : points, lines, polygons and their respective ‘multi’ versions\nsupports geometry collections, which can contain multiple geometry types in a single object.\nsf supersedes the sp ecosystem, which comprises sp , rgdal for data read/write and rgeos for spatial operations.\nrgdal and rgeos are now retired and removed from CRAN\n\n\nsf package: https://cran.r-project.org/package=sfGeocomputation with R, fig 2.2: https://geocompr.robinlovelace.net"
  },
  {
    "objectID": "02-intro-spatial-data.html#reading-previewing-spatial-data",
    "href": "02-intro-spatial-data.html#reading-previewing-spatial-data",
    "title": "Introduction to Spatial Data with R",
    "section": "Reading & previewing spatial data",
    "text": "Reading & previewing spatial data\n\n\n\nlibrary(sf)\nlibrary(mapview)\n\nairports &lt;- read_sf(\n  \"raw_data/bc_airports.gpkg\"\n)\n\nmapview(airports)"
  },
  {
    "objectID": "02-intro-spatial-data.html#mapview",
    "href": "02-intro-spatial-data.html#mapview",
    "title": "Introduction to Spatial Data with R",
    "section": "mapview",
    "text": "mapview\n\nmapview(airports, zcol = \"NUMBER_OF_RUNWAYS\")"
  },
  {
    "objectID": "02-intro-spatial-data.html#structure-of-an-sf-object",
    "href": "02-intro-spatial-data.html#structure-of-an-sf-object",
    "title": "Introduction to Spatial Data with R",
    "section": "Structure of an sf object",
    "text": "Structure of an sf object\n\nairports\n\nSimple feature collection with 455 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 406543.7 ymin: 367957.6 xmax: 1796645 ymax: 1689146\nProjected CRS: NAD83 / BC Albers\n# A tibble: 455 × 6\n   AIRPORT_NAME                   IATA_CODE LOCALITY ELEVATION NUMBER_OF_RUNWAYS\n   &lt;chr&gt;                          &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;             &lt;int&gt;\n 1 Terrace (Northwest Regional) … &lt;NA&gt;      Terrace     217.                   2\n 2 Victoria Harbour (Camel Point… &lt;NA&gt;      Victoria      4.57                 0\n 3 Victoria Inner Harbour Airpor… YWH       Victoria      0                    0\n 4 Victoria Harbour (Shoal Point… &lt;NA&gt;      Victoria      3.05                 0\n 5 Victoria (Royal Jubilee Hospi… &lt;NA&gt;      Saanich      15.6                  0\n 6 Victoria (General Hospital) H… &lt;NA&gt;      View Ro…     15.8                  0\n 7 Victoria (BC Hydro) Heliport   &lt;NA&gt;      Saanich      12.2                  0\n 8 San Juan Point (Coast Guard) … &lt;NA&gt;      Port Re…      7.62                 0\n 9 Shawnigan Lake Water Aerodrome &lt;NA&gt;      Shawnig…      0                    0\n10 Victoria International Airport YYJ       North S…     19.5                  3\n# ℹ 445 more rows\n# ℹ 1 more variable: geom &lt;POINT [m]&gt;\n\n\n\ngo through sf header info: - size (# of features and # of columns/attributes) - geometry type - dimension (XY - can have Z and M) - bbox - CRS"
  },
  {
    "objectID": "02-intro-spatial-data.html#key-features-of-an-sf-object",
    "href": "02-intro-spatial-data.html#key-features-of-an-sf-object",
    "title": "Introduction to Spatial Data with R",
    "section": "Key features of an sf object",
    "text": "Key features of an sf object\n\n\n\nst_geometry(airports)\n\nGeometry set for 455 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 406543.7 ymin: 367957.6 xmax: 1796645 ymax: 1689146\nProjected CRS: NAD83 / BC Albers\nFirst 5 geometries:\n\n\nPOINT (833323.9 1054950)\n\n\nPOINT (1193727 381604.1)\n\n\nPOINT (1194902 382257.7)\n\n\nPOINT (1193719 382179.3)\n\n\nPOINT (1198292 383563.6)\n\nst_bbox(airports)\n\n     xmin      ymin      xmax      ymax \n 406543.7  367957.6 1796645.0 1689145.9 \n\n\n\n\nst_crs(airports)\n\nCoordinate Reference System:\n  User input: NAD83 / BC Albers \n  wkt:\nPROJCRS[\"NAD83 / BC Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"British Columbia Albers\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",45,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-126,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",50,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",58.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Province-wide spatial data management.\"],\n        AREA[\"Canada - British Columbia.\"],\n        BBOX[48.25,-139.04,60.01,-114.08]],\n    ID[\"EPSG\",3005]]"
  },
  {
    "objectID": "02-intro-spatial-data.html#an-sf-object-is-a-data.frame",
    "href": "02-intro-spatial-data.html#an-sf-object-is-a-data.frame",
    "title": "Introduction to Spatial Data with R",
    "section": "An sf object is a data.frame",
    "text": "An sf object is a data.frame\n\nclass(airports)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nis.data.frame(airports)\n\n[1] TRUE\n\nsummary(airports)\n\n AIRPORT_NAME        IATA_CODE           LOCALITY           ELEVATION     \n Length:455         Length:455         Length:455         Min.   :   0.0  \n Class :character   Class :character   Class :character   1st Qu.:   0.0  \n Mode  :character   Mode  :character   Mode  :character   Median :   6.4  \n                                                          Mean   : 194.4  \n                                                          3rd Qu.: 307.1  \n                                                          Max.   :1277.4  \n NUMBER_OF_RUNWAYS            geom    \n Min.   :0.0000    POINT        :455  \n 1st Qu.:0.0000    epsg:3005    :  0  \n Median :0.0000    +proj=aea ...:  0  \n Mean   :0.3385                       \n 3rd Qu.:1.0000                       \n Max.   :3.0000"
  },
  {
    "objectID": "02-intro-spatial-data.html#coordinate-reference-systems",
    "href": "02-intro-spatial-data.html#coordinate-reference-systems",
    "title": "Introduction to Spatial Data with R",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\n\n\nGeographic\n\nspherical or ellipsoidal surface\nellipsoid defined by the datum\nlat/long, measured in angular distances (degrees/radians)\n\n\n\n\n\n\n\nProjected\n\nCartesian coordinates on a flat plane\norigin, x and y axes, linear unit of measurement (e.g., m)\n\n\n\n\n\ndefines how the spatial elements of the data relate to the surface of the Earth\nGeographic: - identify any location on the Earth’s surface using two values — longitude and latitude. - Longitude is location in the East-West direction in angular distance from the Prime Meridian plane. - Latitude is angular distance North or South of the equatorial plane. - Distances in geographic CRSs are therefore not measured in meters. This has important consequences\nProjected: - based on a geographic CRS - rely on map projections to convert the three-dimensional surface of the Earth into Easting and Northing (x and y) values in a projected CRS - often named based on a property they preserve: equal-area preserves area, azimuthal preserve direction, equidistant preserve distance, and conformal preserve local shape. - conic, cylindrical, planar"
  },
  {
    "objectID": "02-intro-spatial-data.html#crss-in-r-epsg-codes",
    "href": "02-intro-spatial-data.html#crss-in-r-epsg-codes",
    "title": "Introduction to Spatial Data with R",
    "section": "CRSs in R: EPSG codes",
    "text": "CRSs in R: EPSG codes\n\nst_crs(airports)$input\n\n[1] \"NAD83 / BC Albers\"\n\n\n\n\n\nBC Albers - B.C.’s standard projection\n\nEqual Area conic\nCentre: c(-126, 54)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://epsg.io/3005\n\n\nEPSG codes BC Albers"
  },
  {
    "objectID": "02-intro-spatial-data.html#wkt-well-known-text",
    "href": "02-intro-spatial-data.html#wkt-well-known-text",
    "title": "Introduction to Spatial Data with R",
    "section": "WKT (Well-Known Text)",
    "text": "WKT (Well-Known Text)\n\nst_crs(3005)\n\nCoordinate Reference System:\n  User input: EPSG:3005 \n  wkt:\nPROJCRS[\"NAD83 / BC Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"British Columbia Albers\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",45,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-126,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",50,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",58.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Province-wide spatial data management.\"],\n        AREA[\"Canada - British Columbia.\"],\n        BBOX[48.25,-139.04,60.01,-114.08]],\n    ID[\"EPSG\",3005]]"
  },
  {
    "objectID": "02-intro-spatial-data.html#your-turn",
    "href": "02-intro-spatial-data.html#your-turn",
    "title": "Introduction to Spatial Data with R",
    "section": "Your turn",
    "text": "Your turn\n\nRead in the electoral districts data in the data folder:\n\nWhat type of geometry is it?\nWhat is the CRS?\nWhat is the EPSG code?"
  },
  {
    "objectID": "02-intro-spatial-data.html#solution",
    "href": "02-intro-spatial-data.html#solution",
    "title": "Introduction to Spatial Data with R",
    "section": "Solution",
    "text": "Solution\n\nelec_bc &lt;- read_sf(\"raw_data/bc_electoral_districts.shp\")\nst_geometry_type(elec_bc, by_geometry = FALSE)\n\n[1] POLYGON\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\nst_crs(elec_bc)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nhttps://epsg.io/4326"
  },
  {
    "objectID": "02-intro-spatial-data.html#basic-plotting",
    "href": "02-intro-spatial-data.html#basic-plotting",
    "title": "Introduction to Spatial Data with R",
    "section": "Basic plotting",
    "text": "Basic plotting\n\nplot(elec_bc)"
  },
  {
    "objectID": "02-intro-spatial-data.html#basic-plotting-1",
    "href": "02-intro-spatial-data.html#basic-plotting-1",
    "title": "Introduction to Spatial Data with R",
    "section": "Basic plotting",
    "text": "Basic plotting\n\n\nJust the shapes\n\nplot(st_geometry(elec_bc))\n\n\n\n\n\n\n\n\n\nA single column\n\nplot(elec_bc[\"ED_NAME\"])\n\n\n\n\n\n\n\n\n\n\n\n\nNotice strange orientation of BC - north is greatly exaggerated\nbecause in WGS84 (lat/long)\nglobal CRS centred at lon 0 (Greenwich) and lat 0 (equator)\ngood for web mapping, not good for BC"
  },
  {
    "objectID": "02-intro-spatial-data.html#transforming-coordinate-systems",
    "href": "02-intro-spatial-data.html#transforming-coordinate-systems",
    "title": "Introduction to Spatial Data with R",
    "section": "Transforming coordinate systems",
    "text": "Transforming coordinate systems\n\nelec_bc_albers &lt;- st_transform(elec_bc, 3005)\n\nOr, if you have another object in the CRS you want to use:\n\nelec_bc_albers &lt;- st_transform(elec_bc, st_crs(airports))\nst_crs(elec_bc_albers)\n\nCoordinate Reference System:\n  User input: NAD83 / BC Albers \n  wkt:\nPROJCRS[\"NAD83 / BC Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"British Columbia Albers\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",45,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-126,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",50,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",58.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Province-wide spatial data management.\"],\n        AREA[\"Canada - British Columbia.\"],\n        BBOX[48.25,-139.04,60.01,-114.08]],\n    ID[\"EPSG\",3005]]"
  },
  {
    "objectID": "02-intro-spatial-data.html#your-turn-1",
    "href": "02-intro-spatial-data.html#your-turn-1",
    "title": "Introduction to Spatial Data with R",
    "section": "Your turn",
    "text": "Your turn\nLoad \"raw_data/ski_resorts.csv\" as an sf object\n\n\n\n\n\nfacility_name\nlocality\nlatitude\nlongitude\nelevation\n\n\n\n\nWapiti Ski Club\nElkford\n50.02168\n-114.9380\n1467\n\n\nSummit Lake Ski Area\nNakusp\n50.14546\n-117.6144\n1132\n\n\nSasquatch Mountain Resort\nHemlock Valley\n49.38011\n-121.9354\n1185\n\n\nApex Mountain Resort\nApex\n49.39042\n-119.9047\n1852\n\n\nSalmo Ski Hill\nSalmo\n49.18640\n-117.3015\n864\n\n\nRed Mountain Resort\nRossland\n49.10238\n-117.8194\n1150"
  },
  {
    "objectID": "02-intro-spatial-data.html#hints",
    "href": "02-intro-spatial-data.html#hints",
    "title": "Introduction to Spatial Data with R",
    "section": "Hints:",
    "text": "Hints:\nLoad \"raw_data/ski_resorts.csv\" as an sf object\n\nski_resorts &lt;- read.csv(\"raw_data/ski_resorts.csv\")\nski_resorts &lt;- st_as_sf(ski_resorts, ...)"
  },
  {
    "objectID": "02-intro-spatial-data.html#solution-1",
    "href": "02-intro-spatial-data.html#solution-1",
    "title": "Introduction to Spatial Data with R",
    "section": "Solution",
    "text": "Solution\n\nski_resorts &lt;- read.csv(\"raw_data/ski_resorts.csv\")\n\nski_resorts &lt;- st_as_sf(ski_resorts,\n                        coords = c(\"longitude\", \"latitude\"),\n                        crs = 4326)\n\nhead(ski_resorts)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -121.9354 ymin: 49.10238 xmax: -114.938 ymax: 50.14546\nGeodetic CRS:  WGS 84\n              facility_name       locality elevation                   geometry\n1           Wapiti Ski Club        Elkford      1467  POINT (-114.938 50.02168)\n2      Summit Lake Ski Area         Nakusp      1132 POINT (-117.6144 50.14546)\n3 Sasquatch Mountain Resort Hemlock Valley      1185 POINT (-121.9354 49.38011)\n4      Apex Mountain Resort           Apex      1852 POINT (-119.9047 49.39042)\n5            Salmo Ski Hill          Salmo       864  POINT (-117.3015 49.1864)\n6       Red Mountain Resort       Rossland      1150 POINT (-117.8194 49.10238)"
  },
  {
    "objectID": "02-intro-spatial-data.html#geometric-calculations",
    "href": "02-intro-spatial-data.html#geometric-calculations",
    "title": "Introduction to Spatial Data with R",
    "section": "Geometric calculations",
    "text": "Geometric calculations\n\n\nGeometric Measurements\n\nst_area()\nst_length()\nst_distance()\n\n\nGeometric Operations\n\nst_union()\nst_intersection()\nst_difference()\nst_sym_difference()"
  },
  {
    "objectID": "02-intro-spatial-data.html#geometry-predicates",
    "href": "02-intro-spatial-data.html#geometry-predicates",
    "title": "Introduction to Spatial Data with R",
    "section": "Geometry Predicates",
    "text": "Geometry Predicates\n\nUse with st_filter() or st_join()\n\n\n\n\nst_intersects(): touch or overlap\nst_disjoint(): !intersects\nst_touches(): touch\nst_crosses(): cross (don’t touch)\nst_within(): within\n\n\n\nst_contains(): contains\nst_overlaps(): overlaps\nst_covers(): cover\nst_covered_by(): covered by\nst_equals(): equals"
  },
  {
    "objectID": "02-intro-spatial-data.html#manipulating-geometries",
    "href": "02-intro-spatial-data.html#manipulating-geometries",
    "title": "Introduction to Spatial Data with R",
    "section": "Manipulating Geometries",
    "text": "Manipulating Geometries\n\n\n\nst_line_merge()\nst_segmentize()\nst_voronoi()\nst_centroid()\nst_convex_hull()\nst_triangulate()\n\n\n\nst_polygonize()\nst_split()\nst_buffer()\nst_make_valid()\nst_boundary()\n…"
  },
  {
    "objectID": "02-intro-spatial-data.html#your-turn-2",
    "href": "02-intro-spatial-data.html#your-turn-2",
    "title": "Introduction to Spatial Data with R",
    "section": "Your turn",
    "text": "Your turn\n\nCalculate the area of each electoral district\nCreate an sf object of only airports within the Nelson-Creston electoral district.\nPlot the ski resorts as circles, where the size of the circle is related to the elevation of the resort.\n\n\nst_area(elec_bc_albers)\nst_filter(airports, elec_bc_albers[elec_bc_albers$ED_NAME == “Nelson-Creston”, ])\nmapview(st_buffer(ski_resorts, dist = ski_resorts$elevation*10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkshop Home"
  },
  {
    "objectID": "bonus-kde.html",
    "href": "bonus-kde.html",
    "title": "Generating Kernel Density Estimates",
    "section": "",
    "text": "In this module we will use caribou data to create home range estimates using kernal density and minimum convex polygons.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n#install.packages(\"ks\")\nlibrary(ks)\nlibrary(mapview)\n\n\nscott &lt;- st_read(\"clean_data/caribou.gpkg\") %&gt;% \n  st_transform(3005)\n\nReading layer `caribou' from data source \n  `/Users/andy/dev/r-telemetry-workshop-nov-2023/clean_data/caribou.gpkg' \n  using driver `GPKG'\nSimple feature collection with 17092 features and 15 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -123.6684 ymin: 55.2249 xmax: -122.081 ymax: 56.01707\nProjected CRS: NAD83 / BC Albers\n\nspt &lt;- scott %&gt;% \n  st_coordinates(scott)\n  \nx &lt;- spt\n\n\n\nst_boundary(scott) \n\nSimple feature collection with 17092 features and 15 fields (with 17092 geometries empty)\nGeometry type: GEOMETRYCOLLECTION\nDimension:     XY\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: NAD83 / BC Albers\nFirst 10 features:\n   location.long location.lat  herd tag.local.identifier animal.id animal.sex\n1      -123.6036     55.90000 Scott               car170 SC_car170          f\n2      -123.5987     55.87343 Scott               car170 SC_car170          f\n3      -123.5903     55.87470 Scott               car170 SC_car170          f\n4      -123.4915     55.83741 Scott               car170 SC_car170          f\n5      -123.4740     55.87877 Scott               car170 SC_car170          f\n6      -123.4412     55.85412 Scott               car170 SC_car170          f\n7      -123.4325     55.91398 Scott               car171 SC_car171          f\n8      -123.4310     55.87820 Scott               car170 SC_car170          f\n9      -123.4280     55.87681 Scott               car170 SC_car170          f\n10     -123.4252     55.87287 Scott               car170 SC_car170          f\n   animal.reproductive.condition tag.manufacturer.name   tag.model\n1                   with calf: N                   ATS GPS Iridium\n2                   with calf: N                   ATS GPS Iridium\n3                   with calf: N                   ATS GPS Iridium\n4                   with calf: N                   ATS GPS Iridium\n5                   with calf: N                   ATS GPS Iridium\n6                   with calf: N                   ATS GPS Iridium\n7                   with calf: Y                   ATS GPS Iridium\n8                   with calf: N                   ATS GPS Iridium\n9                   with calf: N                   ATS GPS Iridium\n10                  with calf: N                   ATS GPS Iridium\n             date_time year month day hour minute                     geom\n1  2013-09-23 03:01:00 2013     9  23   10      1 GEOMETRYCOLLECTION EMPTY\n2  2013-10-09 03:01:00 2013    10   9   10      1 GEOMETRYCOLLECTION EMPTY\n3  2013-10-30 03:01:00 2013    10  30   10      1 GEOMETRYCOLLECTION EMPTY\n4  2014-08-11 03:01:00 2014     8  11   10      1 GEOMETRYCOLLECTION EMPTY\n5  2013-11-09 18:01:00 2013    11  10    2      1 GEOMETRYCOLLECTION EMPTY\n6  2014-08-07 03:01:00 2014     8   7   10      1 GEOMETRYCOLLECTION EMPTY\n7  2014-09-18 19:02:00 2014     9  19    2      2 GEOMETRYCOLLECTION EMPTY\n8  2013-12-06 10:02:00 2013    12   6   18      2 GEOMETRYCOLLECTION EMPTY\n9  2014-07-08 11:01:00 2014     7   8   18      1 GEOMETRYCOLLECTION EMPTY\n10 2013-11-23 18:01:00 2013    11  24    2      1 GEOMETRYCOLLECTION EMPTY\n\n# minimujm convex polygons\n\n\npts1 &lt;- st_as_sf(x = scott, coords = c('location.long', 'location.lat'))\nmy_hull &lt;- st_convex_hull(st_union(pts1))\nplot(my_hull)\nplot(pts1[1], cex=2, col=\"blue\", add = T)\n\n\n\n# \n# \n# set.seed(8192) \n# samp&lt;-200 \n# mus&lt;-rbind(c(-2,2),c(0,0),c(2,-2)) \n# Sigmas&lt;-rbind(diag(2),matrix(c(0.8,-0.72,-0.72,0.8),nrow=2),diag(2)) \n# cwt&lt;-3/11 \n# props&lt;-c((1-cwt)/2,cwt,(1-cwt)/2) \n# x&lt;-rmvnorm.mixt(n=samp,mus=mus,Sigmas=Sigmas,props=props)\n\n\n# Kernal density \n\n# using = h ref \n\nHpi1 &lt;- Hpi(x=x)\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nHpi2 &lt;- Hpi.diag(x = x)\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nfhat.pi1 &lt;-kde(x=x,H=Hpi1) \n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\n\nWarning in quantile.default(dobs, prob = (100 - cont)/100): partial argument\nmatch of 'prob' to 'probs'\n\nfhat.pi2 &lt;-kde(x=x,H=Hpi2)\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\n\nWarning in quantile.default(dobs, prob = (100 - cont)/100): partial argument\nmatch of 'prob' to 'probs'\n\nplot(fhat.pi1)\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\nplot(fhat.pi1)\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'"
  },
  {
    "objectID": "bonus-kde.html#overview",
    "href": "bonus-kde.html#overview",
    "title": "Generating Kernel Density Estimates",
    "section": "",
    "text": "In this module we will use caribou data to create home range estimates using kernal density and minimum convex polygons.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n#install.packages(\"ks\")\nlibrary(ks)\nlibrary(mapview)\n\n\nscott &lt;- st_read(\"clean_data/caribou.gpkg\") %&gt;% \n  st_transform(3005)\n\nReading layer `caribou' from data source \n  `/Users/andy/dev/r-telemetry-workshop-nov-2023/clean_data/caribou.gpkg' \n  using driver `GPKG'\nSimple feature collection with 17092 features and 15 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -123.6684 ymin: 55.2249 xmax: -122.081 ymax: 56.01707\nProjected CRS: NAD83 / BC Albers\n\nspt &lt;- scott %&gt;% \n  st_coordinates(scott)\n  \nx &lt;- spt\n\n\n\nst_boundary(scott) \n\nSimple feature collection with 17092 features and 15 fields (with 17092 geometries empty)\nGeometry type: GEOMETRYCOLLECTION\nDimension:     XY\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: NAD83 / BC Albers\nFirst 10 features:\n   location.long location.lat  herd tag.local.identifier animal.id animal.sex\n1      -123.6036     55.90000 Scott               car170 SC_car170          f\n2      -123.5987     55.87343 Scott               car170 SC_car170          f\n3      -123.5903     55.87470 Scott               car170 SC_car170          f\n4      -123.4915     55.83741 Scott               car170 SC_car170          f\n5      -123.4740     55.87877 Scott               car170 SC_car170          f\n6      -123.4412     55.85412 Scott               car170 SC_car170          f\n7      -123.4325     55.91398 Scott               car171 SC_car171          f\n8      -123.4310     55.87820 Scott               car170 SC_car170          f\n9      -123.4280     55.87681 Scott               car170 SC_car170          f\n10     -123.4252     55.87287 Scott               car170 SC_car170          f\n   animal.reproductive.condition tag.manufacturer.name   tag.model\n1                   with calf: N                   ATS GPS Iridium\n2                   with calf: N                   ATS GPS Iridium\n3                   with calf: N                   ATS GPS Iridium\n4                   with calf: N                   ATS GPS Iridium\n5                   with calf: N                   ATS GPS Iridium\n6                   with calf: N                   ATS GPS Iridium\n7                   with calf: Y                   ATS GPS Iridium\n8                   with calf: N                   ATS GPS Iridium\n9                   with calf: N                   ATS GPS Iridium\n10                  with calf: N                   ATS GPS Iridium\n             date_time year month day hour minute                     geom\n1  2013-09-23 03:01:00 2013     9  23   10      1 GEOMETRYCOLLECTION EMPTY\n2  2013-10-09 03:01:00 2013    10   9   10      1 GEOMETRYCOLLECTION EMPTY\n3  2013-10-30 03:01:00 2013    10  30   10      1 GEOMETRYCOLLECTION EMPTY\n4  2014-08-11 03:01:00 2014     8  11   10      1 GEOMETRYCOLLECTION EMPTY\n5  2013-11-09 18:01:00 2013    11  10    2      1 GEOMETRYCOLLECTION EMPTY\n6  2014-08-07 03:01:00 2014     8   7   10      1 GEOMETRYCOLLECTION EMPTY\n7  2014-09-18 19:02:00 2014     9  19    2      2 GEOMETRYCOLLECTION EMPTY\n8  2013-12-06 10:02:00 2013    12   6   18      2 GEOMETRYCOLLECTION EMPTY\n9  2014-07-08 11:01:00 2014     7   8   18      1 GEOMETRYCOLLECTION EMPTY\n10 2013-11-23 18:01:00 2013    11  24    2      1 GEOMETRYCOLLECTION EMPTY\n\n# minimujm convex polygons\n\n\npts1 &lt;- st_as_sf(x = scott, coords = c('location.long', 'location.lat'))\nmy_hull &lt;- st_convex_hull(st_union(pts1))\nplot(my_hull)\nplot(pts1[1], cex=2, col=\"blue\", add = T)\n\n\n\n# \n# \n# set.seed(8192) \n# samp&lt;-200 \n# mus&lt;-rbind(c(-2,2),c(0,0),c(2,-2)) \n# Sigmas&lt;-rbind(diag(2),matrix(c(0.8,-0.72,-0.72,0.8),nrow=2),diag(2)) \n# cwt&lt;-3/11 \n# props&lt;-c((1-cwt)/2,cwt,(1-cwt)/2) \n# x&lt;-rmvnorm.mixt(n=samp,mus=mus,Sigmas=Sigmas,props=props)\n\n\n# Kernal density \n\n# using = h ref \n\nHpi1 &lt;- Hpi(x=x)\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nHpi2 &lt;- Hpi.diag(x = x)\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nfhat.pi1 &lt;-kde(x=x,H=Hpi1) \n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\n\nWarning in quantile.default(dobs, prob = (100 - cont)/100): partial argument\nmatch of 'prob' to 'probs'\n\nfhat.pi2 &lt;-kde(x=x,H=Hpi2)\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\n\nWarning in quantile.default(dobs, prob = (100 - cont)/100): partial argument\nmatch of 'prob' to 'probs'\n\nplot(fhat.pi1)\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\nplot(fhat.pi1)\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Telemetry Workshop, November 2023",
    "section": "",
    "text": "Welcome!\nWe are happy to be working with you to step through the process to use R to clean, summarise and analyse your telemetry data. This course will be for basic to medium R coders, but never fear, we will have material to follow along with and to take home so dont worry!\nEvent details\n\nWhere: Cedar Boardroom, 2000 South Ospika Blvd, Prince George.\nWhen: November 15th, 16th, 17th 2023\nTimes: The course will be two days (8:30 - 4:30pm, 15th and 16th November)) followed by a half day hackathon (8:30 - 12:30, 17th November). We will have breaks for coffee and lunch. Please bring your own food and snacks.\nWhat to bring: Bring your laptop with installed versions of R and Rstudio. For setup instructions go to setup instructions. Help will be available for the first 30 minutes before the course starts (8:30 - 9:00am) to help trouble shoot any computer problems\n\nWhat is a hackathon? A hackathon is an opportunity to work with your peers on common problems. This might be talking through options, paired coding or splitting up to seperate tasks. We will provide more input during the course to help you best use this time.\nIf you have a dataset you would like to work on, please bring it with you and you can use this for the hacakthon.\nThank-you\nThank-you to Ministry of Forests, Government of British Columbia for sponsering this workshop."
  },
  {
    "objectID": "00-intro-setup.html",
    "href": "00-intro-setup.html",
    "title": "Introduction and System Setup",
    "section": "",
    "text": "This two-day course will introduce you to the basics of working with telemetry data in R.\nWe will providing a data set to use throughout the workshop, but we encourage you to bring along your own data for the hackathon session (Friday AM).\n\n\n\nThe basics of spatial data in R\nHow to read in, clean, and QA your telemetry data\nHow to create useful summaries of your data\nData visualization techniques for spatial and telemetry data (graphs and maps)\nHow to use the bcmaps and bcdata packages to get vector and raster data from official BC government sources.\nHow to perform spatial operations to compile and generate landcape covariates\nHow to extract spatial information for telemetry and random location points\n\n\n\n\n\nMethodological details on technical telemetry parameters i.e. DOP estimates\nAdvanced modeling techniques"
  },
  {
    "objectID": "00-intro-setup.html#overview",
    "href": "00-intro-setup.html#overview",
    "title": "Introduction and System Setup",
    "section": "",
    "text": "This two-day course will introduce you to the basics of working with telemetry data in R.\nWe will providing a data set to use throughout the workshop, but we encourage you to bring along your own data for the hackathon session (Friday AM).\n\n\n\nThe basics of spatial data in R\nHow to read in, clean, and QA your telemetry data\nHow to create useful summaries of your data\nData visualization techniques for spatial and telemetry data (graphs and maps)\nHow to use the bcmaps and bcdata packages to get vector and raster data from official BC government sources.\nHow to perform spatial operations to compile and generate landcape covariates\nHow to extract spatial information for telemetry and random location points\n\n\n\n\n\nMethodological details on technical telemetry parameters i.e. DOP estimates\nAdvanced modeling techniques"
  },
  {
    "objectID": "00-intro-setup.html#computing-requirements",
    "href": "00-intro-setup.html#computing-requirements",
    "title": "Introduction and System Setup",
    "section": "Computing requirements",
    "text": "Computing requirements\nYou will require the following software installed and configured for the workshop. Please have this set up and ready to go before we start.\nYou will need:\n\nA laptop computer, preferably with administrative privileges\nR and RStudio\nSeveral R packages\nQGIS (optional)\n\n\nInstall R and RStudio\nYou will need:\n\nR version &gt;= 4.2.0\nRStudio &gt;= 2023.03.1\n\n\nInstall R\nDownload and install R for your operating system from https://cloud.r-project.org/.\n\nWindows with no admin rights:\nIf you do not have administrator rights, the installer will default to installing somewhere in your user folder (e.g., C:/Users/username/AppData/Local/Programs/). If you prefer, you can change the location to another folder that you have write access to, just make sure it is on your C:/ drive.\n\n\n\nInstall R Studio\nDownload and install RStudio Desktop from https://posit.co/download/rstudio-desktop/. This page should automatically offer you the version suitable for your operating system, but you can scroll down to find versions for all operating systems.\n\nWindows with admin rights:\nDownload the .zip archive for Windows under “Zip/Tarballs”. Create a folder called RStudio in a location on your C:/ drive, where you have write access (e.g. C:/Users/username/AppData/Local/Programs/RStudio), and extract the zip file into this folder. Find the RStudio program in this folder: it is named rstudio.exe, but the file extension will typically be hidden, so look for rstudio. Right-click this file to create a shortcut and drag it to your desktop/task bar. Use the shortcut to open RStudio.\n\n\n\n\nInstall packages\nIn R, install the necessary packages by running:\n\ninstall.packages(\n  c(\"tidyverse\", \"sf\", \"terra\", \"mapview\", \"bcdata\", \"bcmaps\", \"readxl, ggplot2\")\n)\n\n\n\nQGIS (optional)\nQGIS is a free and open-source geographic information system (GIS) that allows you to create, edit, visualize, analyze and publish geospatial information.\nDownload and install from the QGIS website."
  },
  {
    "objectID": "bonus-movebank.html",
    "href": "bonus-movebank.html",
    "title": "Downloading telemetry data from Movebank",
    "section": "",
    "text": "In this module we will introduce the move2 r package and demonstrate how you can download telemetry data directly from the movebank web repository via an API."
  },
  {
    "objectID": "bonus-movebank.html#overview",
    "href": "bonus-movebank.html#overview",
    "title": "Downloading telemetry data from Movebank",
    "section": "",
    "text": "In this module we will introduce the move2 r package and demonstrate how you can download telemetry data directly from the movebank web repository via an API."
  },
  {
    "objectID": "bonus-movebank.html#background",
    "href": "bonus-movebank.html#background",
    "title": "Downloading telemetry data from Movebank",
    "section": "Background",
    "text": "Background\nMovebank is a free, online database of animal movement data hosted by the Max Planck Institute for Ornithology. Data owners can manage their data and have the option to share it with colleagues or the public. If the public or a registered user has permission to see a study, the study can be downloaded as a .csv file and imported directly into R using the move2 package\nNote move2 this is an updated version of the move package\n\n#install.packages(\"move2\")\nlibrary(\"move2\")\n\nlibrary(dplyr, quietly = TRUE)\n\n\nYou will need to create a log in on the movebank platform to be able to download and view data. This can be done at Movebank.\nSet up your credentials within the move2 package. To access any data through R you will also need to supply the credentials you used to create your movebank account to the move2 package. This package uses the keyring package to safely store your credentials. You only need to run the following code once, after which the credentials will be remembered for following R sessions.\n\nNOTE : make sure to keep your credential safe, and do not committ these to a common code platform such as github.\n\nmovebank_store_credentials(\"myUserName\", \"myPassword\")\n\n\n#movebank_store_credentials(\"gcperkins\", \"*****\")\n\n# to delete your credentials run this line\n#movebank_remove_credentials()\n\n# you can check by running this \nkeyring::key_list()"
  },
  {
    "objectID": "bonus-movebank.html#extra-challenge",
    "href": "bonus-movebank.html#extra-challenge",
    "title": "Downloading telemetry data from Movebank",
    "section": "Extra challenge!",
    "text": "Extra challenge!\nIf you are interested in viewing the entire Caribou dataset you can find it on movebank (study_id = 216040785)\n\n# download the study info details\nbou &lt;- movebank_download_study_info(study_id = 216040785)\n\n# download the data \nbou_data &lt;- movebank_download_study(216040785, attributes = NULL)\n\n# download deployment data \nbou_ref_data &lt;- movebank_download_deployment(216040785)\n\nReferences:\n\nMovebank.\nmove2 r package vignette - “Downloading data from movebank”"
  },
  {
    "objectID": "08-rsf-prep.html",
    "href": "08-rsf-prep.html",
    "title": "Generate random sample points for RSF",
    "section": "",
    "text": "In this module we use our prepared spatial data to generate the input datafiles to conduct a resource selection function analysis. This includes:\n\ngenerate a set of “available” or “background points” from our study area.\nextract spatial information for presence and available point locations\nexport as table for future use\nuse the extracted data to provide further graphics and summary options"
  },
  {
    "objectID": "08-rsf-prep.html#overview",
    "href": "08-rsf-prep.html#overview",
    "title": "Generate random sample points for RSF",
    "section": "",
    "text": "In this module we use our prepared spatial data to generate the input datafiles to conduct a resource selection function analysis. This includes:\n\ngenerate a set of “available” or “background points” from our study area.\nextract spatial information for presence and available point locations\nexport as table for future use\nuse the extracted data to provide further graphics and summary options"
  },
  {
    "objectID": "08-rsf-prep.html#background-resource-selection-functions",
    "href": "08-rsf-prep.html#background-resource-selection-functions",
    "title": "Generate random sample points for RSF",
    "section": "Background: Resource Selection Functions",
    "text": "Background: Resource Selection Functions\nNow that we have a cleaned and standardized dataset for the Scott Herd Caribou’s we can prepare the data for further analysis.\nResource Selection Functions are a common method used to assess what is driving patterns of animal habitat preference. This process uses information or covariates (i.e. landscape attribute features) for locations where animals are present and compares them to the same features where animals are absent. In this way we can gather some information on what conditions (i.e. landscape, aspect, distance from road, etc) which contribute to higher habitat use. You can find more references for Resource Selection Functions below.\nWhile in this example we are concentrating on future analysis, the process of extracting background information for known locations can also provide meaningful summary statistics, i.e. proportion of locations within a specific BEC zone."
  },
  {
    "objectID": "08-rsf-prep.html#generate-availalble-location-points.",
    "href": "08-rsf-prep.html#generate-availalble-location-points.",
    "title": "Generate random sample points for RSF",
    "section": "1. Generate availalble location points.",
    "text": "1. Generate availalble location points.\nWe will generate a simple set of “available” based on the geographic distribution of the study area. Note you can limit this area in which you generate “background” points, using more sophisticated methods, such as within a kernal density or home range estimate.\nLets start by reading in the libraries we will use.\n\nlibrary(dplyr)\nlibrary(terra)\nlibrary(sf)\nlibrary(mapview)\nlibrary(ggplot2)\n\nNext, read in the standardized data points for the Scott herd. If needed we can transform to BC Albers projection (EPGS:3005) to match our raster stack prepared in the previous step.\n\n# read in the aoi template \n\ntemplate &lt;- rast(file.path(\"clean_data\",\"template.tif\"))\n\n\n# read in points \n\nbou_pts &lt;- st_read(file.path(\"clean_data/scott_herd_subset.gpkg\")) \nbou_pts &lt;- st_transform(bou_pts, 3005)\n\n\n# Lets keep only the important columns and add a \"presence/absence\" column. \n\nbou_pts &lt;- bou_pts %&gt;% \n  dplyr::select(animal.id, jdate)%&gt;%\n  mutate(pres_abs = 1)\n\nWe can use the spatSample function to generate random points for our given study area. This function has many more options which can be reviewed by using : ?spatSample in the console.\nLets generate a set of points the same length as our “presence” locations using a “random” method.\n\n# Generate random points for RSF use areas.\nset.seed(123)\navail_points &lt;- spatSample(template, size = 2906, as.points = TRUE, na.rm = TRUE, method = \"random\")\n\navail_points &lt;- st_as_sf(avail_points)\n\n# lets rename the column to make it clear these are background points \navail_points &lt;- avail_points %&gt;%\n  rename(\"pres_abs\" = lyr.1 )\n\nWe can do a quick review fo the points to see what they look like using mapview.\n\nmapview(avail_points) +\nmapview(bou_pts, color = \"red\", cex= 3)\n\nWe can now combine our caribou locations and “available” locations into a single dataset. We will retain the spatial information to allow us to easily extract the values in the corresponding raster stack\n\navail_points \n\n\nbou_pts\n\n# note we have slightly different column headers \"geom\" vs \"geometry\" \nst_geometry(avail_points) = \"geom\" \n\n\nallpts &lt;- bind_rows(bou_pts, avail_points ) \n\nNext, we can read in our prepared raster stack as an .rds object. We can now use the extract function from the terra package to extract information for all layers in the raster stack for each of our points.\n\n# read in the raster stack \n\nrstack &lt;- readRDS(file.path(\"clean_data\", \"covars.RDS\"))\n\n\n# extract all values in the raster stack for each location in the bou_pts file. \n\natts &lt;- terra::extract(rstack, allpts)\n\n\n# remove unused columns \n\nbou_full_pts &lt;- cbind( allpts, atts) %&gt;%\n  select(-ID)\n\n\n# export this as spatial file # or \n\n\nbou_table &lt;- bou_full_pts %&gt;%\n  cbind(st_coordinates(.)) %&gt;%\n  st_drop_geometry()\n\n\n# write out as csv \n\nwrite.csv(bou_table, file.path(\"clean_data\", \"allpts_att.csv\"))"
  },
  {
    "objectID": "08-rsf-prep.html#optional",
    "href": "08-rsf-prep.html#optional",
    "title": "Generate random sample points for RSF",
    "section": "OPTIONAL",
    "text": "OPTIONAL"
  },
  {
    "objectID": "08-rsf-prep.html#build-a-mask-with-the-occurance-point-optional",
    "href": "08-rsf-prep.html#build-a-mask-with-the-occurance-point-optional",
    "title": "Generate random sample points for RSF",
    "section": "Build a mask with the “occurance point” (optional)",
    "text": "Build a mask with the “occurance point” (optional)\n\noccur_raster &lt;- rasterize(bou_pts, template, field = \"pres_abs\")\ntemplate_mask &lt;- terra::mask(template, occur_raster, inverse = TRUE)\n\nplot(template_mask)"
  },
  {
    "objectID": "08-rsf-prep.html#add-rsf-refs",
    "href": "08-rsf-prep.html#add-rsf-refs",
    "title": "Generate random sample points for RSF",
    "section": "add RSF refs",
    "text": "add RSF refs"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2023 r-telemetry-workshop-nov-2023 authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "03-intro-telemetry-data.html",
    "href": "03-intro-telemetry-data.html",
    "title": "Introduction to Telemetry Data",
    "section": "",
    "text": "In this course we will be using Telemetry data from Mountain Caribou (Rangifer terendus) herds in the Peace region of British Columbia. While the full dataset and metadata can be found on movebank, we will be working on a modified subset throughout this course. TEST\n\n\nOur first step is to see what our data looks like. In this course we will be providing two csv files (Mountain caribou in British Columbia-reference-data.csv and Mountain caribou.csv).\n\n# Read in our data files.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(readxl)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ggplot2)\n\nref_raw &lt;- read.csv(\"raw_data/Mountain caribou in British Columbia-reference-data.csv\")\n\nloc_raw &lt;- read_excel(\"raw_data/Mountain caribou.xlsx\")\n\nLets take a look….\n\nhead(ref_raw)\n\n   tag.id  animal.id      animal.taxon deploy.on.date deploy.off.date\n1  151.51 HR_151.510 Rangifer tarandus                               \n2    C04a     GR_C04 Rangifer tarandus                        59:00.0\n3     C03     GR_C03 Rangifer tarandus                               \n4 151.805 HR_151.805 Rangifer tarandus                               \n5  151.76 HR_151.760 Rangifer tarandus                               \n6  151.72 HR_151.720 Rangifer tarandus                               \n  animal.death.comments animal.life.stage animal.reproductive.condition\n1                                                                      \n2                                                                      \n3                                                                      \n4                                                                      \n5                                                                      \n6                                                                      \n  animal.sex animal.taxon.detail attachment.type deploy.off.latitude\n1          f             caribou          collar                  NA\n2          f             caribou          collar                  NA\n3          f             caribou          collar                  NA\n4          f             caribou          collar                  NA\n5          f             caribou          collar                  NA\n6          f             caribou          collar                  NA\n  deploy.off.longitude deploy.on.latitude deploy.on.longitude deploy.on.person\n1                   NA                 NA                  NA                 \n2                   NA                 NA                  NA                 \n3                   NA                 NA                  NA                 \n4                   NA                 NA                  NA                 \n5                   NA                 NA                  NA                 \n6                   NA                 NA                  NA                 \n  deployment.comments deployment.end.comments deployment.end.type\n1                                                         unknown\n2                                                         unknown\n3                                                         unknown\n4                                                         unknown\n5                                                         unknown\n6                                                         unknown\n    deployment.id manipulation.type  study.site tag.beacon.frequency\n1 151.510_151.510              none Hart Ranges              151.510\n2        C04_C04a              none      Graham                   NA\n3         C03_C03              none      Graham                   NA\n4 151.805_151.805              none Hart Ranges              151.805\n5 151.760_151.760              none Hart Ranges              151.760\n6 151.720_151.720              none Hart Ranges              151.720\n  tag.manufacturer.name tag.model tag.serial.no\n1                                              \n2                                              \n3                                              \n4                                              \n5                                              \n6                                              \n\nnames(ref_raw)\n\n [1] \"tag.id\"                        \"animal.id\"                    \n [3] \"animal.taxon\"                  \"deploy.on.date\"               \n [5] \"deploy.off.date\"               \"animal.death.comments\"        \n [7] \"animal.life.stage\"             \"animal.reproductive.condition\"\n [9] \"animal.sex\"                    \"animal.taxon.detail\"          \n[11] \"attachment.type\"               \"deploy.off.latitude\"          \n[13] \"deploy.off.longitude\"          \"deploy.on.latitude\"           \n[15] \"deploy.on.longitude\"           \"deploy.on.person\"             \n[17] \"deployment.comments\"           \"deployment.end.comments\"      \n[19] \"deployment.end.type\"           \"deployment.id\"                \n[21] \"manipulation.type\"             \"study.site\"                   \n[23] \"tag.beacon.frequency\"          \"tag.manufacturer.name\"        \n[25] \"tag.model\"                     \"tag.serial.no\"                \n\nref_short &lt;- ref_raw %&gt;%\n  dplyr::select(\"tag.id\",\"animal.id\", \"deploy.on.date\", \"animal.sex\", \"animal.reproductive.condition\",\n                \"deployment.end.type\",\"tag.model\", \"tag.manufacturer.name\", \"tag.serial.no\"    )\n\n\nhead(loc_raw)\n\n# A tibble: 6 × 14\n    event.id timestamp location.long location.lat   DOP FixType     comments\n       &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   \n1 2270202009 01:00.0           -124.         55.9     1 val. GPS-3D Scott   \n2 2270202041 01:00.0           -124.         55.9     1 val. GPS-3D Scott   \n3 2270202100 01:00.0           -124.         55.9     1 val. GPS-3D Scott   \n4 2270202901 01:00.0           -123.         55.8     1 val. GPS-3D Scott   \n5 2270202132 01:00.0           -123.         55.9     1 val. GPS-3D Scott   \n6 2270202890 01:00.0           -123.         55.9     1 val. GPS-3D Scott   \n# ℹ 7 more variables: study.specific.measurement &lt;chr&gt;, sensor.type &lt;chr&gt;,\n#   individual.taxon.canonical.name &lt;chr&gt;, tag.local.identifier &lt;chr&gt;,\n#   individual.local.identifier &lt;chr&gt;, study.name &lt;chr&gt;, date &lt;chr&gt;\n\n\nWe can combine these two dataset and keep only the columns which are of interest\n\nall_data &lt;- left_join(loc_raw, ref_raw, by = c('tag.local.identifier'= 'tag.id') )\n\nall_data &lt;- all_data %&gt;% \n  dplyr::select(event.id, location.long, location.lat, DOP, FixType, comments ,\n                study.specific.measurement , sensor.type, tag.local.identifier, date, animal.id,\n                animal.sex, animal.reproductive.condition, tag.manufacturer.name, tag.model )\n\n#head(all_data)\n\n\n\n\n\n\nNow we have a single data set we can QA the data and provide more useful columns for further analysis.\n\nhead(all_data)\n\n# A tibble: 6 × 15\n    event.id location.long location.lat   DOP FixType     comments\n       &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   \n1 2270202009         -124.         55.9     1 val. GPS-3D Scott   \n2 2270202041         -124.         55.9     1 val. GPS-3D Scott   \n3 2270202100         -124.         55.9     1 val. GPS-3D Scott   \n4 2270202901         -123.         55.8     1 val. GPS-3D Scott   \n5 2270202132         -123.         55.9     1 val. GPS-3D Scott   \n6 2270202890         -123.         55.9     1 val. GPS-3D Scott   \n# ℹ 9 more variables: study.specific.measurement &lt;chr&gt;, sensor.type &lt;chr&gt;,\n#   tag.local.identifier &lt;chr&gt;, date &lt;chr&gt;, animal.id &lt;chr&gt;, animal.sex &lt;chr&gt;,\n#   animal.reproductive.condition &lt;chr&gt;, tag.manufacturer.name &lt;chr&gt;,\n#   tag.model &lt;chr&gt;\n\n# check if there are NA's in the data \n\napply(all_data, 2, function(x) any(is.na(x)))\n\n                     event.id                 location.long \n                        FALSE                          TRUE \n                 location.lat                           DOP \n                         TRUE                         FALSE \n                      FixType                      comments \n                        FALSE                          TRUE \n   study.specific.measurement                   sensor.type \n                         TRUE                         FALSE \n         tag.local.identifier                          date \n                        FALSE                         FALSE \n                    animal.id                    animal.sex \n                        FALSE                         FALSE \nanimal.reproductive.condition         tag.manufacturer.name \n                        FALSE                         FALSE \n                    tag.model \n                        FALSE \n\n# Lets filter out any missing values \n\nlength(all_data$event.id)\n\n[1] 17197\n\ntdata &lt;- all_data %&gt;% \n  filter(!is.na(date)) %&gt;%\n  filter(!is.na(location.long)) %&gt;%\n  filter(!is.na(location.lat)) \n\n\n# comments \nunique(tdata$comments)\n\n[1] \"Scott\"      \"Burnt Pine\" NA          \n\n# two missing herd values which we can fill in (or delete)\n\ntdata &lt;- tdata %&gt;% \n  mutate(comments = case_when(\n    animal.id == \"BP_car043\" ~ \"Burnt Pine\", \n    animal.id == \"SC_car170\" ~ \"Scott\",\n    .default = comments\n  ))\n\n\n\n#length(tdata$event.id)\n\nNow lets covert the timestamp into a usable format\n\n# calculate time differences\ntdata &lt;- tdata  %&gt;%\n  mutate(date_time = ymd_hms(date)) \n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `date_time = ymd_hms(date)`.\nCaused by warning:\n!  5 failed to parse.\n\n# owch we still have an error in this dataset\n\n# lets see if we can find it..... \n\nhead(sort(unique(tdata$date)))\n\n[1] \"2003-12-12 13:03:29.000\" \"2003-12-13 09:03:10.000\"\n[3] \"2003-12-14 05:03:11.000\" \"2003-12-15 01:03:10.000\"\n[5] \"2003-12-15 21:04:00.000\" \"2003-12-16 17:03:10.000\"\n\ntail(sort(unique(tdata$date)))\n\n[1] \"2016-03-16 10:01:00.000\" \"2016-03-16 17:49:00.000\"\n[3] \"2016-03-16 19:49:00.000\" \"2016-03-17 17:53:00.000\"\n[5] \"2016-03-17 19:53:00.000\" \"NA\"                     \n\ntdata &lt;- tdata  %&gt;% \n  filter(date != \"NA\")\n\ntdata  &lt;- tdata  %&gt;%\n  mutate(date_time = ymd_hms(date)) \n\nhead(tdata$date_time) \n\n[1] \"2013-09-23 10:01:00 UTC\" \"2013-10-09 10:01:00 UTC\"\n[3] \"2013-10-30 10:01:00 UTC\" \"2014-08-11 10:01:00 UTC\"\n[5] \"2013-11-10 02:01:00 UTC\" \"2014-08-07 10:01:00 UTC\"\n\n# Note the Universal Coordinated Time Zone\n\n\n# lets split this data format into something more useful \n\ntdata  &lt;- tdata  %&gt;%\n  mutate(year = year(date_time )) %&gt;%\n  mutate(month = month(date_time ),\n         day = day(date_time),\n         hour = hour(date_time),\n         minute = minute(date_time))\n\n\n\n\nNow we have fixed our data entry problems we also want to review the spatial accuracy. We have two metrics: DOP (Dilution of Precision), and a Fix Type.\n\n# review the lat / longs \n\nrange(tdata$location.lat)\n\n[1]  55.2249 155.4764\n\nhist(tdata$location.lat)\n\n\n\n# above 65 latidude\n\nrange(tdata$location.long)\n\n[1] -123.6684  -22.0000\n\nhist(tdata$location.long)\n\n\n\n# greater than -100 longitude. \n\ntdata &lt;- tdata %&gt;% \n  filter(location.long &lt;= -100) %&gt;%\n  filter(location.lat &lt;= 65)\n\n#bou &lt;- st_as_sf(tdata, coords = c(\"location.long\", \"location.lat\"), crs = 4326)\n#mapview::mapview (bou)\n\n# DOP \n\nrange (tdata$DOP)\n\n[1]  1.0 43.1\n\nhist(tdata$DOP)\n\n\n\n# for this example we only want to keep fixes with a DOP less than 10m\n\nfdata &lt;- tdata %&gt;% \n  filter(DOP &lt;= 10)\n\nhist(fdata$DOP)\n\n\n\n# Fix Type : \n\nfixtype &lt;- fdata %&gt;% \n  group_by(FixType) %&gt;%\n  summarise(count = n())\n\nfixtype\n\n# A tibble: 3 × 2\n  FixType     count\n  &lt;chr&gt;       &lt;int&gt;\n1 GPS-2D         92\n2 GPS-3D        291\n3 val. GPS-3D 16801\n\n# remove the 2d locations \n\nfdata &lt;- fdata %&gt;% \n  filter(FixType != \"GPS-2D\")\n\n# see what the data looks like\nglimpse(fdata)\n\nRows: 17,092\nColumns: 21\n$ event.id                      &lt;dbl&gt; 2270202009, 2270202041, 2270202100, 2270…\n$ location.long                 &lt;dbl&gt; -123.6036, -123.5987, -123.5903, -123.49…\n$ location.lat                  &lt;dbl&gt; 55.90000, 55.87343, 55.87470, 55.83741, …\n$ DOP                           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ FixType                       &lt;chr&gt; \"val. GPS-3D\", \"val. GPS-3D\", \"val. GPS-…\n$ comments                      &lt;chr&gt; \"Scott\", \"Scott\", \"Scott\", \"Scott\", \"Sco…\n$ study.specific.measurement    &lt;chr&gt; \"Summer\", \"Summer\", \"Summer\", \"Summer\", …\n$ sensor.type                   &lt;chr&gt; \"gps\", \"gps\", \"gps\", \"gps\", \"gps\", \"gps\"…\n$ tag.local.identifier          &lt;chr&gt; \"car170\", \"car170\", \"car170\", \"car170\", …\n$ date                          &lt;chr&gt; \"2013-09-23 10:01:00.000\", \"2013-10-09 1…\n$ animal.id                     &lt;chr&gt; \"SC_car170\", \"SC_car170\", \"SC_car170\", \"…\n$ animal.sex                    &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", …\n$ animal.reproductive.condition &lt;chr&gt; \"with calf: N\", \"with calf: N\", \"with ca…\n$ tag.manufacturer.name         &lt;chr&gt; \"ATS\", \"ATS\", \"ATS\", \"ATS\", \"ATS\", \"ATS\"…\n$ tag.model                     &lt;chr&gt; \"GPS Iridium\", \"GPS Iridium\", \"GPS Iridi…\n$ date_time                     &lt;dttm&gt; 2013-09-23 10:01:00, 2013-10-09 10:01:0…\n$ year                          &lt;dbl&gt; 2013, 2013, 2013, 2014, 2013, 2014, 2014…\n$ month                         &lt;dbl&gt; 9, 10, 10, 8, 11, 8, 9, 12, 7, 11, 6, 12…\n$ day                           &lt;int&gt; 23, 9, 30, 11, 10, 7, 19, 6, 8, 24, 20, …\n$ hour                          &lt;int&gt; 10, 10, 10, 10, 2, 10, 2, 18, 18, 2, 2, …\n$ minute                        &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 0, 0, 1…\n\n# lets check if this column is any use? \nunique(fdata$sensor.type)\n\n[1] \"gps\"\n\n# remove the columns that we dont need\nfdata &lt;- fdata %&gt;% \n  select(-FixType, -DOP, -date, -study.specific.measurement , -sensor.type, -event.id)"
  },
  {
    "objectID": "03-intro-telemetry-data.html#how-to-qa-and-summarise-your-telemetry-data",
    "href": "03-intro-telemetry-data.html#how-to-qa-and-summarise-your-telemetry-data",
    "title": "Introduction to Telemetry Data",
    "section": "",
    "text": "In this course we will be using Telemetry data from Mountain Caribou (Rangifer terendus) herds in the Peace region of British Columbia. While the full dataset and metadata can be found on movebank, we will be working on a modified subset throughout this course. TEST\n\n\nOur first step is to see what our data looks like. In this course we will be providing two csv files (Mountain caribou in British Columbia-reference-data.csv and Mountain caribou.csv).\n\n# Read in our data files.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\nlibrary(readxl)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ggplot2)\n\nref_raw &lt;- read.csv(\"raw_data/Mountain caribou in British Columbia-reference-data.csv\")\n\nloc_raw &lt;- read_excel(\"raw_data/Mountain caribou.xlsx\")\n\nLets take a look….\n\nhead(ref_raw)\n\n   tag.id  animal.id      animal.taxon deploy.on.date deploy.off.date\n1  151.51 HR_151.510 Rangifer tarandus                               \n2    C04a     GR_C04 Rangifer tarandus                        59:00.0\n3     C03     GR_C03 Rangifer tarandus                               \n4 151.805 HR_151.805 Rangifer tarandus                               \n5  151.76 HR_151.760 Rangifer tarandus                               \n6  151.72 HR_151.720 Rangifer tarandus                               \n  animal.death.comments animal.life.stage animal.reproductive.condition\n1                                                                      \n2                                                                      \n3                                                                      \n4                                                                      \n5                                                                      \n6                                                                      \n  animal.sex animal.taxon.detail attachment.type deploy.off.latitude\n1          f             caribou          collar                  NA\n2          f             caribou          collar                  NA\n3          f             caribou          collar                  NA\n4          f             caribou          collar                  NA\n5          f             caribou          collar                  NA\n6          f             caribou          collar                  NA\n  deploy.off.longitude deploy.on.latitude deploy.on.longitude deploy.on.person\n1                   NA                 NA                  NA                 \n2                   NA                 NA                  NA                 \n3                   NA                 NA                  NA                 \n4                   NA                 NA                  NA                 \n5                   NA                 NA                  NA                 \n6                   NA                 NA                  NA                 \n  deployment.comments deployment.end.comments deployment.end.type\n1                                                         unknown\n2                                                         unknown\n3                                                         unknown\n4                                                         unknown\n5                                                         unknown\n6                                                         unknown\n    deployment.id manipulation.type  study.site tag.beacon.frequency\n1 151.510_151.510              none Hart Ranges              151.510\n2        C04_C04a              none      Graham                   NA\n3         C03_C03              none      Graham                   NA\n4 151.805_151.805              none Hart Ranges              151.805\n5 151.760_151.760              none Hart Ranges              151.760\n6 151.720_151.720              none Hart Ranges              151.720\n  tag.manufacturer.name tag.model tag.serial.no\n1                                              \n2                                              \n3                                              \n4                                              \n5                                              \n6                                              \n\nnames(ref_raw)\n\n [1] \"tag.id\"                        \"animal.id\"                    \n [3] \"animal.taxon\"                  \"deploy.on.date\"               \n [5] \"deploy.off.date\"               \"animal.death.comments\"        \n [7] \"animal.life.stage\"             \"animal.reproductive.condition\"\n [9] \"animal.sex\"                    \"animal.taxon.detail\"          \n[11] \"attachment.type\"               \"deploy.off.latitude\"          \n[13] \"deploy.off.longitude\"          \"deploy.on.latitude\"           \n[15] \"deploy.on.longitude\"           \"deploy.on.person\"             \n[17] \"deployment.comments\"           \"deployment.end.comments\"      \n[19] \"deployment.end.type\"           \"deployment.id\"                \n[21] \"manipulation.type\"             \"study.site\"                   \n[23] \"tag.beacon.frequency\"          \"tag.manufacturer.name\"        \n[25] \"tag.model\"                     \"tag.serial.no\"                \n\nref_short &lt;- ref_raw %&gt;%\n  dplyr::select(\"tag.id\",\"animal.id\", \"deploy.on.date\", \"animal.sex\", \"animal.reproductive.condition\",\n                \"deployment.end.type\",\"tag.model\", \"tag.manufacturer.name\", \"tag.serial.no\"    )\n\n\nhead(loc_raw)\n\n# A tibble: 6 × 14\n    event.id timestamp location.long location.lat   DOP FixType     comments\n       &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   \n1 2270202009 01:00.0           -124.         55.9     1 val. GPS-3D Scott   \n2 2270202041 01:00.0           -124.         55.9     1 val. GPS-3D Scott   \n3 2270202100 01:00.0           -124.         55.9     1 val. GPS-3D Scott   \n4 2270202901 01:00.0           -123.         55.8     1 val. GPS-3D Scott   \n5 2270202132 01:00.0           -123.         55.9     1 val. GPS-3D Scott   \n6 2270202890 01:00.0           -123.         55.9     1 val. GPS-3D Scott   \n# ℹ 7 more variables: study.specific.measurement &lt;chr&gt;, sensor.type &lt;chr&gt;,\n#   individual.taxon.canonical.name &lt;chr&gt;, tag.local.identifier &lt;chr&gt;,\n#   individual.local.identifier &lt;chr&gt;, study.name &lt;chr&gt;, date &lt;chr&gt;\n\n\nWe can combine these two dataset and keep only the columns which are of interest\n\nall_data &lt;- left_join(loc_raw, ref_raw, by = c('tag.local.identifier'= 'tag.id') )\n\nall_data &lt;- all_data %&gt;% \n  dplyr::select(event.id, location.long, location.lat, DOP, FixType, comments ,\n                study.specific.measurement , sensor.type, tag.local.identifier, date, animal.id,\n                animal.sex, animal.reproductive.condition, tag.manufacturer.name, tag.model )\n\n#head(all_data)\n\n\n\n\n\n\nNow we have a single data set we can QA the data and provide more useful columns for further analysis.\n\nhead(all_data)\n\n# A tibble: 6 × 15\n    event.id location.long location.lat   DOP FixType     comments\n       &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   \n1 2270202009         -124.         55.9     1 val. GPS-3D Scott   \n2 2270202041         -124.         55.9     1 val. GPS-3D Scott   \n3 2270202100         -124.         55.9     1 val. GPS-3D Scott   \n4 2270202901         -123.         55.8     1 val. GPS-3D Scott   \n5 2270202132         -123.         55.9     1 val. GPS-3D Scott   \n6 2270202890         -123.         55.9     1 val. GPS-3D Scott   \n# ℹ 9 more variables: study.specific.measurement &lt;chr&gt;, sensor.type &lt;chr&gt;,\n#   tag.local.identifier &lt;chr&gt;, date &lt;chr&gt;, animal.id &lt;chr&gt;, animal.sex &lt;chr&gt;,\n#   animal.reproductive.condition &lt;chr&gt;, tag.manufacturer.name &lt;chr&gt;,\n#   tag.model &lt;chr&gt;\n\n# check if there are NA's in the data \n\napply(all_data, 2, function(x) any(is.na(x)))\n\n                     event.id                 location.long \n                        FALSE                          TRUE \n                 location.lat                           DOP \n                         TRUE                         FALSE \n                      FixType                      comments \n                        FALSE                          TRUE \n   study.specific.measurement                   sensor.type \n                         TRUE                         FALSE \n         tag.local.identifier                          date \n                        FALSE                         FALSE \n                    animal.id                    animal.sex \n                        FALSE                         FALSE \nanimal.reproductive.condition         tag.manufacturer.name \n                        FALSE                         FALSE \n                    tag.model \n                        FALSE \n\n# Lets filter out any missing values \n\nlength(all_data$event.id)\n\n[1] 17197\n\ntdata &lt;- all_data %&gt;% \n  filter(!is.na(date)) %&gt;%\n  filter(!is.na(location.long)) %&gt;%\n  filter(!is.na(location.lat)) \n\n\n# comments \nunique(tdata$comments)\n\n[1] \"Scott\"      \"Burnt Pine\" NA          \n\n# two missing herd values which we can fill in (or delete)\n\ntdata &lt;- tdata %&gt;% \n  mutate(comments = case_when(\n    animal.id == \"BP_car043\" ~ \"Burnt Pine\", \n    animal.id == \"SC_car170\" ~ \"Scott\",\n    .default = comments\n  ))\n\n\n\n#length(tdata$event.id)\n\nNow lets covert the timestamp into a usable format\n\n# calculate time differences\ntdata &lt;- tdata  %&gt;%\n  mutate(date_time = ymd_hms(date)) \n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `date_time = ymd_hms(date)`.\nCaused by warning:\n!  5 failed to parse.\n\n# owch we still have an error in this dataset\n\n# lets see if we can find it..... \n\nhead(sort(unique(tdata$date)))\n\n[1] \"2003-12-12 13:03:29.000\" \"2003-12-13 09:03:10.000\"\n[3] \"2003-12-14 05:03:11.000\" \"2003-12-15 01:03:10.000\"\n[5] \"2003-12-15 21:04:00.000\" \"2003-12-16 17:03:10.000\"\n\ntail(sort(unique(tdata$date)))\n\n[1] \"2016-03-16 10:01:00.000\" \"2016-03-16 17:49:00.000\"\n[3] \"2016-03-16 19:49:00.000\" \"2016-03-17 17:53:00.000\"\n[5] \"2016-03-17 19:53:00.000\" \"NA\"                     \n\ntdata &lt;- tdata  %&gt;% \n  filter(date != \"NA\")\n\ntdata  &lt;- tdata  %&gt;%\n  mutate(date_time = ymd_hms(date)) \n\nhead(tdata$date_time) \n\n[1] \"2013-09-23 10:01:00 UTC\" \"2013-10-09 10:01:00 UTC\"\n[3] \"2013-10-30 10:01:00 UTC\" \"2014-08-11 10:01:00 UTC\"\n[5] \"2013-11-10 02:01:00 UTC\" \"2014-08-07 10:01:00 UTC\"\n\n# Note the Universal Coordinated Time Zone\n\n\n# lets split this data format into something more useful \n\ntdata  &lt;- tdata  %&gt;%\n  mutate(year = year(date_time )) %&gt;%\n  mutate(month = month(date_time ),\n         day = day(date_time),\n         hour = hour(date_time),\n         minute = minute(date_time))\n\n\n\n\nNow we have fixed our data entry problems we also want to review the spatial accuracy. We have two metrics: DOP (Dilution of Precision), and a Fix Type.\n\n# review the lat / longs \n\nrange(tdata$location.lat)\n\n[1]  55.2249 155.4764\n\nhist(tdata$location.lat)\n\n\n\n# above 65 latidude\n\nrange(tdata$location.long)\n\n[1] -123.6684  -22.0000\n\nhist(tdata$location.long)\n\n\n\n# greater than -100 longitude. \n\ntdata &lt;- tdata %&gt;% \n  filter(location.long &lt;= -100) %&gt;%\n  filter(location.lat &lt;= 65)\n\n#bou &lt;- st_as_sf(tdata, coords = c(\"location.long\", \"location.lat\"), crs = 4326)\n#mapview::mapview (bou)\n\n# DOP \n\nrange (tdata$DOP)\n\n[1]  1.0 43.1\n\nhist(tdata$DOP)\n\n\n\n# for this example we only want to keep fixes with a DOP less than 10m\n\nfdata &lt;- tdata %&gt;% \n  filter(DOP &lt;= 10)\n\nhist(fdata$DOP)\n\n\n\n# Fix Type : \n\nfixtype &lt;- fdata %&gt;% \n  group_by(FixType) %&gt;%\n  summarise(count = n())\n\nfixtype\n\n# A tibble: 3 × 2\n  FixType     count\n  &lt;chr&gt;       &lt;int&gt;\n1 GPS-2D         92\n2 GPS-3D        291\n3 val. GPS-3D 16801\n\n# remove the 2d locations \n\nfdata &lt;- fdata %&gt;% \n  filter(FixType != \"GPS-2D\")\n\n# see what the data looks like\nglimpse(fdata)\n\nRows: 17,092\nColumns: 21\n$ event.id                      &lt;dbl&gt; 2270202009, 2270202041, 2270202100, 2270…\n$ location.long                 &lt;dbl&gt; -123.6036, -123.5987, -123.5903, -123.49…\n$ location.lat                  &lt;dbl&gt; 55.90000, 55.87343, 55.87470, 55.83741, …\n$ DOP                           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ FixType                       &lt;chr&gt; \"val. GPS-3D\", \"val. GPS-3D\", \"val. GPS-…\n$ comments                      &lt;chr&gt; \"Scott\", \"Scott\", \"Scott\", \"Scott\", \"Sco…\n$ study.specific.measurement    &lt;chr&gt; \"Summer\", \"Summer\", \"Summer\", \"Summer\", …\n$ sensor.type                   &lt;chr&gt; \"gps\", \"gps\", \"gps\", \"gps\", \"gps\", \"gps\"…\n$ tag.local.identifier          &lt;chr&gt; \"car170\", \"car170\", \"car170\", \"car170\", …\n$ date                          &lt;chr&gt; \"2013-09-23 10:01:00.000\", \"2013-10-09 1…\n$ animal.id                     &lt;chr&gt; \"SC_car170\", \"SC_car170\", \"SC_car170\", \"…\n$ animal.sex                    &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", …\n$ animal.reproductive.condition &lt;chr&gt; \"with calf: N\", \"with calf: N\", \"with ca…\n$ tag.manufacturer.name         &lt;chr&gt; \"ATS\", \"ATS\", \"ATS\", \"ATS\", \"ATS\", \"ATS\"…\n$ tag.model                     &lt;chr&gt; \"GPS Iridium\", \"GPS Iridium\", \"GPS Iridi…\n$ date_time                     &lt;dttm&gt; 2013-09-23 10:01:00, 2013-10-09 10:01:0…\n$ year                          &lt;dbl&gt; 2013, 2013, 2013, 2014, 2013, 2014, 2014…\n$ month                         &lt;dbl&gt; 9, 10, 10, 8, 11, 8, 9, 12, 7, 11, 6, 12…\n$ day                           &lt;int&gt; 23, 9, 30, 11, 10, 7, 19, 6, 8, 24, 20, …\n$ hour                          &lt;int&gt; 10, 10, 10, 10, 2, 10, 2, 18, 18, 2, 2, …\n$ minute                        &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 0, 0, 1…\n\n# lets check if this column is any use? \nunique(fdata$sensor.type)\n\n[1] \"gps\"\n\n# remove the columns that we dont need\nfdata &lt;- fdata %&gt;% \n  select(-FixType, -DOP, -date, -study.specific.measurement , -sensor.type, -event.id)"
  },
  {
    "objectID": "03-intro-telemetry-data.html#conver-to-spatial-file-and-export",
    "href": "03-intro-telemetry-data.html#conver-to-spatial-file-and-export",
    "title": "Introduction to Telemetry Data",
    "section": "conver to spatial file and export",
    "text": "conver to spatial file and export\n\n# conver to a sf object \n\nbou &lt;- st_as_sf(fdata, coords = c(\"location.long\", \"location.lat\"), crs = 4326)\n\n#mapview::mapview (bou)\n\n\n# export as .gpkg\nst_write(bou, \"clean_data/caribou.gpkg\")\n\nst_write(bou, \"clean_data/caribou.shp\", append=FALSE)\n\n# note warning on names for shapefile"
  },
  {
    "objectID": "03-intro-telemetry-data.html#output-cleaned",
    "href": "03-intro-telemetry-data.html#output-cleaned",
    "title": "Introduction to Telemetry Data",
    "section": "Output cleaned",
    "text": "Output cleaned\nWe can output out cleaned data as a table\n\nwrite.csv(fdata, \"clean_data/caribou.csv\", row.names = F)"
  },
  {
    "objectID": "03-intro-telemetry-data.html#generating-tabular-summaries",
    "href": "03-intro-telemetry-data.html#generating-tabular-summaries",
    "title": "Introduction to Telemetry Data",
    "section": "Generating tabular summaries",
    "text": "Generating tabular summaries\nNow we have clean data to work with we can get to the fun data exploration part!\n\n#bou = read.csv(\"clean_data/caribou.csv\")\nbou_sf = st_read(\"clean_data/caribou.gpkg\")\n\nReading layer `caribou' from data source \n  `/Users/andy/dev/r-telemetry-workshop-nov-2023/clean_data/caribou.gpkg' \n  using driver `GPKG'\nSimple feature collection with 17092 features and 13 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -123.6684 ymin: 55.2249 xmax: -122.081 ymax: 56.01707\nGeodetic CRS:  WGS 84\n\nbou &lt;- bou_sf %&gt;%\n  cbind(st_coordinates(bou_sf)) %&gt;%\n          st_drop_geometry(bou_sf)\n\n\nhead(bou)\n\n  comments tag.local.identifier animal.id animal.sex\n1    Scott               car170 SC_car170          f\n2    Scott               car170 SC_car170          f\n3    Scott               car170 SC_car170          f\n4    Scott               car170 SC_car170          f\n5    Scott               car170 SC_car170          f\n6    Scott               car170 SC_car170          f\n  animal.reproductive.condition tag.manufacturer.name   tag.model\n1                  with calf: N                   ATS GPS Iridium\n2                  with calf: N                   ATS GPS Iridium\n3                  with calf: N                   ATS GPS Iridium\n4                  with calf: N                   ATS GPS Iridium\n5                  with calf: N                   ATS GPS Iridium\n6                  with calf: N                   ATS GPS Iridium\n            date_time year month day hour minute         X        Y\n1 2013-09-23 03:01:00 2013     9  23   10      1 -123.6036 55.90000\n2 2013-10-09 03:01:00 2013    10   9   10      1 -123.5987 55.87343\n3 2013-10-30 03:01:00 2013    10  30   10      1 -123.5903 55.87470\n4 2014-08-11 03:01:00 2014     8  11   10      1 -123.4915 55.83741\n5 2013-11-09 18:01:00 2013    11  10    2      1 -123.4740 55.87877\n6 2014-08-07 03:01:00 2014     8   7   10      1 -123.4412 55.85412\n\nglimpse(bou)\n\nRows: 17,092\nColumns: 15\n$ comments                      &lt;chr&gt; \"Scott\", \"Scott\", \"Scott\", \"Scott\", \"Sco…\n$ tag.local.identifier          &lt;chr&gt; \"car170\", \"car170\", \"car170\", \"car170\", …\n$ animal.id                     &lt;chr&gt; \"SC_car170\", \"SC_car170\", \"SC_car170\", \"…\n$ animal.sex                    &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", …\n$ animal.reproductive.condition &lt;chr&gt; \"with calf: N\", \"with calf: N\", \"with ca…\n$ tag.manufacturer.name         &lt;chr&gt; \"ATS\", \"ATS\", \"ATS\", \"ATS\", \"ATS\", \"ATS\"…\n$ tag.model                     &lt;chr&gt; \"GPS Iridium\", \"GPS Iridium\", \"GPS Iridi…\n$ date_time                     &lt;dttm&gt; 2013-09-23 03:01:00, 2013-10-09 03:01:0…\n$ year                          &lt;dbl&gt; 2013, 2013, 2013, 2014, 2013, 2014, 2014…\n$ month                         &lt;dbl&gt; 9, 10, 10, 8, 11, 8, 9, 12, 7, 11, 6, 12…\n$ day                           &lt;int&gt; 23, 9, 30, 11, 10, 7, 19, 6, 8, 24, 20, …\n$ hour                          &lt;int&gt; 10, 10, 10, 10, 2, 10, 2, 18, 18, 2, 2, …\n$ minute                        &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 0, 0, 1…\n$ X                             &lt;dbl&gt; -123.6036, -123.5987, -123.5903, -123.49…\n$ Y                             &lt;dbl&gt; 55.90000, 55.87343, 55.87470, 55.83741, …\n\n\nMany Questions we can ask here:\n\nhow many herds do we have?\nhow many animals in each herd?\nwhat is the sex ratio of collared animals?\nwhat is the duration of each collar? Start and end years?\n\n\nNo_herds = unique(bou$comments)\n\nno_records &lt;- bou %&gt;% \n  group_by(comments)%&gt;% \n  summarise(count = n())\n\n\nno_animals_sex &lt;- bou %&gt;% \n  group_by(comments, animal.sex)%&gt;% \n  summarise(count = n())\n\n`summarise()` has grouped output by 'comments'. You can override using the\n`.groups` argument.\n\nno_animals_id &lt;- bou %&gt;% \n  group_by(comments, animal.id)%&gt;% \n  summarise(count = n())\n\n`summarise()` has grouped output by 'comments'. You can override using the\n`.groups` argument.\n\n\nLets concentrate on the Scott herd.\n\n# look at the Scott herd.\n\nsbou &lt;- bou %&gt;% \n  filter(comments == \"Scott\")\n\n# how many animals?\nno_animals &lt;- unique(sbou$animal.id)\n\n\n# lets look at the time period: \n\np1 &lt;- ggplot(sbou, aes(year, fill = animal.id))+\n    geom_bar(position = \"dodge\")#+\n    #xlim(2021,2024)#+\n\n\n# duration of the collars within the Scott herd. \n\ntable_max &lt;- sbou %&gt;% \n  dplyr::select(animal.id, date_time) %&gt;%\n  slice_max(date_time, by = animal.id) \ncolnames(table_max)&lt;- c(\"animal.id\",\"max\")\n\ntable_min &lt;- sbou %&gt;% \n  dplyr::select(animal.id, date_time) %&gt;%\n  slice_min(date_time, by = animal.id) \ncolnames(table_min)&lt;- c(\"animal.id\",\"min\")\n\ndur &lt;- left_join(table_max, table_min, by = join_by(animal.id)) %&gt;%\n  distinct() %&gt;%\n  dplyr::mutate(duration = max - min) %&gt;%\n  mutate(dur_days = round( duration,1))%&gt;%\n  mutate(dur_hrs = round(as.numeric(dur_days)*24,1)) %&gt;%\n #mutate(dur_days = round( dur_hrs/24,1))%&gt;%\n  mutate(year_start = year(min), \n         year_end = year(max))\n\n\n\ndur_plot &lt;- ggplot(dur, aes(y=factor(animal.id))) +\n  geom_segment(aes(x=min, xend=max, y=factor(animal.id), yend=factor(animal.id)), size=1)+\n  xlab(\"Date\") + ylab(\"Tag\") \n\ndur_plot\n\n\n\n# months of the year. \np_duration &lt;- ggplot(sbou, aes(factor(month), fill = factor(year)))+\n  geom_bar(position = \"dodge\") +\n  #xlim(1,12)+\n  facet_wrap(~animal.id)+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n# months of the year. \np_duration &lt;- ggplot(sbou, aes(factor(year), fill = factor(month)))+\n  geom_bar(position = \"dodge\") +\n  #xlim(1,12)+\n  facet_wrap(~animal.id)+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))"
  },
  {
    "objectID": "06-raster-data.html",
    "href": "06-raster-data.html",
    "title": "Exploring Raster Data",
    "section": "",
    "text": "In this module we will explore raster data, and the available datasets which can be used in R for analysis. We will:\n\nprepare our study aoi to use with rasters\nextract DEM data through the bcmaps package\ngenerate DEM derived covariates,\nprepare a raster stack with all spatial layers generated to date."
  },
  {
    "objectID": "06-raster-data.html#overview",
    "href": "06-raster-data.html#overview",
    "title": "Exploring Raster Data",
    "section": "",
    "text": "In this module we will explore raster data, and the available datasets which can be used in R for analysis. We will:\n\nprepare our study aoi to use with rasters\nextract DEM data through the bcmaps package\ngenerate DEM derived covariates,\nprepare a raster stack with all spatial layers generated to date."
  },
  {
    "objectID": "06-raster-data.html#background-rasters",
    "href": "06-raster-data.html#background-rasters",
    "title": "Exploring Raster Data",
    "section": "Background: Rasters",
    "text": "Background: Rasters\nRaster data is information built on a standard grid. These can be characterised by the grid extent (xmin, xmax, ymin, ymax) and can have a coordinate system to orient them in space. Rasters are made up of cells or pixels, based on a resolution or cell size. Each cell contains a single value. The terra package contains many functions for manipulating and processing rasters.\nNote we will be using the BC Albers coordinate reference system (EPSG:3005). The advantages of this are 1) BCAlbers is an equal area projection across BC, 2) extent is marked using meters."
  },
  {
    "objectID": "06-raster-data.html#create-a-standard-raster-template.",
    "href": "06-raster-data.html#create-a-standard-raster-template.",
    "title": "Exploring Raster Data",
    "section": "1. Create a standard raster template.",
    "text": "1. Create a standard raster template.\nFor ease of calculations we will create a standard “template” raster which will form the basis for all our data analysis.\n\n# read in the libraries needed\n\nlibrary(dplyr)\nlibrary(sf)\nlibrary(bcmaps)\nlibrary(terra)\n\nRead in our AOI that we made and saved previously. We can then convert this to a raster with a resolution of given size. In this example we will select 25m.\n\n# \n# # read in the spatial file \n# \n# bou &lt;- st_read(file.path(\"clean_data/scott_herd_subset.gpkg\"), crs = 4326)\n# \n# # check this is not only output as a csv \n\n\n# read in the spatial file \n\n\n# read in the aoi previously created\naoi &lt;- st_read(file.path(\"clean_data\", \"scott_aoi.gpkg\"))\n\nWe can now convert our polygon to a raster object.\n\n# create a template raster with a resolution of 25m\ntemplate &lt;- terra::rast(aoi , resolution = 25)\n\n# create a column name\nnames(template) = \"aoi\"\n\n# assign all values to zero \nterra::values(template) &lt;- 0\n\n# export the raster for later processing\nterra::writeRaster(template, file.path(\"clean_data\", \"template.tif\"), overwrite = TRUE)"
  },
  {
    "objectID": "06-raster-data.html#extract-base-data-using-the-cded-dataset",
    "href": "06-raster-data.html#extract-base-data-using-the-cded-dataset",
    "title": "Exploring Raster Data",
    "section": "2. Extract base data using the CDED dataset",
    "text": "2. Extract base data using the CDED dataset\nNow we can use the bcmaps package to directly download digital elevation data from the Canadian Digital Elevation Model CDED. Within BC, this is laregly equivalent to the TRIM DEM dataset. We will use the cded_terra function.\n\n# extract DEM raw data - Note this is downloaded in tiles which will be cached to avoid the need to redownload repeatedly \n\ntrim_raw &lt;- bcmaps::cded_terra(aoi)\n\n# lets look at the raster\ntrim_raw\nres(trim_raw)\n\n# note this is in WGS so we need to convert to 3005\n# reproject to match our raster template crs and extent\n\ntrim_3005 &lt;- project(trim_raw, template)\n\n# write out the individual raster\nterra::writeRaster(trim_3005, file.path(\"clean_data\", \"dem.tif\"), overwrite = TRUE)"
  },
  {
    "objectID": "06-raster-data.html#generate-dem-derived-covariates",
    "href": "06-raster-data.html#generate-dem-derived-covariates",
    "title": "Exploring Raster Data",
    "section": "3. Generate DEM derived covariates",
    "text": "3. Generate DEM derived covariates\nNow we can use the terrain functions within the terra package to generate some standard base layers derived from the DEM.\n\n# generate slope \nrslope &lt;- terra::terrain(trim_3005, v = \"slope\", neighbors = 8, unit = \"degrees\") \n\n# generate aspect\naspect &lt;- terra::terrain(trim_3005, v = \"aspect\", neighbors = 8,  unit = \"degrees\") \n\n# generate topographic roughness index\ntri &lt;- terra::terrain(trim_3005, v = \"TRI\", neighbors = 8)\n\n# create a raster stack\nrstack &lt;- c(trim_3005, rslope, aspect, tri)\n\nplot(rstack)\n\n\n# write our the individual rasters\n\n#terra::writeRaster(rslope, file.path(out_path, \"slope.tif\"), overwrite = TRUE) \n#terra::writeRaster(aspect , file.path(out_path, \"aspect.tif\"), overwrite = TRUE) \n#terra::writeRaster(tri , file.path(out_path, \"tri.tif\"), overwrite = TRUE)"
  },
  {
    "objectID": "06-raster-data.html#bonus-questions.",
    "href": "06-raster-data.html#bonus-questions.",
    "title": "Exploring Raster Data",
    "section": "Bonus Questions.",
    "text": "Bonus Questions.\n\nExplore what other covariates you can generate using the terra::terrain function. Hint use ?terrain to bring up a help file.\nCompare outputs of aspect which the neighbours parameter is adjusted. What difference does this make to the output and the time to process?"
  },
  {
    "objectID": "05-vector-bcdata.html",
    "href": "05-vector-bcdata.html",
    "title": "Getting B.C. Open Data with R",
    "section": "",
    "text": "The B.C. Government makes a huge amount of open data available, both spatial and non-spatial, and documents it in the B.C. Data Catalogue.\nThe {bcdata} package allows you to interact with the catalogue, and download data, directly from within R.\nWe are going to use {bcdata} to get some data for our telemetry analysis:\nlibrary(bcdata)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(mapview)"
  },
  {
    "objectID": "05-vector-bcdata.html#create-an-area-of-interest-aoi",
    "href": "05-vector-bcdata.html#create-an-area-of-interest-aoi",
    "title": "Getting B.C. Open Data with R",
    "section": "Create an Area of Interest (AOI)",
    "text": "Create an Area of Interest (AOI)\nWe’ll read in the caribou data created in the previous module, and create an area of interest (a bounding box) around it, so we can use that to spatially subset the covariate data we will be using.\n\nscott &lt;- read_sf(\"clean_data/caribou.gpkg\") |&gt; \n  filter(comments == \"Scott\") |&gt; \n  st_transform(3005)\n\nscott_bbox &lt;- st_bbox(scott)\n\n## Round to nearest 100m outside the box to align with raster grids\nscott_bbox[\"xmin\"] &lt;- floor(scott_bbox[\"xmin\"] / 100) * 100\nscott_bbox[\"ymin\"] &lt;- floor(scott_bbox[\"ymin\"] / 100) * 100\nscott_bbox[\"xmax\"] &lt;- ceiling(scott_bbox[\"xmax\"] / 100) * 100\nscott_bbox[\"ymax\"] &lt;- ceiling(scott_bbox[\"ymax\"] / 100) * 100\n\nscott_aoi &lt;- st_as_sfc(scott_bbox)\n\n\nwrite_sf(scott_aoi, file.path(\"clean_data\", \"scott_aoi.gpkg\"))\n\nFirst, let’s open the B.C. Data Catalogue in our browser:\n\nbcdc_browse()\n\nIf you search for “BEC”, one of the first hits should be the BEC Map record. If you click on the “Share” (  ) button you will get a url: https://catalogue.data.gov.bc.ca/dataset/f358a53b-ffde-4830-a325-a5a03ff672c3. The last bit of the url (f358a53b-ffde-4830-a325-a5a03ff672c3) is the unique identifier for the record, and we can use that ID to query the dataset with {bcdata}.\nLet’s find out about the record:\n\nbcdc_get_record(\"f358a53b-ffde-4830-a325-a5a03ff672c3\")\n\nB.C. Data Catalogue Record: BEC Map\nName: bec-map (ID: f358a53b-ffde-4830-a325-a5a03ff672c3)\nPermalink:\n https://catalogue.data.gov.bc.ca/dataset/f358a53b-ffde-4830-a325-a5a03ff672c3\nLicence: Open Government Licence - British Columbia\nDescription: The current and most detailed version of the approved\n corporate provincial digital Biogeoclimatic Ecosystem Classification\n (BEC) Zone/Subzone/Variant/Phase map (version 12, September 2, 2021).\n Use this version when performing GIS analysis regardless of scale.\n This mapping is deliberately extended across the ocean, lakes,\n glaciers, etc to facilitate intersection with a terrestrial landcover\n layer of your choice\nAvailable Resources (1):\n 1. WMS getCapabilities request (wms)\nAccess the full 'Resources' data frame using:\n bcdc_tidy_resources('f358a53b-ffde-4830-a325-a5a03ff672c3')\nQuery and filter this data using:\n bcdc_query_geodata('f358a53b-ffde-4830-a325-a5a03ff672c3')"
  },
  {
    "objectID": "05-vector-bcdata.html#bec",
    "href": "05-vector-bcdata.html#bec",
    "title": "Getting B.C. Open Data with R",
    "section": "BEC",
    "text": "BEC\nbcdc_query_geodata() by itself does not download the data - it retrieves the first few records and shows them to us, as well as some helpful information about the data:\n\nbcdc_query_geodata(\"f358a53b-ffde-4830-a325-a5a03ff672c3\")\n\nQuerying 'bec-map' record\n• Using collect() on this object will return 15666 features and 20 fields\n• Accessing this record requires pagination and will make 16 separate\n• requests to the WFS. See ?bcdc_options\n• At most six rows of the record are printed here\n────────────────────────────────────────────────────────────────────────────────\nSimple feature collection with 6 features and 20 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 575312.5 ymin: 1553588 xmax: 780612.5 ymax: 1668388\nProjected CRS: NAD83 / BC Albers\n# A tibble: 6 × 21\n  id          FEATURE_CLASS_SKEY ZONE  SUBZONE VARIANT PHASE NATURAL_DISTURBANCE\n  &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;              \n1 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n2 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n3 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n4 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n5 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n6 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n# ℹ 14 more variables: MAP_LABEL &lt;chr&gt;, BGC_LABEL &lt;chr&gt;, ZONE_NAME &lt;chr&gt;,\n#   SUBZONE_NAME &lt;chr&gt;, VARIANT_NAME &lt;chr&gt;, PHASE_NAME &lt;chr&gt;,\n#   NATURAL_DISTURBANCE_NAME &lt;chr&gt;, FEATURE_AREA_SQM &lt;int&gt;,\n#   FEATURE_LENGTH_M &lt;int&gt;, FEATURE_AREA &lt;int&gt;, FEATURE_LENGTH &lt;int&gt;,\n#   OBJECTID &lt;int&gt;, SE_ANNO_CAD_DATA &lt;chr&gt;, geometry &lt;POLYGON [m]&gt;\n\n\nYou can use dplyr verbs filter() and select() to cut down the amount of data you need to download from the web service. filter() can take logical predicates such as ==, &gt;, %in% etc., as well as geometry predicates such as INTERSECTS(), OVERLAPS(), WITHIN() etc. See this vignette for details.\nOnce your query is complete you call collect() to tell the server to execute the query and send you the data:\n\nbec &lt;- bcdc_query_geodata(\"f358a53b-ffde-4830-a325-a5a03ff672c3\") |&gt;\n    filter(INTERSECTS(scott_aoi)) |&gt; \n    select(MAP_LABEL) |&gt; \n    collect()\n\nmapview(bec, zcol = \"MAP_LABEL\") + mapview(st_as_sf(scott_aoi))\n\n\n\n\n\n\nYou can see that filtering using the INTERSECTS() function does a good job of only downloading the features that intersect the AOI, but it doesn’t actually clip them to the AOI. We can do that with sf::st_intersection(). Also, there are some columns that are “sticky” - even if you don’t select them in a select() statement before you call collect, they come along anyway. We can do a final select() once the data is downloaded:\n\nbec &lt;- st_intersection(bec, scott_aoi) |&gt; \n  select(MAP_LABEL)\n\nmapview(bec)\n\n\n\n\n\n\nAnd now we can write our BEC data to a file to use later in the analysis.\n\nwrite_sf(bec, file.path(\"clean_data\", \"bec.gpkg\"))"
  },
  {
    "objectID": "05-vector-bcdata.html#vri",
    "href": "05-vector-bcdata.html#vri",
    "title": "Getting B.C. Open Data with R",
    "section": "VRI",
    "text": "VRI\nSee a list of VRI codes.\n\nvri &lt;- bcdc_query_geodata(\"2ebb35d8-c82f-4a17-9c96-612ac3532d55\") |&gt; \n  filter(INTERSECTS(scott_aoi)) |&gt; \n  select(PROJ_AGE_CLASS_CD_1, BCLCS_LEVEL_4, CROWN_CLOSURE_CLASS_CD) |&gt;  \n  collect() |&gt; \n  st_intersection(scott_aoi)\n\nWe want to split this into separate files for different variables:\n\nConiferous-leading stands\nStands with age greater than 40 yrs (age class &gt;=3)\nCrown closure\n\n\n# Tree coniferous leading - select coniferous leading vri plots\nvri_conif &lt;- vri |&gt;  \n    mutate(conif = BCLCS_LEVEL_4) |&gt; \n    filter(conif == \"TC\") |&gt; \n    select(conif)\n\n\nwrite_sf(vri_conif, file.path(\"clean_data\",\"vri_conif.gpkg\"))\n\n\n# Age class greater than 40 years\nvri_ageclass &lt;- vri |&gt; \n    mutate(age_class = as.numeric(PROJ_AGE_CLASS_CD_1)) |&gt; \n    filter(age_class &gt;= 3) |&gt; \n    select(age_class)\n\n\nwrite_sf(vri_ageclass, file.path(\"clean_data\", \"vri_ageclass.gpkg\"))\n\n\n# Crown closure class \nvri_cc &lt;- vri |&gt; \n    mutate(cc_class = as.numeric(CROWN_CLOSURE_CLASS_CD)) |&gt; \n    select(cc_class)\n\n\nwrite_sf(vri_cc, file.path(\"clean_data\", \"vri_cc.gpkg\"))"
  },
  {
    "objectID": "05-vector-bcdata.html#cutblocks",
    "href": "05-vector-bcdata.html#cutblocks",
    "title": "Getting B.C. Open Data with R",
    "section": "Cutblocks",
    "text": "Cutblocks\nTo get the cutblocks, we filter to our AOI, and also choose those blocks that have a harvest year in the last 30 years.\nWe can get information about the columns in a given dataset with bcdc_describe_feature():\n\nbcdc_describe_feature(\"b1b647a6-f271-42e0-9cd0-89ec24bce9f7\")\n\n# A tibble: 13 × 5\n   col_name                sticky remote_col_type local_col_type column_comments\n   &lt;chr&gt;                   &lt;lgl&gt;  &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;          \n 1 id                      TRUE   xsd:string      character      &lt;NA&gt;           \n 2 VEG_CONSOLIDATED_CUT_B… TRUE   xsd:decimal     numeric        VEG_CONSOLIDAT…\n 3 OPENING_ID              FALSE  xsd:decimal     numeric        OPENING_ID is …\n 4 HARVEST_YEAR            FALSE  xsd:decimal     numeric        HARVEST_YEAR i…\n 5 DISTURBANCE_START_DATE  FALSE  xsd:date        date           DISTURBANCE_ST…\n 6 DISTURBANCE_END_DATE    FALSE  xsd:date        date           DISTURBANCE_EN…\n 7 DATA_SOURCE             FALSE  xsd:string      character      DATA_SOURCE is…\n 8 AREA_HA                 FALSE  xsd:decimal     numeric        AREA_HA is the…\n 9 FEATURE_AREA_SQM        FALSE  xsd:decimal     numeric        FEATURE_AREA_S…\n10 FEATURE_LENGTH_M        FALSE  xsd:decimal     numeric        FEATURE_LENGTH…\n11 SHAPE                   FALSE  gml:GeometryPr… sfc geometry   SHAPE is the c…\n12 OBJECTID                TRUE   xsd:decimal     numeric        OBJECTID is a …\n13 SE_ANNO_CAD_DATA        FALSE  xsd:hexBinary   numeric        SE_ANNO_CAD_DA…\n\ncutblocks &lt;- bcdc_query_geodata(\"b1b647a6-f271-42e0-9cd0-89ec24bce9f7\") |&gt;\n  filter(\n    INTERSECTS(scott_aoi), \n    HARVEST_YEAR &gt;= 1993) |&gt;\n  select(HARVEST_YEAR) |&gt;\n  collect() |&gt; \n  st_intersection(scott_aoi)\n\nmapview(cutblocks, zcol = \"HARVEST_YEAR\")\n\n\n\n\n\n\n\nwrite_sf(cutblocks, file.path(\"clean_data\", \"cutblocks.gpkg\"))"
  },
  {
    "objectID": "05-vector-bcdata.html#water-bodies",
    "href": "05-vector-bcdata.html#water-bodies",
    "title": "Getting B.C. Open Data with R",
    "section": "Water bodies",
    "text": "Water bodies\n\nSearching the catalogue for the BC Freshwater Atlas\nWe can search the catalogue for data using keywords, with bcdc_search(). Control the number of results returned with n.\n\nbcdc_search(\"freshwater atlas\", n = 20)\n\nList of B.C. Data Catalogue Records\nNumber of records: 20\nTitles:\n1: Freshwater Atlas Lakes (multiple, html, pdf, wms, kml)\n ID: cb1e3aba-d3fe-4de1-a2d4-b8b6650fb1f6\n Name: freshwater-atlas-lakes\n2: Freshwater Atlas Rivers (multiple, fgdb, pdf, wms, kml)\n ID: f7dac054-efbf-402f-ab62-6fc4b32a619e\n Name: freshwater-atlas-rivers\n3: Freshwater Atlas Obstructions (multiple, html, pdf, wms, kml)\n ID: 64797286-3ca5-4202-9064-a7f790321e9e\n Name: freshwater-atlas-obstructions\n4: Freshwater Atlas Watersheds (multiple, html, pdf, wms, kml, fgdb)\n ID: 3ee497c4-57d7-47f8-b030-2e0c03f8462a\n Name: freshwater-atlas-watersheds\n5: Freshwater Atlas Coastlines (multiple, html, pdf, wms, kml)\n ID: 87b1d6a7-d4d1-4c25-a879-233becdbffed\n Name: freshwater-atlas-coastlines\n6: Freshwater Atlas Glaciers (multiple, html, pdf, wms, kml)\n ID: 8f2aee65-9f4c-4f72-b54c-0937dbf3e6f7\n Name: freshwater-atlas-glaciers\n7: Freshwater Atlas Wetlands (multiple, html, pdf, wms, kml)\n ID: 93b413d8-1840-4770-9629-641d74bd1cc6\n Name: freshwater-atlas-wetlands\n8: Freshwater Atlas Islands (multiple, html, pdf, wms, kml)\n ID: 4483aeea-df26-4b83-a565-934c769e74de\n Name: freshwater-atlas-islands\n9: Freshwater Atlas Watershed Boundaries (multiple, html, pdf, wms,\n kml, fgdb)\n ID: ab758580-809d-4e11-bb2c-df02ac5465c9\n Name: freshwater-atlas-watershed-boundaries\n10: Freshwater Atlas Linear Boundaries (multiple, html, pdf, wms, kml,\n fgdb)\n ID: 2af1388e-d5f7-46dc-a6e2-f85415ddbd1c\n Name: freshwater-atlas-linear-boundaries\n11: Freshwater Atlas Watershed Groups (multiple, html, pdf, wms, kml)\n ID: 51f20b1a-ab75-42de-809d-bf415a0f9c62\n Name: freshwater-atlas-watershed-groups\n12: Freshwater Atlas Manmade Waterbodies (multiple, html, pdf, wms,\n kml)\n ID: 055fd71e-b771-4d47-a863-8a54f91a954c\n Name: freshwater-atlas-manmade-waterbodies\n13: Freshwater Atlas Named Watersheds (multiple, html, pdf, wms, kml)\n ID: ea63ea04-eab0-4b83-8729-f8a93ac688a1\n Name: freshwater-atlas-named-watersheds\n14: Freshwater Atlas Assessment Watersheds (multiple, html, pdf, wms,\n kml)\n ID: 97d8ef37-b8d2-4c3b-b772-6b25c1db13d0\n Name: freshwater-atlas-assessment-watersheds\n15: Freshwater Atlas Stream Directions (multiple, html, pdf, wms, kml)\n ID: d7165359-52ef-41d0-b762-c53e3468ff3f\n Name: freshwater-atlas-stream-directions\n16: Freshwater Atlas Stream Network (multiple, html, pdf, wms, kml,\n fgdb)\n ID: 92344413-8035-4c08-b996-65a9b3f62fca\n Name: freshwater-atlas-stream-network\n17: Freshwater Atlas Edge Type Codes (multiple, html, pdf)\n ID: 509cbf74-7ee7-44d3-a88d-4e088ea67325\n Name: freshwater-atlas-edge-type-codes\n18: Freshwater Atlas Waterbody Type Codes (multiple, html, pdf)\n ID: ade4f36a-1fd4-4583-8253-2b2a1bbe34ff\n Name: freshwater-atlas-waterbody-type-codes\n19: Freshwater Atlas Watershed Type Codes (multiple, html, pdf)\n ID: f7efa3ea-bf1c-4c4f-bb33-ba841aa076c0\n Name: freshwater-atlas-watershed-type-codes\n20: Freshwater Atlas Named Point Features (multiple, html, pdf, wms,\n kml)\n ID: db43a358-273c-4c2e-8a5c-cc28eaaffaa7\n Name: freshwater-atlas-named-point-features\n\nAccess a single record by calling `bcdc_get_record(ID)` with the ID\n from the desired record.\n\n\n\nlakes &lt;- bcdc_query_geodata(\"cb1e3aba-d3fe-4de1-a2d4-b8b6650fb1f6\") |&gt;\n  filter(INTERSECTS(scott_aoi)) |&gt;\n  select(id, WATERBODY_TYPE, AREA_HA) |&gt;\n  collect()\n\nwetlands &lt;- bcdc_query_geodata(\"93b413d8-1840-4770-9629-641d74bd1cc6\") |&gt;\n  filter(INTERSECTS(scott_aoi)) |&gt;\n  select(id, WATERBODY_TYPE, AREA_HA) |&gt;\n  collect()\n\n# Combine the datasets into one, select only the columns we want, and \n# clip to aoi\nwater &lt;- bind_rows(lakes, wetlands) |&gt; \n  select(id, WATERBODY_TYPE, AREA_HA) |&gt; \n  st_intersection(scott_aoi)\n\n\nwrite_sf(wetlands, file.path(\"clean_data\", \"water.gpkg\"))"
  },
  {
    "objectID": "05-vector-bcdata.html#stream-index",
    "href": "05-vector-bcdata.html#stream-index",
    "title": "Getting B.C. Open Data with R",
    "section": "Stream Index",
    "text": "Stream Index\n\nstreams_cols &lt;- bcdc_describe_feature(\"92344413-8035-4c08-b996-65a9b3f62fca\")\nprint(streams_cols, n = 20)\n\n# A tibble: 28 × 5\n   col_name                sticky remote_col_type local_col_type column_comments\n   &lt;chr&gt;                   &lt;lgl&gt;  &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;          \n 1 id                      TRUE   xsd:string      character       &lt;NA&gt;          \n 2 LINEAR_FEATURE_ID       TRUE   xsd:decimal     numeric        \"A unique nume…\n 3 WATERSHED_GROUP_ID      TRUE   xsd:decimal     numeric        \"An automatica…\n 4 EDGE_TYPE               TRUE   xsd:decimal     numeric        \"A 4 digit num…\n 5 BLUE_LINE_KEY           FALSE  xsd:decimal     numeric        \"Uniquely iden…\n 6 WATERSHED_KEY           FALSE  xsd:decimal     numeric        \"A key that id…\n 7 FWA_WATERSHED_CODE      FALSE  xsd:string      character      \"A 143 charact…\n 8 LOCAL_WATERSHED_CODE    FALSE  xsd:string      character      \"The 143 chara…\n 9 WATERSHED_GROUP_CODE    TRUE   xsd:string      character      \"The watershed…\n10 DOWNSTREAM_ROUTE_MEASU… FALSE  xsd:decimal     numeric        \"The distance,…\n11 LENGTH_METRE            TRUE   xsd:decimal     numeric        \"The length in…\n12 FEATURE_SOURCE          TRUE   xsd:string      character      \"The source of…\n13 GNIS_ID                 FALSE  xsd:decimal     numeric        \"The BCGNIS  (…\n14 GNIS_NAME               FALSE  xsd:string      character      \"The BCGNIS  (…\n15 LEFT_RIGHT_TRIBUTARY    FALSE  xsd:string      character      \"Describes whi…\n16 STREAM_ORDER            TRUE   xsd:decimal     numeric        \"The calculate…\n17 STREAM_MAGNITUDE        TRUE   xsd:decimal     numeric        \"The calculate…\n18 WATERBODY_KEY           FALSE  xsd:decimal     numeric        \"The waterbody…\n19 BLUE_LINE_KEY_50K       FALSE  xsd:decimal     numeric        \"The best matc…\n20 WATERSHED_CODE_50K      FALSE  xsd:string      character      \"The hierarchi…\n# ℹ 8 more rows\n\n\nLet’s use the STREAM_ORDER column to just get streams of order 3 and 4 and still intersect with our AOI.\n\nstreams &lt;- bcdc_query_geodata(\"92344413-8035-4c08-b996-65a9b3f62fca\") |&gt;\n  filter(\n    INTERSECTS(scott_aoi), \n    STREAM_ORDER &gt;= 3\n  ) |&gt;\n  select(id, STREAM_ORDER) |&gt;\n  collect() |&gt; \n  select(id, STREAM_ORDER) |&gt;\n  st_zm() |&gt; \n  st_intersection(scott_aoi)\n\nmapview(streams)\n\n\n\n\n\n\n\nwrite_sf(streams, file.path(\"clean_data\", \"streams.gpkg\"))"
  },
  {
    "objectID": "05-vector-bcdata.html#roads",
    "href": "05-vector-bcdata.html#roads",
    "title": "Getting B.C. Open Data with R",
    "section": "Roads",
    "text": "Roads\n\nroads &lt;- bcdc_query_geodata(\"bb060417-b6e6-4548-b837-f9060d94743e\") |&gt; \n  filter(INTERSECTS(scott_aoi))  |&gt; \n  select(id, ROAD_CLASS, ROAD_SURFACE) |&gt; \n  collect() |&gt; \n  select(ROAD_SURFACE, ROAD_CLASS) |&gt; \n  st_intersection(scott_aoi) |&gt; # clip roads so all inside aoi\n  st_cast(\"MULTILINESTRING\")\n\n\nwrite_sf(roads, file.path(\"clean_data\", \"roads.gpkg\"))"
  },
  {
    "objectID": "07-processing-vector-data.html",
    "href": "07-processing-vector-data.html",
    "title": "Processing Base Vector Data",
    "section": "",
    "text": "In this module we will post-process the vector data we prepared earlier to generate some additional covariates using the terra package. We will also prepare all data for the analysis. This includes\n\ngenerating a road density layer\ncalculating distance to water metrics\nconverting our prepared vector to raster\nbuilding a raster stack with all covariate data"
  },
  {
    "objectID": "07-processing-vector-data.html#overview",
    "href": "07-processing-vector-data.html#overview",
    "title": "Processing Base Vector Data",
    "section": "",
    "text": "In this module we will post-process the vector data we prepared earlier to generate some additional covariates using the terra package. We will also prepare all data for the analysis. This includes\n\ngenerating a road density layer\ncalculating distance to water metrics\nconverting our prepared vector to raster\nbuilding a raster stack with all covariate data"
  },
  {
    "objectID": "07-processing-vector-data.html#calculate-density-of-road-network.",
    "href": "07-processing-vector-data.html#calculate-density-of-road-network.",
    "title": "Processing Base Vector Data",
    "section": "1. Calculate density of road network.",
    "text": "1. Calculate density of road network.\nOften we want to post-process our raw vector data to create more meaningful covariates in our analysis. In this case we are interested in the density of roads and how this might impact our caribou habitat choice.\n\n# load in the libraries needed\n\nlibrary(bcdata)\nlibrary(sf)\nlibrary(dplyr)\n\n# read in raster template\ntemplate &lt;- rast(file.path(\"clean_data\", \"template.tif\"))\n\nWe can assume that the type of road will have a different impact on Caribou avoidance or predator use. We can assign a value based on the road type. In this case we will assign a higher value to roads used more frequently.\n\n# read in roads\nroads &lt;- st_read(file.path(\"clean_data\", \"roads.gpkg\"))\n\n# assign a value to the roads based on estimated speed of travel or use\n\nroads &lt;- roads %&gt;% \n  mutate(rd_value = case_when(\n            ROAD_SURFACE == \"loose\" ~ 25,\n            ROAD_SURFACE == \"overgrown\" ~ 5,\n            ROAD_SURFACE == \"rough\" ~ 10,\n            ROAD_SURFACE == \"unknown\" ~ 7.5)) \n\nNow we can convert this to a raster using the template we created already\n\n# convert roads to a raster \nrroads &lt;- rasterize(roads, template, field = \"rd_value\" )\n\nplot(rroads)\n\nAs the impact of the roads is likely to impact a wider influence than a single pixal we can use a moving window analysis to expand the influence of roads for a given value.\n\n# create a moving window \nrrdens &lt;- focal(rroads, w=9, fun=\"sum\", na.rm=TRUE)\n\nplot(rrdens)\n\nterra::writeRaster(rrdens, file.path(\"clean_data\", \"road_density.tif\"), overwrite = TRUE)"
  },
  {
    "objectID": "07-processing-vector-data.html#calculate-distance-from-water",
    "href": "07-processing-vector-data.html#calculate-distance-from-water",
    "title": "Processing Base Vector Data",
    "section": "2. Calculate distance from water",
    "text": "2. Calculate distance from water\nWhile density provides a measure of impact we may also be interested in capturing the distance of fixes to caribou points. In this case we will explore the distance to water bodies within the study areas.\n\n# read in water \nwater &lt;- st_read(file.path(\"clean_data\", \"roads.gpkg\"))\n\n# calculate the distance to water for each pixel in the raster\n# be patient - this might take some time. \nwater_dis &lt;- terra::distance(template, water, unit=\"km\")\n\nplot(water_dis)\n\n# what sort of values are we seeing?\nrange(water_dis)\nsort(unique(values(water_dis)))\nrange(unique(values(water_dis)))"
  },
  {
    "objectID": "07-processing-vector-data.html#convert-base-vector-data-to-rasters",
    "href": "07-processing-vector-data.html#convert-base-vector-data-to-rasters",
    "title": "Processing Base Vector Data",
    "section": "3. Convert base vector data to rasters",
    "text": "3. Convert base vector data to rasters\nWe can also convert each vector layer we extracted into a raster.\nNote this can also be done at the vector stage, however predictive modelling will normally require a stacked covariate list so that you can predict from the built model\n\nbec &lt;- st_read(file.path(\"clean_data\",\"bec.gpkg\"))\nvri_cc &lt;- st_read(file.path(\"clean_data\",\"vri_cc.gpkg\"))\nvri_conif &lt;- st_read(file.path(\"clean_data\",\"vri_conif.gpkg\"))\nvri_ageclass &lt;- st_read(file.path(\"clean_data\",\"vri_ageclass.gpkg\"))\n#water\n#roads \n#cutblocks\n\n\n\n\n# convert to rasters\nvri_cc &lt;- rasterize(vri_cc, template, field = \"cc_class\" )\nvri_conif &lt;- rasterize(vri_conif, template, field = \"conif\" )\nvri_ageclass &lt;- rasterize(vri_ageclass, template, field = \"age_class\" )\n\n\n# stack into a set of rasters\nvect_stack &lt;- c(vri_cc, vri_conif, vri_ageclass, water_dis, rrdens)"
  },
  {
    "objectID": "07-processing-vector-data.html#lets-combine-our-already-created-rasters",
    "href": "07-processing-vector-data.html#lets-combine-our-already-created-rasters",
    "title": "Processing Base Vector Data",
    "section": "Lets combine our already created rasters",
    "text": "Lets combine our already created rasters\n\nrslope &lt;- rast(file.path(\"clean_data\",\"slope.gpkg\"))\naspect &lt;- rast(file.path(\"clean_data\",\"aspect.gpkg\"))\ntri  &lt;- rast(file.path(\"clean_data\",\"tri.gpkg\"))\ntrim_3005 &lt;- rast(file.path(\"clean_data\",\"dem.gpkg\"))\n\n\n\n# create a raster stack\nrast_stack &lt;- c(trim_3005, rslope, aspect, tri)"
  },
  {
    "objectID": "07-processing-vector-data.html#combine-raster-layers-into-a-stack",
    "href": "07-processing-vector-data.html#combine-raster-layers-into-a-stack",
    "title": "Processing Base Vector Data",
    "section": "4. Combine raster layers into a stack",
    "text": "4. Combine raster layers into a stack\n\n# lets combine both the raster stack and the vector stack \n\nall_stack &lt;- c(vect_stack, rast_stack)\n\nplot(all_stack)\n\n# we can write this out as a tif (raster object) \n#writeRaster(all_stack, file.path(\"clean_data\", \"rstack.tif\"), overwrite = T)\n\n\n## or we can write this out to a very small R object \n\nsaveRDS(all_stack, file.path(\"clean_data\", \"covars.RDS\")) # much faster"
  }
]