[
  {
    "objectID": "04-spatial-data-viz.html",
    "href": "04-spatial-data-viz.html",
    "title": "Visualization of Spatial Data",
    "section": "",
    "text": "Quick recap of plot()\nPretty (and useful) maps with ggplot2\nInteractive maps with mapview/leaflet"
  },
  {
    "objectID": "04-spatial-data-viz.html#outline",
    "href": "04-spatial-data-viz.html#outline",
    "title": "Visualization of Spatial Data",
    "section": "",
    "text": "Quick recap of plot()\nPretty (and useful) maps with ggplot2\nInteractive maps with mapview/leaflet"
  },
  {
    "objectID": "02-intro-spatial-data.html#outline",
    "href": "02-intro-spatial-data.html#outline",
    "title": "Introduction to Spatial Data with R",
    "section": "Outline",
    "text": "Outline\n\nIntroduction to Simple Features and the sf package\nCoordinate Reference Systems\nReading various spatial data formats into R\nBasic operations with spatial data"
  },
  {
    "objectID": "02-intro-spatial-data.html#learning-objectives",
    "href": "02-intro-spatial-data.html#learning-objectives",
    "title": "Introduction to Spatial Data with R",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nThe “simple features” representation of vector data\nUse and understand sf objects in R\nBasic understanding of Coordinate Reference Systems (CRS)\nUse mapview and sf to preview spatial data\nHow to do basic operations with spatial data using sf"
  },
  {
    "objectID": "02-intro-spatial-data.html#before-we-start",
    "href": "02-intro-spatial-data.html#before-we-start",
    "title": "Introduction to Spatial Data with R",
    "section": "Before we start",
    "text": "Before we start\nGet the course materials (if you haven’t already):\n\nusethis::use_course(\"https://tinyurl.com/3dmxtxaz\")"
  },
  {
    "objectID": "02-intro-spatial-data.html#vector-simple-features",
    "href": "02-intro-spatial-data.html#vector-simple-features",
    "title": "Introduction to Spatial Data with R",
    "section": "Vector: Simple Features",
    "text": "Vector: Simple Features\n\n\nThe sf R package1\nReplaces\n\nsp\nrgdal\nrgeos\n\n\n2\n\n\n\n\nSimple Features is a standard specification (Open Geospatial Consortium) - agreed-upon way to represent vector spatial data\nrepresent all common vector geometry types : points, lines, polygons and their respective ‘multi’ versions\nsupports geometry collections, which can contain multiple geometry types in a single object.\nsf supersedes the sp ecosystem, which comprises sp , rgdal for data read/write and rgeos for spatial operations.\nrgdal and rgeos are now retired and removed from CRAN\n\n\nsf package: https://cran.r-project.org/package=sfGeocomputation with R, fig 2.2: https://geocompr.robinlovelace.net"
  },
  {
    "objectID": "02-intro-spatial-data.html#reading-previewing-spatial-data",
    "href": "02-intro-spatial-data.html#reading-previewing-spatial-data",
    "title": "Introduction to Spatial Data with R",
    "section": "Reading & previewing spatial data",
    "text": "Reading & previewing spatial data\n\n\n\nlibrary(sf)\nlibrary(mapview)\n\nairports &lt;- read_sf(\n  \"raw_data/bc_airports.gpkg\"\n)\n\nmapview(airports)"
  },
  {
    "objectID": "02-intro-spatial-data.html#mapview",
    "href": "02-intro-spatial-data.html#mapview",
    "title": "Introduction to Spatial Data with R",
    "section": "mapview",
    "text": "mapview\n\nmapview(airports, zcol = \"NUMBER_OF_RUNWAYS\")"
  },
  {
    "objectID": "02-intro-spatial-data.html#structure-of-an-sf-object",
    "href": "02-intro-spatial-data.html#structure-of-an-sf-object",
    "title": "Introduction to Spatial Data with R",
    "section": "Structure of an sf object",
    "text": "Structure of an sf object\n\nairports\n\nSimple feature collection with 455 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 406543.7 ymin: 367957.6 xmax: 1796645 ymax: 1689146\nProjected CRS: NAD83 / BC Albers\n# A tibble: 455 × 6\n   AIRPORT_NAME                   IATA_CODE LOCALITY ELEVATION NUMBER_OF_RUNWAYS\n   &lt;chr&gt;                          &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;             &lt;int&gt;\n 1 Terrace (Northwest Regional) … &lt;NA&gt;      Terrace     217.                   2\n 2 Victoria Harbour (Camel Point… &lt;NA&gt;      Victoria      4.57                 0\n 3 Victoria Inner Harbour Airpor… YWH       Victoria      0                    0\n 4 Victoria Harbour (Shoal Point… &lt;NA&gt;      Victoria      3.05                 0\n 5 Victoria (Royal Jubilee Hospi… &lt;NA&gt;      Saanich      15.6                  0\n 6 Victoria (General Hospital) H… &lt;NA&gt;      View Ro…     15.8                  0\n 7 Victoria (BC Hydro) Heliport   &lt;NA&gt;      Saanich      12.2                  0\n 8 San Juan Point (Coast Guard) … &lt;NA&gt;      Port Re…      7.62                 0\n 9 Shawnigan Lake Water Aerodrome &lt;NA&gt;      Shawnig…      0                    0\n10 Victoria International Airport YYJ       North S…     19.5                  3\n# ℹ 445 more rows\n# ℹ 1 more variable: geom &lt;POINT [m]&gt;\n\n\n\ngo through sf header info: - size (# of features and # of columns/attributes) - geometry type - dimension (XY - can have Z and M) - bbox - CRS"
  },
  {
    "objectID": "02-intro-spatial-data.html#key-features-of-an-sf-object",
    "href": "02-intro-spatial-data.html#key-features-of-an-sf-object",
    "title": "Introduction to Spatial Data with R",
    "section": "Key features of an sf object",
    "text": "Key features of an sf object\n\n\n\nst_geometry(airports)\n\nGeometry set for 455 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 406543.7 ymin: 367957.6 xmax: 1796645 ymax: 1689146\nProjected CRS: NAD83 / BC Albers\nFirst 5 geometries:\n\n\nPOINT (833323.9 1054950)\n\n\nPOINT (1193727 381604.1)\n\n\nPOINT (1194902 382257.7)\n\n\nPOINT (1193719 382179.3)\n\n\nPOINT (1198292 383563.6)\n\nst_bbox(airports)\n\n     xmin      ymin      xmax      ymax \n 406543.7  367957.6 1796645.0 1689145.9 \n\n\n\n\nst_crs(airports)\n\nCoordinate Reference System:\n  User input: NAD83 / BC Albers \n  wkt:\nPROJCRS[\"NAD83 / BC Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"British Columbia Albers\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",45,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-126,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",50,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",58.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Province-wide spatial data management.\"],\n        AREA[\"Canada - British Columbia.\"],\n        BBOX[48.25,-139.04,60.01,-114.08]],\n    ID[\"EPSG\",3005]]"
  },
  {
    "objectID": "02-intro-spatial-data.html#an-sf-object-is-a-data.frame",
    "href": "02-intro-spatial-data.html#an-sf-object-is-a-data.frame",
    "title": "Introduction to Spatial Data with R",
    "section": "An sf object is a data.frame",
    "text": "An sf object is a data.frame\n\nclass(airports)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nis.data.frame(airports)\n\n[1] TRUE\n\nsummary(airports)\n\n AIRPORT_NAME        IATA_CODE           LOCALITY           ELEVATION     \n Length:455         Length:455         Length:455         Min.   :   0.0  \n Class :character   Class :character   Class :character   1st Qu.:   0.0  \n Mode  :character   Mode  :character   Mode  :character   Median :   6.4  \n                                                          Mean   : 194.4  \n                                                          3rd Qu.: 307.1  \n                                                          Max.   :1277.4  \n NUMBER_OF_RUNWAYS            geom    \n Min.   :0.0000    POINT        :455  \n 1st Qu.:0.0000    epsg:3005    :  0  \n Median :0.0000    +proj=aea ...:  0  \n Mean   :0.3385                       \n 3rd Qu.:1.0000                       \n Max.   :3.0000"
  },
  {
    "objectID": "02-intro-spatial-data.html#coordinate-reference-systems",
    "href": "02-intro-spatial-data.html#coordinate-reference-systems",
    "title": "Introduction to Spatial Data with R",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\n\n\nGeographic\n\nspherical or ellipsoidal surface\nellipsoid defined by the datum\nlat/long, measured in angular distances (degrees/radians)\n\n\n\n\n\n\n\nProjected\n\nCartesian coordinates on a flat plane\norigin, x and y axes, linear unit of measurement (e.g., m)\n\n\n\n\n\ndefines how the spatial elements of the data relate to the surface of the Earth\nGeographic: - identify any location on the Earth’s surface using two values — longitude and latitude. - Longitude is location in the East-West direction in angular distance from the Prime Meridian plane. - Latitude is angular distance North or South of the equatorial plane. - Distances in geographic CRSs are therefore not measured in meters. This has important consequences\nProjected: - based on a geographic CRS - rely on map projections to convert the three-dimensional surface of the Earth into Easting and Northing (x and y) values in a projected CRS - often named based on a property they preserve: equal-area preserves area, azimuthal preserve direction, equidistant preserve distance, and conformal preserve local shape. - conic, cylindrical, planar"
  },
  {
    "objectID": "02-intro-spatial-data.html#crss-in-r-epsg-codes",
    "href": "02-intro-spatial-data.html#crss-in-r-epsg-codes",
    "title": "Introduction to Spatial Data with R",
    "section": "CRSs in R: EPSG codes",
    "text": "CRSs in R: EPSG codes\n\nst_crs(airports)$input\n\n[1] \"NAD83 / BC Albers\"\n\n\n\n\n\nBC Albers - B.C.’s standard projection\n\nEqual Area conic\nCentre: c(-126, 54)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://epsg.io/3005\n\n\nEPSG codes BC Albers"
  },
  {
    "objectID": "02-intro-spatial-data.html#wkt-well-known-text",
    "href": "02-intro-spatial-data.html#wkt-well-known-text",
    "title": "Introduction to Spatial Data with R",
    "section": "WKT (Well-Known Text)",
    "text": "WKT (Well-Known Text)\n\nst_crs(3005)\n\nCoordinate Reference System:\n  User input: EPSG:3005 \n  wkt:\nPROJCRS[\"NAD83 / BC Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"British Columbia Albers\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",45,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-126,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",50,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",58.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Province-wide spatial data management.\"],\n        AREA[\"Canada - British Columbia.\"],\n        BBOX[48.25,-139.04,60.01,-114.08]],\n    ID[\"EPSG\",3005]]"
  },
  {
    "objectID": "02-intro-spatial-data.html#your-turn",
    "href": "02-intro-spatial-data.html#your-turn",
    "title": "Introduction to Spatial Data with R",
    "section": "Your turn",
    "text": "Your turn\n\nRead in the electoral districts data in the raw_data folder:\n\nWhat type of geometry is it?\nWhat is the CRS?\nWhat is the EPSG code?"
  },
  {
    "objectID": "02-intro-spatial-data.html#solution",
    "href": "02-intro-spatial-data.html#solution",
    "title": "Introduction to Spatial Data with R",
    "section": "Solution",
    "text": "Solution\n\nelec_bc &lt;- read_sf(\"raw_data/bc_electoral_districts.shp\")\nst_geometry_type(elec_bc, by_geometry = FALSE)\n\n[1] POLYGON\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\nst_crs(elec_bc)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nhttps://epsg.io/4326"
  },
  {
    "objectID": "02-intro-spatial-data.html#basic-plotting",
    "href": "02-intro-spatial-data.html#basic-plotting",
    "title": "Introduction to Spatial Data with R",
    "section": "Basic plotting",
    "text": "Basic plotting\n\nplot(elec_bc)"
  },
  {
    "objectID": "02-intro-spatial-data.html#basic-plotting-1",
    "href": "02-intro-spatial-data.html#basic-plotting-1",
    "title": "Introduction to Spatial Data with R",
    "section": "Basic plotting",
    "text": "Basic plotting\n\n\nJust the shapes\n\nplot(st_geometry(elec_bc))\n\n\n\n\n\n\n\n\n\nA single column\n\nplot(elec_bc[\"ED_NAME\"])\n\n\n\n\n\n\n\n\n\n\n\n\nNotice strange orientation of BC - north is greatly exaggerated\nbecause in WGS84 (lat/long)\nglobal CRS centred at lon 0 (Greenwich) and lat 0 (equator)\ngood for web mapping, not good for BC"
  },
  {
    "objectID": "02-intro-spatial-data.html#transforming-coordinate-systems",
    "href": "02-intro-spatial-data.html#transforming-coordinate-systems",
    "title": "Introduction to Spatial Data with R",
    "section": "Transforming coordinate systems",
    "text": "Transforming coordinate systems\n\nelec_bc_albers &lt;- st_transform(elec_bc, 3005)\n\nOr, if you have another object in the CRS you want to use:\n\nelec_bc_albers &lt;- st_transform(elec_bc, st_crs(airports))\nst_crs(elec_bc_albers)\n\nCoordinate Reference System:\n  User input: NAD83 / BC Albers \n  wkt:\nPROJCRS[\"NAD83 / BC Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"British Columbia Albers\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",45,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-126,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",50,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",58.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Province-wide spatial data management.\"],\n        AREA[\"Canada - British Columbia.\"],\n        BBOX[48.25,-139.04,60.01,-114.08]],\n    ID[\"EPSG\",3005]]"
  },
  {
    "objectID": "02-intro-spatial-data.html#your-turn-1",
    "href": "02-intro-spatial-data.html#your-turn-1",
    "title": "Introduction to Spatial Data with R",
    "section": "Your turn",
    "text": "Your turn\nLoad \"raw_data/ski_resorts.csv\" as an sf object\n\n\n\n\n\nfacility_name\nlocality\nlatitude\nlongitude\nelevation\n\n\n\n\nWapiti Ski Club\nElkford\n50.02168\n-114.9380\n1467\n\n\nSummit Lake Ski Area\nNakusp\n50.14546\n-117.6144\n1132\n\n\nSasquatch Mountain Resort\nHemlock Valley\n49.38011\n-121.9354\n1185\n\n\nApex Mountain Resort\nApex\n49.39042\n-119.9047\n1852\n\n\nSalmo Ski Hill\nSalmo\n49.18640\n-117.3015\n864\n\n\nRed Mountain Resort\nRossland\n49.10238\n-117.8194\n1150"
  },
  {
    "objectID": "02-intro-spatial-data.html#hints",
    "href": "02-intro-spatial-data.html#hints",
    "title": "Introduction to Spatial Data with R",
    "section": "Hints:",
    "text": "Hints:\nLoad \"raw_data/ski_resorts.csv\" as an sf object\n\nski_resorts &lt;- read.csv(\"raw_data/ski_resorts.csv\")\nski_resorts &lt;- st_as_sf(ski_resorts, ...)"
  },
  {
    "objectID": "02-intro-spatial-data.html#solution-1",
    "href": "02-intro-spatial-data.html#solution-1",
    "title": "Introduction to Spatial Data with R",
    "section": "Solution",
    "text": "Solution\n\nski_resorts &lt;- read.csv(\"raw_data/ski_resorts.csv\")\n\nski_resorts &lt;- st_as_sf(ski_resorts,\n                        coords = c(\"longitude\", \"latitude\"),\n                        crs = 4326)\n\nhead(ski_resorts)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -121.9354 ymin: 49.10238 xmax: -114.938 ymax: 50.14546\nGeodetic CRS:  WGS 84\n              facility_name       locality elevation                   geometry\n1           Wapiti Ski Club        Elkford      1467  POINT (-114.938 50.02168)\n2      Summit Lake Ski Area         Nakusp      1132 POINT (-117.6144 50.14546)\n3 Sasquatch Mountain Resort Hemlock Valley      1185 POINT (-121.9354 49.38011)\n4      Apex Mountain Resort           Apex      1852 POINT (-119.9047 49.39042)\n5            Salmo Ski Hill          Salmo       864  POINT (-117.3015 49.1864)\n6       Red Mountain Resort       Rossland      1150 POINT (-117.8194 49.10238)"
  },
  {
    "objectID": "02-intro-spatial-data.html#geometric-calculations",
    "href": "02-intro-spatial-data.html#geometric-calculations",
    "title": "Introduction to Spatial Data with R",
    "section": "Geometric calculations",
    "text": "Geometric calculations\n\n\nGeometric Measurements\n\nst_area()\nst_length()\nst_distance()\n\n\nGeometric Operations\n\nst_union()\nst_intersection()\nst_difference()\nst_sym_difference()"
  },
  {
    "objectID": "02-intro-spatial-data.html#geometry-predicates",
    "href": "02-intro-spatial-data.html#geometry-predicates",
    "title": "Introduction to Spatial Data with R",
    "section": "Geometry Predicates",
    "text": "Geometry Predicates\n\nUse with st_filter() or st_join()\n\n\n\n\nst_intersects(): touch or overlap\nst_disjoint(): !intersects\nst_touches(): touch\nst_crosses(): cross (don’t touch)\nst_within(): within\n\n\n\nst_contains(): contains\nst_overlaps(): overlaps\nst_covers(): cover\nst_covered_by(): covered by\nst_equals(): equals"
  },
  {
    "objectID": "02-intro-spatial-data.html#manipulating-geometries",
    "href": "02-intro-spatial-data.html#manipulating-geometries",
    "title": "Introduction to Spatial Data with R",
    "section": "Manipulating Geometries",
    "text": "Manipulating Geometries\n\n\n\nst_line_merge()\nst_segmentize()\nst_voronoi()\nst_centroid()\nst_convex_hull()\nst_triangulate()\n\n\n\nst_polygonize()\nst_split()\nst_buffer()\nst_make_valid()\nst_boundary()\n…"
  },
  {
    "objectID": "02-intro-spatial-data.html#your-turn-2",
    "href": "02-intro-spatial-data.html#your-turn-2",
    "title": "Introduction to Spatial Data with R",
    "section": "Your turn",
    "text": "Your turn\n\nCalculate the area of each electoral district\nCreate an sf object of only airports within the Nelson-Creston electoral district.\nPlot the ski resorts as circles, where the size of the circle is related to the elevation of the resort.\n\n\nst_area(elec_bc_albers)\nst_filter(airports, elec_bc_albers[elec_bc_albers$ED_NAME == “Nelson-Creston”, ])\nmapview(st_buffer(ski_resorts, dist = ski_resorts$elevation*10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkshop Home"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at gperkins@ninoxconsulting.ca or andy@andyteucher.ca. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by [Mozilla’s code of conduct enforcement ladder][https://github.com/mozilla/inclusion].\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at gperkins@ninoxconsulting.ca or andy@andyteucher.ca. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by [Mozilla’s code of conduct enforcement ladder][https://github.com/mozilla/inclusion].\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "06-raster-data.html",
    "href": "06-raster-data.html",
    "title": "Exploring Raster Data",
    "section": "",
    "text": "In this module we will explore raster data, and the available datasets which can be used in R for analysis. We will:\n\nprepare our study aoi to use with rasters\nextract DEM data through the bcmaps package\ngenerate DEM derived covariates,\nprepare a raster stack with all spatial layers generated to date."
  },
  {
    "objectID": "06-raster-data.html#overview",
    "href": "06-raster-data.html#overview",
    "title": "Exploring Raster Data",
    "section": "",
    "text": "In this module we will explore raster data, and the available datasets which can be used in R for analysis. We will:\n\nprepare our study aoi to use with rasters\nextract DEM data through the bcmaps package\ngenerate DEM derived covariates,\nprepare a raster stack with all spatial layers generated to date."
  },
  {
    "objectID": "06-raster-data.html#background-rasters",
    "href": "06-raster-data.html#background-rasters",
    "title": "Exploring Raster Data",
    "section": "Background: Rasters",
    "text": "Background: Rasters\nRaster data is information built on a standard grid. These can be characterised by the grid extent (xmin, xmax, ymin, ymax) and can have a coordinate system to orient them in space. Rasters are made up of cells or pixels, based on a resolution or cell size. Each cell contains a single value. The terra package contains many functions for manipulating and processing rasters.\nNote we will be using the BC Albers coordinate reference system (EPSG:3005). The advantages of this are 1) BCAlbers is an equal area projection across BC, 2) extent is marked using meters."
  },
  {
    "objectID": "06-raster-data.html#create-a-standard-raster-template.",
    "href": "06-raster-data.html#create-a-standard-raster-template.",
    "title": "Exploring Raster Data",
    "section": "1. Create a standard raster template.",
    "text": "1. Create a standard raster template.\nFor ease of calculations we will create a standard “template” raster which will form the basis for all our data analysis.\n\n# read in the libraries needed\n\nlibrary(dplyr)\nlibrary(sf)\nlibrary(bcmaps)\nlibrary(terra)\n\nRead in our AOI that we made and saved previously. We can then convert this to a raster with a resolution of given size. In this example we will select 25m.\n\n# read in the spatial file\n\nbou &lt;- read_sf(file.path(\"clean_data/scott_herd_subset.gpkg\"))\n\n\n# read in the aoi previously created\naoi &lt;- read_sf(file.path(\"clean_data\", \"scott_aoi.gpkg\"))\n\nWe can now convert our polygon to a raster object.\n\n# create a template raster with a resolution of 25m\ntemplate &lt;- rast(aoi , resolution = 25)\n\n# create a column name\nnames(template) = \"aoi\"\n\n# assign all values to zero \nvalues(template) &lt;- 0\n\nexport the raster template for later processing\n\nwriteRaster(template, file.path(\"clean_data\", \"template.tif\"), overwrite = TRUE)"
  },
  {
    "objectID": "06-raster-data.html#extract-base-data-using-the-cded-dataset",
    "href": "06-raster-data.html#extract-base-data-using-the-cded-dataset",
    "title": "Exploring Raster Data",
    "section": "2. Extract base data using the CDED dataset",
    "text": "2. Extract base data using the CDED dataset\nNow we can use the bcmaps package to directly download digital elevation data from the Canadian Digital Elevation Model CDED. Within BC, this is laregly equivalent to the TRIM DEM dataset. We will use the cded_terra function.\n\n# extract DEM raw data - Note this is downloaded in tiles which will be cached to avoid the need to redownload repeatedly \n\ntrim_raw &lt;- bcmaps::cded_terra(aoi)\n\nchecking your existing tiles for mapsheet 93o are up to date\n\n\nchecking your existing tiles for mapsheet 94b are up to date\n\n# lets look at the raster\ntrim_raw\n\nclass       : SpatRaster \ndimensions  : 1659, 3208, 1  (nrow, ncol, nlyr)\nresolution  : 0.0002083333, 0.0002083333  (x, y)\nextent      : -123.6851, -123.0168, 55.6746, 56.02022  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 (EPSG:4269) \nsource      : file5ebd3593e6f4.vrt \nname        : elevation \n\nres(trim_raw)\n\n[1] 0.0002083333 0.0002083333\n\n# note this is in WGS so we need to convert to 3005\n# reproject to match our raster template crs and extent\n\ntrim_3005 &lt;- project(trim_raw, template)\n\n\n# write out the individual raster\nwriteRaster(trim_3005, file.path(\"clean_data\", \"dem.tif\"), overwrite = TRUE)"
  },
  {
    "objectID": "06-raster-data.html#generate-dem-derived-covariates",
    "href": "06-raster-data.html#generate-dem-derived-covariates",
    "title": "Exploring Raster Data",
    "section": "3. Generate DEM derived covariates",
    "text": "3. Generate DEM derived covariates\nNow we can use the terrain functions within the terra package to generate some standard base layers derived from the DEM.\n\n# generate slope \nrslope &lt;- terrain(trim_3005, v = \"slope\", neighbors = 8, unit = \"degrees\") \n\n# generate aspect\naspect &lt;- terrain(trim_3005, v = \"aspect\", neighbors = 8,  unit = \"degrees\") \n\n# generate topographic roughness index\ntri &lt;- terrain(trim_3005, v = \"TRI\", neighbors = 8)\n\n# create a raster stack\nrstack &lt;- c(trim_3005, rslope, aspect, tri)\n\nplot(rstack)\n\n\n\n\nwrite our the individual rasters\n\nwriteRaster(rslope, file.path(\"clean_data\", \"slope.tif\"), overwrite = TRUE) \n\n\nwriteRaster(aspect , file.path(\"clean_data\", \"aspect.tif\"), overwrite = TRUE) \n\n\nwriteRaster(tri , file.path(\"clean_data\", \"tri.tif\"), overwrite = TRUE)"
  },
  {
    "objectID": "06-raster-data.html#bonus-questions.",
    "href": "06-raster-data.html#bonus-questions.",
    "title": "Exploring Raster Data",
    "section": "Bonus Questions.",
    "text": "Bonus Questions.\n\nExplore what other covariates you can generate using the terra::terrain function. Hint use ?terrain to bring up a help file.\nCompare outputs of aspect which the neighbours parameter is adjusted. What difference does this make to the output and the time to process?"
  },
  {
    "objectID": "03-intro-telemetry-data.html",
    "href": "03-intro-telemetry-data.html",
    "title": "Introduction to Telemetry Data",
    "section": "",
    "text": "In this course we will be using Telemetry data from Mountain Caribou (Rangifer terendus) herds in the Peace region of British Columbia. While the full dataset and metadata can be found on movebank, we will be working on a modified subset throughout this course. TEST\n\n\nOur first step is to see what our data looks like. In this course we will be providing two csv files (Mountain caribou in British Columbia-reference-data.csv and Mountain caribou.csv).\n\n# Read in our data files.\n\nlibrary(readr)\nlibrary(sf)\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\n\nref_raw &lt;- read_csv(\"raw_data/Mountain caribou in British Columbia-reference-data.csv\", \n                    name_repair = \"universal\")\n\nloc_raw &lt;- read_xlsx(\"raw_data/Mountain caribou.xlsx\")\n\nLets take a look….\n\nhead(ref_raw)\n\n# A tibble: 6 × 26\n  tag.id  animal.id  animal.taxon      deploy.on.date deploy.off.date\n  &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;             &lt;time&gt;         &lt;time&gt;         \n1 151.51  HR_151.510 Rangifer tarandus    NA             NA          \n2 C04a    GR_C04     Rangifer tarandus    NA          59:00          \n3 C03     GR_C03     Rangifer tarandus    NA             NA          \n4 151.805 HR_151.805 Rangifer tarandus    NA             NA          \n5 151.76  HR_151.760 Rangifer tarandus    NA             NA          \n6 151.72  HR_151.720 Rangifer tarandus    NA             NA          \n# ℹ 21 more variables: animal.death.comments &lt;chr&gt;, animal.life.stage &lt;chr&gt;,\n#   animal.reproductive.condition &lt;chr&gt;, animal.sex &lt;chr&gt;,\n#   animal.taxon.detail &lt;chr&gt;, attachment.type &lt;chr&gt;,\n#   deploy.off.latitude &lt;dbl&gt;, deploy.off.longitude &lt;dbl&gt;,\n#   deploy.on.latitude &lt;dbl&gt;, deploy.on.longitude &lt;dbl&gt;,\n#   deploy.on.person &lt;chr&gt;, deployment.comments &lt;chr&gt;,\n#   deployment.end.comments &lt;chr&gt;, deployment.end.type &lt;chr&gt;, …\n\nnames(ref_raw)\n\n [1] \"tag.id\"                        \"animal.id\"                    \n [3] \"animal.taxon\"                  \"deploy.on.date\"               \n [5] \"deploy.off.date\"               \"animal.death.comments\"        \n [7] \"animal.life.stage\"             \"animal.reproductive.condition\"\n [9] \"animal.sex\"                    \"animal.taxon.detail\"          \n[11] \"attachment.type\"               \"deploy.off.latitude\"          \n[13] \"deploy.off.longitude\"          \"deploy.on.latitude\"           \n[15] \"deploy.on.longitude\"           \"deploy.on.person\"             \n[17] \"deployment.comments\"           \"deployment.end.comments\"      \n[19] \"deployment.end.type\"           \"deployment.id\"                \n[21] \"manipulation.type\"             \"study.site\"                   \n[23] \"tag.beacon.frequency\"          \"tag.manufacturer.name\"        \n[25] \"tag.model\"                     \"tag.serial.no\"                \n\nref_short &lt;- ref_raw %&gt;%\n  select(tag.id, animal.id, deploy.on.date, animal.sex, animal.reproductive.condition,\n         deployment.end.type,tag.model, tag.manufacturer.name, tag.serial.no)\n\nhead(loc_raw)\n\n# A tibble: 6 × 14\n    event.id timestamp location.long location.lat   DOP FixType     herd \n       &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;\n1 2270202009 01:00.0           -124.         55.9     1 val. GPS-3D Scott\n2 2270202041 01:00.0           -124.         55.9     1 val. GPS-3D Scott\n3 2270202100 01:00.0           -124.         55.9     1 val. GPS-3D Scott\n4 2270202901 01:00.0           -123.         55.8     1 val. GPS-3D Scott\n5 2270202132 01:00.0           -123.         55.9     1 val. GPS-3D Scott\n6 2270202890 01:00.0           -123.         55.9     1 val. GPS-3D Scott\n# ℹ 7 more variables: study.specific.measurement &lt;chr&gt;, sensor.type &lt;chr&gt;,\n#   individual.taxon.canonical.name &lt;chr&gt;, tag.local.identifier &lt;chr&gt;,\n#   individual.local.identifier &lt;chr&gt;, study.name &lt;chr&gt;, date &lt;chr&gt;\n\n\nWe can combine these two dataset and keep only the columns which are of interest\n\nall_data &lt;- left_join(loc_raw, ref_raw, by = c('tag.local.identifier' = 'tag.id') )\n\nall_data &lt;- all_data %&gt;% \n  select(event.id, location.long, location.lat, DOP, FixType, herd,\n         study.specific.measurement, sensor.type, tag.local.identifier, date, animal.id,\n         animal.sex, animal.reproductive.condition, tag.manufacturer.name, tag.model )\n\n\n\n\n\n\nNow we have a single data set we can QA the data and provide more useful columns for further analysis.\n\nhead(all_data)\n\n# A tibble: 6 × 15\n  event.id location.long location.lat   DOP FixType herd  study.specific.measu…¹\n     &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;                 \n1   2.27e9         -124.         55.9     1 val. G… Scott Summer                \n2   2.27e9         -124.         55.9     1 val. G… Scott Summer                \n3   2.27e9         -124.         55.9     1 val. G… Scott Summer                \n4   2.27e9         -123.         55.8     1 val. G… Scott Summer                \n5   2.27e9         -123.         55.9     1 val. G… Scott Winter                \n6   2.27e9         -123.         55.9     1 val. G… Scott Summer                \n# ℹ abbreviated name: ¹​study.specific.measurement\n# ℹ 8 more variables: sensor.type &lt;chr&gt;, tag.local.identifier &lt;chr&gt;,\n#   date &lt;chr&gt;, animal.id &lt;chr&gt;, animal.sex &lt;chr&gt;,\n#   animal.reproductive.condition &lt;chr&gt;, tag.manufacturer.name &lt;chr&gt;,\n#   tag.model &lt;chr&gt;\n\n# check if there are NA's in the data \n\napply(all_data, 2, function(x) any(is.na(x)))\n\n                     event.id                 location.long \n                        FALSE                          TRUE \n                 location.lat                           DOP \n                         TRUE                         FALSE \n                      FixType                          herd \n                        FALSE                          TRUE \n   study.specific.measurement                   sensor.type \n                         TRUE                         FALSE \n         tag.local.identifier                          date \n                        FALSE                         FALSE \n                    animal.id                    animal.sex \n                        FALSE                         FALSE \nanimal.reproductive.condition         tag.manufacturer.name \n                         TRUE                         FALSE \n                    tag.model \n                        FALSE \n\n# Lets filter out any missing values \n\nlength(all_data$event.id)\n\n[1] 17197\n\ntdata &lt;- all_data %&gt;% \n  filter(!is.na(date)) %&gt;%\n  filter(!is.na(location.long)) %&gt;%\n  filter(!is.na(location.lat)) \n\n\n# Herd\nunique(tdata$herd)\n\n[1] \"Scott\"      \"Burnt Pine\" NA          \n\n# two missing herd values which we can fill in (or delete)\n\ntdata &lt;- tdata %&gt;% \n  mutate(herd = case_when(\n    animal.id == \"BP_car043\" ~ \"Burnt Pine\", \n    animal.id == \"SC_car170\" ~ \"Scott\",\n    .default = herd\n  ))\n\n\n\n#length(tdata$event.id)\n\nNow lets covert the timestamp into a usable format\n\n# calculate time differences\ntdata &lt;- tdata  %&gt;%\n  mutate(date_time = ymd_hms(date)) \n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `date_time = ymd_hms(date)`.\nCaused by warning:\n!  5 failed to parse.\n\n# owch we still have an error in this dataset\n\n# lets see if we can find it..... \n\nhead(sort(unique(tdata$date)))\n\n[1] \"2003-12-12 13:03:29.000\" \"2003-12-13 09:03:10.000\"\n[3] \"2003-12-14 05:03:11.000\" \"2003-12-15 01:03:10.000\"\n[5] \"2003-12-15 21:04:00.000\" \"2003-12-16 17:03:10.000\"\n\ntail(sort(unique(tdata$date)))\n\n[1] \"2016-03-16 10:01:00.000\" \"2016-03-16 17:49:00.000\"\n[3] \"2016-03-16 19:49:00.000\" \"2016-03-17 17:53:00.000\"\n[5] \"2016-03-17 19:53:00.000\" \"NA\"                     \n\ntdata &lt;- tdata  %&gt;% \n  filter(date != \"NA\")\n\ntdata  &lt;- tdata  %&gt;%\n  mutate(date_time = ymd_hms(date)) \n\n#head(tdata$date_time) \n\n# Note the Universal Coordinated Time Zone\n\n\n# lets split this data format into something more useful \n\ntdata  &lt;- tdata  %&gt;%\n  mutate(year = year(date_time )) %&gt;%\n  mutate(month = month(date_time ),\n         day = day(date_time),\n         hour = hour(date_time),\n         minute = minute(date_time))\n\n\n\n\nNow we have fixed our data entry problems we also want to review the spatial accuracy. We have two metrics: DOP (Dilution of Precision), and a Fix Type.\n\n# review the lat / longs \n\nrange(tdata$location.lat)\n\n[1]  55.2249 155.4764\n\nhist(tdata$location.lat)\n\n\n\n# above 65 latidude\n\nrange(tdata$location.long)\n\n[1] -123.6684  -22.0000\n\nhist(tdata$location.long)\n\n\n\n# greater than -100 longitude. \n\ntdata &lt;- tdata %&gt;% \n  filter(location.long &lt;= -100) %&gt;%\n  filter(location.lat &lt;= 65)\n\n# DOP \n\nrange (tdata$DOP)\n\n[1]  1.0 43.1\n\nhist(tdata$DOP)\n\n\n\nunique(tdata$DOP)\n\n [1]  1.0  1.2  1.4  1.6  1.8  2.0  2.2  2.4  2.6  2.8  3.0  3.2  3.4  3.6  3.8\n[16]  4.0  4.2  4.4  4.6  4.8  5.0  5.2  5.4  5.6  5.8  6.4  6.6  7.2  7.8  7.9\n[31] 11.8 18.0 43.1\n\n# for this example we only want to keep fixes with a DOP less than 10m\n\nfdata &lt;- tdata %&gt;% \n  filter(DOP &lt;= 10)\n\nhist(fdata$DOP)\n\n\n\nunique(tdata$DOP)\n\n [1]  1.0  1.2  1.4  1.6  1.8  2.0  2.2  2.4  2.6  2.8  3.0  3.2  3.4  3.6  3.8\n[16]  4.0  4.2  4.4  4.6  4.8  5.0  5.2  5.4  5.6  5.8  6.4  6.6  7.2  7.8  7.9\n[31] 11.8 18.0 43.1\n\n# Fix Type : \n\nfixtype &lt;- fdata %&gt;% \n  group_by(FixType) %&gt;%\n  summarise(count = n())\n\nfixtype\n\n# A tibble: 3 × 2\n  FixType     count\n  &lt;chr&gt;       &lt;int&gt;\n1 GPS-2D         92\n2 GPS-3D        291\n3 val. GPS-3D 16801\n\n# remove the 2d locations \n\nfdata &lt;- fdata %&gt;% \n  filter(FixType != \"GPS-2D\")\n\n# see what the data looks like\nglimpse(fdata)\n\nRows: 17,092\nColumns: 21\n$ event.id                      &lt;dbl&gt; 2270202009, 2270202041, 2270202100, 2270…\n$ location.long                 &lt;dbl&gt; -123.6036, -123.5987, -123.5903, -123.49…\n$ location.lat                  &lt;dbl&gt; 55.90000, 55.87343, 55.87470, 55.83741, …\n$ DOP                           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ FixType                       &lt;chr&gt; \"val. GPS-3D\", \"val. GPS-3D\", \"val. GPS-…\n$ herd                          &lt;chr&gt; \"Scott\", \"Scott\", \"Scott\", \"Scott\", \"Sco…\n$ study.specific.measurement    &lt;chr&gt; \"Summer\", \"Summer\", \"Summer\", \"Summer\", …\n$ sensor.type                   &lt;chr&gt; \"gps\", \"gps\", \"gps\", \"gps\", \"gps\", \"gps\"…\n$ tag.local.identifier          &lt;chr&gt; \"car170\", \"car170\", \"car170\", \"car170\", …\n$ date                          &lt;chr&gt; \"2013-09-23 10:01:00.000\", \"2013-10-09 1…\n$ animal.id                     &lt;chr&gt; \"SC_car170\", \"SC_car170\", \"SC_car170\", \"…\n$ animal.sex                    &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", …\n$ animal.reproductive.condition &lt;chr&gt; \"with calf: N\", \"with calf: N\", \"with ca…\n$ tag.manufacturer.name         &lt;chr&gt; \"ATS\", \"ATS\", \"ATS\", \"ATS\", \"ATS\", \"ATS\"…\n$ tag.model                     &lt;chr&gt; \"GPS Iridium\", \"GPS Iridium\", \"GPS Iridi…\n$ date_time                     &lt;dttm&gt; 2013-09-23 10:01:00, 2013-10-09 10:01:0…\n$ year                          &lt;dbl&gt; 2013, 2013, 2013, 2014, 2013, 2014, 2014…\n$ month                         &lt;dbl&gt; 9, 10, 10, 8, 11, 8, 9, 12, 7, 11, 6, 12…\n$ day                           &lt;int&gt; 23, 9, 30, 11, 10, 7, 19, 6, 8, 24, 20, …\n$ hour                          &lt;int&gt; 10, 10, 10, 10, 2, 10, 2, 18, 18, 2, 2, …\n$ minute                        &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 0, 0, 1…\n\n# lets check if this column is any use? \nunique(fdata$sensor.type)\n\n[1] \"gps\"\n\n# remove the columns that we dont need\nfdata &lt;- fdata %&gt;% \n  select(-FixType, -DOP, -date, -study.specific.measurement, -sensor.type, -event.id)"
  },
  {
    "objectID": "03-intro-telemetry-data.html#how-to-qa-and-summarise-your-telemetry-data",
    "href": "03-intro-telemetry-data.html#how-to-qa-and-summarise-your-telemetry-data",
    "title": "Introduction to Telemetry Data",
    "section": "",
    "text": "In this course we will be using Telemetry data from Mountain Caribou (Rangifer terendus) herds in the Peace region of British Columbia. While the full dataset and metadata can be found on movebank, we will be working on a modified subset throughout this course. TEST\n\n\nOur first step is to see what our data looks like. In this course we will be providing two csv files (Mountain caribou in British Columbia-reference-data.csv and Mountain caribou.csv).\n\n# Read in our data files.\n\nlibrary(readr)\nlibrary(sf)\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\n\nref_raw &lt;- read_csv(\"raw_data/Mountain caribou in British Columbia-reference-data.csv\", \n                    name_repair = \"universal\")\n\nloc_raw &lt;- read_xlsx(\"raw_data/Mountain caribou.xlsx\")\n\nLets take a look….\n\nhead(ref_raw)\n\n# A tibble: 6 × 26\n  tag.id  animal.id  animal.taxon      deploy.on.date deploy.off.date\n  &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;             &lt;time&gt;         &lt;time&gt;         \n1 151.51  HR_151.510 Rangifer tarandus    NA             NA          \n2 C04a    GR_C04     Rangifer tarandus    NA          59:00          \n3 C03     GR_C03     Rangifer tarandus    NA             NA          \n4 151.805 HR_151.805 Rangifer tarandus    NA             NA          \n5 151.76  HR_151.760 Rangifer tarandus    NA             NA          \n6 151.72  HR_151.720 Rangifer tarandus    NA             NA          \n# ℹ 21 more variables: animal.death.comments &lt;chr&gt;, animal.life.stage &lt;chr&gt;,\n#   animal.reproductive.condition &lt;chr&gt;, animal.sex &lt;chr&gt;,\n#   animal.taxon.detail &lt;chr&gt;, attachment.type &lt;chr&gt;,\n#   deploy.off.latitude &lt;dbl&gt;, deploy.off.longitude &lt;dbl&gt;,\n#   deploy.on.latitude &lt;dbl&gt;, deploy.on.longitude &lt;dbl&gt;,\n#   deploy.on.person &lt;chr&gt;, deployment.comments &lt;chr&gt;,\n#   deployment.end.comments &lt;chr&gt;, deployment.end.type &lt;chr&gt;, …\n\nnames(ref_raw)\n\n [1] \"tag.id\"                        \"animal.id\"                    \n [3] \"animal.taxon\"                  \"deploy.on.date\"               \n [5] \"deploy.off.date\"               \"animal.death.comments\"        \n [7] \"animal.life.stage\"             \"animal.reproductive.condition\"\n [9] \"animal.sex\"                    \"animal.taxon.detail\"          \n[11] \"attachment.type\"               \"deploy.off.latitude\"          \n[13] \"deploy.off.longitude\"          \"deploy.on.latitude\"           \n[15] \"deploy.on.longitude\"           \"deploy.on.person\"             \n[17] \"deployment.comments\"           \"deployment.end.comments\"      \n[19] \"deployment.end.type\"           \"deployment.id\"                \n[21] \"manipulation.type\"             \"study.site\"                   \n[23] \"tag.beacon.frequency\"          \"tag.manufacturer.name\"        \n[25] \"tag.model\"                     \"tag.serial.no\"                \n\nref_short &lt;- ref_raw %&gt;%\n  select(tag.id, animal.id, deploy.on.date, animal.sex, animal.reproductive.condition,\n         deployment.end.type,tag.model, tag.manufacturer.name, tag.serial.no)\n\nhead(loc_raw)\n\n# A tibble: 6 × 14\n    event.id timestamp location.long location.lat   DOP FixType     herd \n       &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;\n1 2270202009 01:00.0           -124.         55.9     1 val. GPS-3D Scott\n2 2270202041 01:00.0           -124.         55.9     1 val. GPS-3D Scott\n3 2270202100 01:00.0           -124.         55.9     1 val. GPS-3D Scott\n4 2270202901 01:00.0           -123.         55.8     1 val. GPS-3D Scott\n5 2270202132 01:00.0           -123.         55.9     1 val. GPS-3D Scott\n6 2270202890 01:00.0           -123.         55.9     1 val. GPS-3D Scott\n# ℹ 7 more variables: study.specific.measurement &lt;chr&gt;, sensor.type &lt;chr&gt;,\n#   individual.taxon.canonical.name &lt;chr&gt;, tag.local.identifier &lt;chr&gt;,\n#   individual.local.identifier &lt;chr&gt;, study.name &lt;chr&gt;, date &lt;chr&gt;\n\n\nWe can combine these two dataset and keep only the columns which are of interest\n\nall_data &lt;- left_join(loc_raw, ref_raw, by = c('tag.local.identifier' = 'tag.id') )\n\nall_data &lt;- all_data %&gt;% \n  select(event.id, location.long, location.lat, DOP, FixType, herd,\n         study.specific.measurement, sensor.type, tag.local.identifier, date, animal.id,\n         animal.sex, animal.reproductive.condition, tag.manufacturer.name, tag.model )\n\n\n\n\n\n\nNow we have a single data set we can QA the data and provide more useful columns for further analysis.\n\nhead(all_data)\n\n# A tibble: 6 × 15\n  event.id location.long location.lat   DOP FixType herd  study.specific.measu…¹\n     &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;                 \n1   2.27e9         -124.         55.9     1 val. G… Scott Summer                \n2   2.27e9         -124.         55.9     1 val. G… Scott Summer                \n3   2.27e9         -124.         55.9     1 val. G… Scott Summer                \n4   2.27e9         -123.         55.8     1 val. G… Scott Summer                \n5   2.27e9         -123.         55.9     1 val. G… Scott Winter                \n6   2.27e9         -123.         55.9     1 val. G… Scott Summer                \n# ℹ abbreviated name: ¹​study.specific.measurement\n# ℹ 8 more variables: sensor.type &lt;chr&gt;, tag.local.identifier &lt;chr&gt;,\n#   date &lt;chr&gt;, animal.id &lt;chr&gt;, animal.sex &lt;chr&gt;,\n#   animal.reproductive.condition &lt;chr&gt;, tag.manufacturer.name &lt;chr&gt;,\n#   tag.model &lt;chr&gt;\n\n# check if there are NA's in the data \n\napply(all_data, 2, function(x) any(is.na(x)))\n\n                     event.id                 location.long \n                        FALSE                          TRUE \n                 location.lat                           DOP \n                         TRUE                         FALSE \n                      FixType                          herd \n                        FALSE                          TRUE \n   study.specific.measurement                   sensor.type \n                         TRUE                         FALSE \n         tag.local.identifier                          date \n                        FALSE                         FALSE \n                    animal.id                    animal.sex \n                        FALSE                         FALSE \nanimal.reproductive.condition         tag.manufacturer.name \n                         TRUE                         FALSE \n                    tag.model \n                        FALSE \n\n# Lets filter out any missing values \n\nlength(all_data$event.id)\n\n[1] 17197\n\ntdata &lt;- all_data %&gt;% \n  filter(!is.na(date)) %&gt;%\n  filter(!is.na(location.long)) %&gt;%\n  filter(!is.na(location.lat)) \n\n\n# Herd\nunique(tdata$herd)\n\n[1] \"Scott\"      \"Burnt Pine\" NA          \n\n# two missing herd values which we can fill in (or delete)\n\ntdata &lt;- tdata %&gt;% \n  mutate(herd = case_when(\n    animal.id == \"BP_car043\" ~ \"Burnt Pine\", \n    animal.id == \"SC_car170\" ~ \"Scott\",\n    .default = herd\n  ))\n\n\n\n#length(tdata$event.id)\n\nNow lets covert the timestamp into a usable format\n\n# calculate time differences\ntdata &lt;- tdata  %&gt;%\n  mutate(date_time = ymd_hms(date)) \n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `date_time = ymd_hms(date)`.\nCaused by warning:\n!  5 failed to parse.\n\n# owch we still have an error in this dataset\n\n# lets see if we can find it..... \n\nhead(sort(unique(tdata$date)))\n\n[1] \"2003-12-12 13:03:29.000\" \"2003-12-13 09:03:10.000\"\n[3] \"2003-12-14 05:03:11.000\" \"2003-12-15 01:03:10.000\"\n[5] \"2003-12-15 21:04:00.000\" \"2003-12-16 17:03:10.000\"\n\ntail(sort(unique(tdata$date)))\n\n[1] \"2016-03-16 10:01:00.000\" \"2016-03-16 17:49:00.000\"\n[3] \"2016-03-16 19:49:00.000\" \"2016-03-17 17:53:00.000\"\n[5] \"2016-03-17 19:53:00.000\" \"NA\"                     \n\ntdata &lt;- tdata  %&gt;% \n  filter(date != \"NA\")\n\ntdata  &lt;- tdata  %&gt;%\n  mutate(date_time = ymd_hms(date)) \n\n#head(tdata$date_time) \n\n# Note the Universal Coordinated Time Zone\n\n\n# lets split this data format into something more useful \n\ntdata  &lt;- tdata  %&gt;%\n  mutate(year = year(date_time )) %&gt;%\n  mutate(month = month(date_time ),\n         day = day(date_time),\n         hour = hour(date_time),\n         minute = minute(date_time))\n\n\n\n\nNow we have fixed our data entry problems we also want to review the spatial accuracy. We have two metrics: DOP (Dilution of Precision), and a Fix Type.\n\n# review the lat / longs \n\nrange(tdata$location.lat)\n\n[1]  55.2249 155.4764\n\nhist(tdata$location.lat)\n\n\n\n# above 65 latidude\n\nrange(tdata$location.long)\n\n[1] -123.6684  -22.0000\n\nhist(tdata$location.long)\n\n\n\n# greater than -100 longitude. \n\ntdata &lt;- tdata %&gt;% \n  filter(location.long &lt;= -100) %&gt;%\n  filter(location.lat &lt;= 65)\n\n# DOP \n\nrange (tdata$DOP)\n\n[1]  1.0 43.1\n\nhist(tdata$DOP)\n\n\n\nunique(tdata$DOP)\n\n [1]  1.0  1.2  1.4  1.6  1.8  2.0  2.2  2.4  2.6  2.8  3.0  3.2  3.4  3.6  3.8\n[16]  4.0  4.2  4.4  4.6  4.8  5.0  5.2  5.4  5.6  5.8  6.4  6.6  7.2  7.8  7.9\n[31] 11.8 18.0 43.1\n\n# for this example we only want to keep fixes with a DOP less than 10m\n\nfdata &lt;- tdata %&gt;% \n  filter(DOP &lt;= 10)\n\nhist(fdata$DOP)\n\n\n\nunique(tdata$DOP)\n\n [1]  1.0  1.2  1.4  1.6  1.8  2.0  2.2  2.4  2.6  2.8  3.0  3.2  3.4  3.6  3.8\n[16]  4.0  4.2  4.4  4.6  4.8  5.0  5.2  5.4  5.6  5.8  6.4  6.6  7.2  7.8  7.9\n[31] 11.8 18.0 43.1\n\n# Fix Type : \n\nfixtype &lt;- fdata %&gt;% \n  group_by(FixType) %&gt;%\n  summarise(count = n())\n\nfixtype\n\n# A tibble: 3 × 2\n  FixType     count\n  &lt;chr&gt;       &lt;int&gt;\n1 GPS-2D         92\n2 GPS-3D        291\n3 val. GPS-3D 16801\n\n# remove the 2d locations \n\nfdata &lt;- fdata %&gt;% \n  filter(FixType != \"GPS-2D\")\n\n# see what the data looks like\nglimpse(fdata)\n\nRows: 17,092\nColumns: 21\n$ event.id                      &lt;dbl&gt; 2270202009, 2270202041, 2270202100, 2270…\n$ location.long                 &lt;dbl&gt; -123.6036, -123.5987, -123.5903, -123.49…\n$ location.lat                  &lt;dbl&gt; 55.90000, 55.87343, 55.87470, 55.83741, …\n$ DOP                           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ FixType                       &lt;chr&gt; \"val. GPS-3D\", \"val. GPS-3D\", \"val. GPS-…\n$ herd                          &lt;chr&gt; \"Scott\", \"Scott\", \"Scott\", \"Scott\", \"Sco…\n$ study.specific.measurement    &lt;chr&gt; \"Summer\", \"Summer\", \"Summer\", \"Summer\", …\n$ sensor.type                   &lt;chr&gt; \"gps\", \"gps\", \"gps\", \"gps\", \"gps\", \"gps\"…\n$ tag.local.identifier          &lt;chr&gt; \"car170\", \"car170\", \"car170\", \"car170\", …\n$ date                          &lt;chr&gt; \"2013-09-23 10:01:00.000\", \"2013-10-09 1…\n$ animal.id                     &lt;chr&gt; \"SC_car170\", \"SC_car170\", \"SC_car170\", \"…\n$ animal.sex                    &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", …\n$ animal.reproductive.condition &lt;chr&gt; \"with calf: N\", \"with calf: N\", \"with ca…\n$ tag.manufacturer.name         &lt;chr&gt; \"ATS\", \"ATS\", \"ATS\", \"ATS\", \"ATS\", \"ATS\"…\n$ tag.model                     &lt;chr&gt; \"GPS Iridium\", \"GPS Iridium\", \"GPS Iridi…\n$ date_time                     &lt;dttm&gt; 2013-09-23 10:01:00, 2013-10-09 10:01:0…\n$ year                          &lt;dbl&gt; 2013, 2013, 2013, 2014, 2013, 2014, 2014…\n$ month                         &lt;dbl&gt; 9, 10, 10, 8, 11, 8, 9, 12, 7, 11, 6, 12…\n$ day                           &lt;int&gt; 23, 9, 30, 11, 10, 7, 19, 6, 8, 24, 20, …\n$ hour                          &lt;int&gt; 10, 10, 10, 10, 2, 10, 2, 18, 18, 2, 2, …\n$ minute                        &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 0, 0, 1…\n\n# lets check if this column is any use? \nunique(fdata$sensor.type)\n\n[1] \"gps\"\n\n# remove the columns that we dont need\nfdata &lt;- fdata %&gt;% \n  select(-FixType, -DOP, -date, -study.specific.measurement, -sensor.type, -event.id)"
  },
  {
    "objectID": "03-intro-telemetry-data.html#output-cleaned",
    "href": "03-intro-telemetry-data.html#output-cleaned",
    "title": "Introduction to Telemetry Data",
    "section": "Output cleaned",
    "text": "Output cleaned\nWe can output out cleaned data as a table\n\nwrite_csv(fdata, file.path(\"clean_data\", \"caribou.csv\"))"
  },
  {
    "objectID": "03-intro-telemetry-data.html#convert-to-spatial-file-and-export",
    "href": "03-intro-telemetry-data.html#convert-to-spatial-file-and-export",
    "title": "Introduction to Telemetry Data",
    "section": "Convert to spatial file and export",
    "text": "Convert to spatial file and export\n\n# convert to a sf object and transform to BC Albers\n\nbou &lt;- st_as_sf(fdata, coords = c(\"location.long\", \"location.lat\"), \n                crs = 4326, remove = FALSE) |&gt; \n  st_transform(3005)\n\nexport as .gpkg\n\nwrite_sf(bou, file.path(\"clean_data\", \"caribou.gpkg\"))\n\nWe can also export to a .shp file\n\nwrite_sf(bou, file.path(\"clean_data\", \"caribou.shp\"))\n\n# note warning on names for shapefile"
  },
  {
    "objectID": "03-intro-telemetry-data.html#generating-tabular-summaries",
    "href": "03-intro-telemetry-data.html#generating-tabular-summaries",
    "title": "Introduction to Telemetry Data",
    "section": "Generating tabular summaries",
    "text": "Generating tabular summaries\nNow we have clean data to work with we can get to the fun data exploration part!\n\n#bou = read.csv(file.path(\"clean_data\", \"caribou.csv\")\n\n# or \n\nbou_sf = read_sf(file.path(\"clean_data\", \"caribou.gpkg\"))\n\nbou &lt;- st_drop_geometry(bou_sf)\n\n\nhead(bou)\n\n# A tibble: 6 × 15\n  location.long location.lat herd  tag.local.identifier animal.id animal.sex\n          &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                &lt;chr&gt;     &lt;chr&gt;     \n1         -124.         55.9 Scott car170               SC_car170 f         \n2         -124.         55.9 Scott car170               SC_car170 f         \n3         -124.         55.9 Scott car170               SC_car170 f         \n4         -123.         55.8 Scott car170               SC_car170 f         \n5         -123.         55.9 Scott car170               SC_car170 f         \n6         -123.         55.9 Scott car170               SC_car170 f         \n# ℹ 9 more variables: animal.reproductive.condition &lt;chr&gt;,\n#   tag.manufacturer.name &lt;chr&gt;, tag.model &lt;chr&gt;, date_time &lt;dttm&gt;, year &lt;dbl&gt;,\n#   month &lt;dbl&gt;, day &lt;int&gt;, hour &lt;int&gt;, minute &lt;int&gt;\n\nglimpse(bou)\n\nRows: 17,092\nColumns: 15\n$ location.long                 &lt;dbl&gt; -123.6036, -123.5987, -123.5903, -123.49…\n$ location.lat                  &lt;dbl&gt; 55.90000, 55.87343, 55.87470, 55.83741, …\n$ herd                          &lt;chr&gt; \"Scott\", \"Scott\", \"Scott\", \"Scott\", \"Sco…\n$ tag.local.identifier          &lt;chr&gt; \"car170\", \"car170\", \"car170\", \"car170\", …\n$ animal.id                     &lt;chr&gt; \"SC_car170\", \"SC_car170\", \"SC_car170\", \"…\n$ animal.sex                    &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", …\n$ animal.reproductive.condition &lt;chr&gt; \"with calf: N\", \"with calf: N\", \"with ca…\n$ tag.manufacturer.name         &lt;chr&gt; \"ATS\", \"ATS\", \"ATS\", \"ATS\", \"ATS\", \"ATS\"…\n$ tag.model                     &lt;chr&gt; \"GPS Iridium\", \"GPS Iridium\", \"GPS Iridi…\n$ date_time                     &lt;dttm&gt; 2013-09-23 03:01:00, 2013-10-09 03:01:0…\n$ year                          &lt;dbl&gt; 2013, 2013, 2013, 2014, 2013, 2014, 2014…\n$ month                         &lt;dbl&gt; 9, 10, 10, 8, 11, 8, 9, 12, 7, 11, 6, 12…\n$ day                           &lt;int&gt; 23, 9, 30, 11, 10, 7, 19, 6, 8, 24, 20, …\n$ hour                          &lt;int&gt; 10, 10, 10, 10, 2, 10, 2, 18, 18, 2, 2, …\n$ minute                        &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 0, 0, 1…\n\n\nMany questions we can ask here:\n\nhow many herds do we have?\nhow many animals in each herd?\nwhat is the sex ratio of collared animals?\nwhat is the duration of each collar? Start and end years?\n\n\nno_herds = unique(bou$herd)\n\nno_records &lt;- bou %&gt;% \n  group_by(herd) %&gt;% \n  summarise(count = n())\n\nno_animals_id &lt;- bou %&gt;% \n  group_by(herd, animal.id) %&gt;% \n  summarise(count = n())\n\n`summarise()` has grouped output by 'herd'. You can override using the\n`.groups` argument.\n\nno_animals_sex &lt;- bou %&gt;% \n  group_by(herd, animal.sex) %&gt;% \n  summarise(count = n())\n\n`summarise()` has grouped output by 'herd'. You can override using the\n`.groups` argument.\n\ncollar_type &lt;- bou %&gt;% \n  group_by(herd, tag.manufacturer.name, tag.model) %&gt;% \n  summarise(count = n())\n\n`summarise()` has grouped output by 'herd', 'tag.manufacturer.name'. You can\noverride using the `.groups` argument.\n\n\nThere is alot of data here. For the next section of this course we will just concentrate on the Scott Herd.\n\n# Filter to keep the Scott herd only \nsbou &lt;- bou %&gt;% \n  filter(herd == \"Scott\")\n\n# tidy the data by removing columns that are redundant\nsbou &lt;- sbou %&gt;% select(-herd, -tag.local.identifier, -tag.manufacturer.name, -tag.model)\n\n\n# how many animals?\nno_animals &lt;- unique(sbou$animal.id)\n\n\n# lets look at the time period: \np1 &lt;- ggplot(sbou, aes(year, fill = animal.id)) +\n    geom_bar(position = \"dodge\")\n\n\n# duration of the collars within the Scott herd. \n\ntable_max &lt;- sbou %&gt;% \n  select(animal.id, date_time) %&gt;%\n  slice_max(date_time, by = animal.id) \ncolnames(table_max)&lt;- c(\"animal.id\",\"max\")\n\ntable_min &lt;- sbou %&gt;% \n  select(animal.id, date_time) %&gt;%\n  slice_min(date_time, by = animal.id) \n\ncolnames(table_min)&lt;- c(\"animal.id\",\"min\")\n\ndur &lt;- left_join(table_max, table_min, by = join_by(animal.id)) %&gt;%\n  distinct() %&gt;%\n  mutate(duration = max - min) %&gt;%\n  mutate(dur_days = round( duration,1)) %&gt;%\n  mutate(dur_hrs = round(as.numeric(dur_days)*24,1)) %&gt;%\n #mutate(dur_days = round( dur_hrs/24,1)) %&gt;%\n  mutate(year_start = year(min), \n         year_end = year(max))\n\n\n\ndur_plot &lt;- ggplot(dur, aes(y=factor(animal.id))) +\n  geom_segment(aes(x=min, xend=max, y=factor(animal.id), yend=factor(animal.id)), linewidth = 3)+\n  xlab(\"Date\") + ylab(\"Tag\") \n\ndur_plot\n\n\n\n# months of the year. \np_duration &lt;- ggplot(sbou, aes(factor(month), fill = factor(year)))+\n  geom_bar(position = \"dodge\") +\n  #xlim(1,12)+\n  facet_wrap(~animal.id)+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\np_duration\n\n\n\n\n\nLets check the number of fixes per month per individual\nwe can see that the number of fixes are relatively steady throughout the years for all individuals except SC_car171? something looks strange here:\nLarger spikes in Oct 2015 and Feb 2016\n\nhead(sbou)\n\n# A tibble: 6 × 11\n  location.long location.lat animal.id animal.sex animal.reproductive.condition\n          &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;                        \n1         -124.         55.9 SC_car170 f          with calf: N                 \n2         -124.         55.9 SC_car170 f          with calf: N                 \n3         -124.         55.9 SC_car170 f          with calf: N                 \n4         -123.         55.8 SC_car170 f          with calf: N                 \n5         -123.         55.9 SC_car170 f          with calf: N                 \n6         -123.         55.9 SC_car170 f          with calf: N                 \n# ℹ 6 more variables: date_time &lt;dttm&gt;, year &lt;dbl&gt;, month &lt;dbl&gt;, day &lt;int&gt;,\n#   hour &lt;int&gt;, minute &lt;int&gt;\n\nsbou &lt;- sbou %&gt;% \n  arrange(animal.id, date_time)\n\nsbou_dur &lt;- sbou %&gt;%\n    mutate(time = as.POSIXct(date_time, format = \"%y/%d/%m %H:%M:%S\")) %&gt;%\n    group_by(animal.id) %&gt;%\n    mutate(diff = difftime(time, lag(time),  units = c(\"hours\")), \n           diff = as.numeric(diff))\n\n\n# we can see a big range in the time intervals for the fixes\n\nrange(sbou_dur$diff, na.rm = TRUE)\n\n[1]   0.01666667 111.75000000\n\n# most fall in the less than than 10 \nhist(sbou_dur$diff)\n\n\n\n# lets look at the individual animals\n\nggplot(sbou_dur, aes(diff)) + \n  geom_histogram(bins=30) +\n  facet_grid(.~animal.id)\n\nWarning: Removed 4 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n# much of the problem is with the SC_car171 individual \n\nggplot(sbou_dur, aes(y = diff, x = date_time)) + \n  geom_point() +\n  facet_wrap(.~animal.id, scales = \"free\")\n\nWarning: Removed 4 rows containing missing values (`geom_point()`).\n\n\n\n\n# suspect that these are a mortality signal from ? \n# some collars have a higher fix rate (SC_car168 consistent less than 10 hr difference)\n\nTo create a standardized fix per day, lets take the first fix per day. This could be based on a number of factors, depending on our research question we want to ask\n\n# we can subset base on a Julian date\n\nsbou_dur &lt;- sbou_dur %&gt;%\n  mutate(\n    date2=as.Date(date_time, format = '%Y-%m-%d'),\n    jdate=julian(date2)\n  )\n\n# Check the multiple counts of anaimals per day \ncounts.per.day.jdate &lt;- sbou_dur %&gt;% \n  group_by(animal.id, jdate, year) %&gt;% \n  summarise(total = n(), unique = unique(jdate)) %&gt;% \n  group_by(animal.id, total, year) %&gt;% \n  summarise(total.j = n()) \n\n`summarise()` has grouped output by 'animal.id', 'jdate'. You can override\nusing the `.groups` argument.\n`summarise()` has grouped output by 'animal.id', 'total'. You can override\nusing the `.groups` argument.\n\nggplot(counts.per.day.jdate, aes(x = total, y = total.j)) + \n  geom_point() +\n  geom_vline(xintercept = 1, color = \"red\")+\n  facet_wrap(.~animal.id + year)\n\n\n\nsbou_sub &lt;- sbou_dur %&gt;%\n    group_by(animal.id, jdate) %&gt;%\n    slice_sample( n = 1) \n\n\n# repeat this\n\n# Check the multiple counts of anaimals per day \ncounts.per.day.jdate &lt;- sbou_sub %&gt;% group_by(animal.id, jdate, year) %&gt;% \n  summarise(total = n(), unique = unique(jdate)) %&gt;% \n  group_by(animal.id, total, year) %&gt;% \n  summarise(total.j = n()) \n\n`summarise()` has grouped output by 'animal.id', 'jdate'. You can override\nusing the `.groups` argument.\n`summarise()` has grouped output by 'animal.id', 'total'. You can override\nusing the `.groups` argument.\n\nggplot(counts.per.day.jdate, aes(x = total, y = total.j)) + \n  geom_point() +\n  geom_vline(xintercept = 1, color = \"red\") +\n  facet_wrap(.~animal.id + year)\n\n\n\nlength(sbou_sub$location.long)\n\n[1] 2906\n\n\n\n# lets write this out as .csv and .Gpkg\n\nwrite_csv(sbou_sub, file.path(\"clean_data\", \"scott_herd_subset.csv\"))\n\nconvert to a sf object\n\nsbou_sub_sf &lt;- st_as_sf(sbou_sub, coords = c(\"location.long\", \"location.lat\"), \n                        crs = 4326, remove = FALSE) |&gt; \n  st_transform(3005)\n\nThen write to a gpkg:\n\n# # export as .gpkg\nwrite_sf(sbou_sub_sf, file.path(\"clean_data\", \"scott_herd_subset.gpkg\"))"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2023 r-telemetry-workshop-nov-2023 authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "08-rsf-prep.html",
    "href": "08-rsf-prep.html",
    "title": "Generate random sample points for RSF",
    "section": "",
    "text": "In this module we use our prepared spatial data to generate the input datafiles to conduct a resource selection function analysis. This includes:\n\ngenerate a set of “available” or “background points” from our study area.\nextract spatial information for presence and available point locations\nexport as table for future use\nuse the extracted data to provide further graphics and summary options"
  },
  {
    "objectID": "08-rsf-prep.html#overview",
    "href": "08-rsf-prep.html#overview",
    "title": "Generate random sample points for RSF",
    "section": "",
    "text": "In this module we use our prepared spatial data to generate the input datafiles to conduct a resource selection function analysis. This includes:\n\ngenerate a set of “available” or “background points” from our study area.\nextract spatial information for presence and available point locations\nexport as table for future use\nuse the extracted data to provide further graphics and summary options"
  },
  {
    "objectID": "08-rsf-prep.html#background-resource-selection-functions",
    "href": "08-rsf-prep.html#background-resource-selection-functions",
    "title": "Generate random sample points for RSF",
    "section": "Background: Resource Selection Functions",
    "text": "Background: Resource Selection Functions\nNow that we have a cleaned and standardized dataset for the Scott Herd Caribou’s we can prepare the data for further analysis.\nResource Selection Functions are a common method used to assess what is driving patterns of animal habitat preference. This process uses information or covariates (i.e. landscape attribute features) for locations where animals are present and compares them to the same features where animals are absent. In this way we can gather some information on what conditions (i.e. landscape, aspect, distance from road, etc) which contribute to higher habitat use. You can find more references for Resource Selection Functions below.\nWhile in this example we are concentrating on future analysis, the process of extracting background information for known locations can also provide meaningful summary statistics, i.e. proportion of locations within a specific BEC zone."
  },
  {
    "objectID": "08-rsf-prep.html#generate-availalble-location-points.",
    "href": "08-rsf-prep.html#generate-availalble-location-points.",
    "title": "Generate random sample points for RSF",
    "section": "1. Generate availalble location points.",
    "text": "1. Generate availalble location points.\nWe will generate a simple set of “available” based on the geographic distribution of the study area. Note you can limit this area in which you generate “background” points, using more sophisticated methods, such as within a kernal density or home range estimate.\nLets start by reading in the libraries we will use.\n\nlibrary(dplyr)\nlibrary(terra)\nlibrary(sf)\nlibrary(mapview)\nlibrary(ggplot2)\n\nNext, read in the standardized data points for the Scott herd. If needed we can transform to BC Albers projection (EPGS:3005) to match our raster stack prepared in the previous step.\n\n# read in the aoi template \n\ntemplate &lt;- rast(file.path(\"clean_data\",\"template.tif\"))\n\n\n# read in points \n\nbou_pts &lt;- st_read(file.path(\"clean_data/scott_herd_subset.gpkg\")) \nbou_pts &lt;- st_transform(bou_pts, 3005)\n\n\n# Lets keep only the important columns and add a \"presence/absence\" column. \n\nbou_pts &lt;- bou_pts %&gt;% \n  dplyr::select(animal.id, jdate)%&gt;%\n  mutate(pres_abs = 1)\n\nWe can use the spatSample function to generate random points for our given study area. This function has many more options which can be reviewed by using : ?spatSample in the console.\nLets generate a set of points the same length as our “presence” locations using a “random” method.\n\n# Generate random points for RSF use areas.\nset.seed(123)\navail_points &lt;- spatSample(template, size = 2906, as.points = TRUE, na.rm = TRUE, method = \"random\")\n\navail_points &lt;- st_as_sf(avail_points)\n\n# lets rename the column to make it clear these are background points \navail_points &lt;- avail_points %&gt;%\n  rename(\"pres_abs\" = lyr.1 )\n\nWe can do a quick review fo the points to see what they look like using mapview.\n\nmapview(avail_points) +\nmapview(bou_pts, color = \"red\", cex= 3)\n\nWe can now combine our caribou locations and “available” locations into a single dataset. We will retain the spatial information to allow us to easily extract the values in the corresponding raster stack\n\navail_points \n\n\nbou_pts\n\n# note we have slightly different column headers \"geom\" vs \"geometry\" \nst_geometry(avail_points) = \"geom\" \n\n\nallpts &lt;- bind_rows(bou_pts, avail_points ) \n\nNext, we can read in our prepared raster stack as an .rds object. We can now use the extract function from the terra package to extract information for all layers in the raster stack for each of our points.\n\n# read in the raster stack \n\nrstack &lt;- readRDS(file.path(\"clean_data\", \"covars.RDS\"))\n\n\n# extract all values in the raster stack for each location in the bou_pts file. \n\natts &lt;- terra::extract(rstack, allpts)\n\n\n# remove unused columns \n# Could show how to use st_write() and explain the cbind(st_coordinates()) bit\nbou_full_pts &lt;- cbind(allpts, atts) %&gt;%\n  select(-ID)\n\n\n# export this as spatial file # or \n\n\nbou_table &lt;- bou_full_pts %&gt;%\n  cbind(st_coordinates(.)) %&gt;%\n  st_drop_geometry()\n\n\n# write out as csv \n\nwrite.csv(bou_table, file.path(\"clean_data\", \"allpts_att.csv\"))"
  },
  {
    "objectID": "08-rsf-prep.html#optional",
    "href": "08-rsf-prep.html#optional",
    "title": "Generate random sample points for RSF",
    "section": "OPTIONAL",
    "text": "OPTIONAL"
  },
  {
    "objectID": "08-rsf-prep.html#build-a-mask-with-the-occurance-point-optional",
    "href": "08-rsf-prep.html#build-a-mask-with-the-occurance-point-optional",
    "title": "Generate random sample points for RSF",
    "section": "Build a mask with the “occurance point” (optional)",
    "text": "Build a mask with the “occurance point” (optional)\n\noccur_raster &lt;- rasterize(bou_pts, template, field = \"pres_abs\")\ntemplate_mask &lt;- terra::mask(template, occur_raster, inverse = TRUE)\n\nplot(template_mask)"
  },
  {
    "objectID": "08-rsf-prep.html#add-rsf-refs",
    "href": "08-rsf-prep.html#add-rsf-refs",
    "title": "Generate random sample points for RSF",
    "section": "add RSF refs",
    "text": "add RSF refs"
  },
  {
    "objectID": "bonus-movebank.html",
    "href": "bonus-movebank.html",
    "title": "Downloading telemetry data from Movebank",
    "section": "",
    "text": "In this module we will introduce the move2 r package and demonstrate how you can download telemetry data directly from the movebank web repository via an API."
  },
  {
    "objectID": "bonus-movebank.html#overview",
    "href": "bonus-movebank.html#overview",
    "title": "Downloading telemetry data from Movebank",
    "section": "",
    "text": "In this module we will introduce the move2 r package and demonstrate how you can download telemetry data directly from the movebank web repository via an API."
  },
  {
    "objectID": "bonus-movebank.html#background",
    "href": "bonus-movebank.html#background",
    "title": "Downloading telemetry data from Movebank",
    "section": "Background",
    "text": "Background\nMovebank is a free, online database of animal movement data hosted by the Max Planck Institute for Ornithology. Data owners can manage their data and have the option to share it with colleagues or the public. If the public or a registered user has permission to see a study, the study can be downloaded as a .csv file and imported directly into R using the move2 package\nNote move2 this is an updated version of the move package\n\n#install.packages(\"move2\")\nlibrary(\"move2\")\n\nlibrary(dplyr, quietly = TRUE)\n\n\nYou will need to create a log in on the movebank platform to be able to download and view data. This can be done at Movebank.\nSet up your credentials within the move2 package. To access any data through R you will also need to supply the credentials you used to create your movebank account to the move2 package. This package uses the keyring package to safely store your credentials. You only need to run the following code once, after which the credentials will be remembered for following R sessions.\n\nNOTE : make sure to keep your credential safe, and do not committ these to a common code platform such as github.\n\nmovebank_store_credentials(\"myUserName\", \"myPassword\")\n\n\n#movebank_store_credentials(\"gcperkins\", \"*****\")\n\n# to delete your credentials run this line\n#movebank_remove_credentials()\n\n# you can check by running this \nkeyring::key_list()"
  },
  {
    "objectID": "bonus-movebank.html#extra-challenge",
    "href": "bonus-movebank.html#extra-challenge",
    "title": "Downloading telemetry data from Movebank",
    "section": "Extra challenge!",
    "text": "Extra challenge!\nIf you are interested in viewing the entire Caribou dataset you can find it on movebank (study_id = 216040785)\n\n# download the study info details\nbou &lt;- movebank_download_study_info(study_id = 216040785)\n\n# download the data \nbou_data &lt;- movebank_download_study(216040785, attributes = NULL)\n\n# download deployment data \nbou_ref_data &lt;- movebank_download_deployment(216040785)\n\nReferences:\n\nMovebank.\nmove2 r package vignette - “Downloading data from movebank”"
  },
  {
    "objectID": "00-intro-setup.html",
    "href": "00-intro-setup.html",
    "title": "Introduction and System Setup",
    "section": "",
    "text": "This two-day course will introduce you to the basics of working with telemetry data in R.\nWe will providing a data set to use throughout the workshop, but we encourage you to bring along your own data for the hackathon session (Friday AM).\n\n\n\nThe basics of spatial data in R\nHow to read in, clean, and QA your telemetry data\nHow to create useful summaries of your data\nData visualization techniques for spatial and telemetry data (graphs and maps)\nHow to use the bcmaps and bcdata packages to get vector and raster data from official BC government sources.\nHow to perform spatial operations to compile and generate landcape covariates\nHow to extract spatial information for telemetry and random location points\n\n\n\n\n\nMethodological details on technical telemetry parameters i.e. DOP estimates\nAdvanced modeling techniques\n\n\n\n\n\n\n\nSetup and troubleshooting (8:30 - 9:00)\nWorking with spatial data in R (9:00 - 10:30)\nBREAK (10:30 - 11:00)\nTelemetry data in R; clean, QA, and prepare data (Part I; 11:00 - 12:30)\nLUNCH (12:30 - 1:30)\nTelemetry data in R; clean, QA, and prepare data (Part II; 1:30 - 3:00)\nBREAK (3:00 - 3:30)\nVisualizing spatial data in R (3:30 - 4:30)\n\n\n\n\n\nSetup and troubleshooting (8:30 - 9:00)\nRetrieving spatial data from the B.C. Data Catalogue (9:00 - 10:30)\nBREAK (10:30 - 11:00)\nGetting and working with raster data (Part I; 11:00 - 12:30)\nLUNCH (12:30 - 1:30)\nPreparing base data for telemetry analysis (Part II; 1:30 - 3:00)\nBREAK (3:00 - 3:30)\nPreparing data for Resource Selection Function (RSF) analysis (3:30 - 4:30)"
  },
  {
    "objectID": "00-intro-setup.html#overview",
    "href": "00-intro-setup.html#overview",
    "title": "Introduction and System Setup",
    "section": "",
    "text": "This two-day course will introduce you to the basics of working with telemetry data in R.\nWe will providing a data set to use throughout the workshop, but we encourage you to bring along your own data for the hackathon session (Friday AM).\n\n\n\nThe basics of spatial data in R\nHow to read in, clean, and QA your telemetry data\nHow to create useful summaries of your data\nData visualization techniques for spatial and telemetry data (graphs and maps)\nHow to use the bcmaps and bcdata packages to get vector and raster data from official BC government sources.\nHow to perform spatial operations to compile and generate landcape covariates\nHow to extract spatial information for telemetry and random location points\n\n\n\n\n\nMethodological details on technical telemetry parameters i.e. DOP estimates\nAdvanced modeling techniques\n\n\n\n\n\n\n\nSetup and troubleshooting (8:30 - 9:00)\nWorking with spatial data in R (9:00 - 10:30)\nBREAK (10:30 - 11:00)\nTelemetry data in R; clean, QA, and prepare data (Part I; 11:00 - 12:30)\nLUNCH (12:30 - 1:30)\nTelemetry data in R; clean, QA, and prepare data (Part II; 1:30 - 3:00)\nBREAK (3:00 - 3:30)\nVisualizing spatial data in R (3:30 - 4:30)\n\n\n\n\n\nSetup and troubleshooting (8:30 - 9:00)\nRetrieving spatial data from the B.C. Data Catalogue (9:00 - 10:30)\nBREAK (10:30 - 11:00)\nGetting and working with raster data (Part I; 11:00 - 12:30)\nLUNCH (12:30 - 1:30)\nPreparing base data for telemetry analysis (Part II; 1:30 - 3:00)\nBREAK (3:00 - 3:30)\nPreparing data for Resource Selection Function (RSF) analysis (3:30 - 4:30)"
  },
  {
    "objectID": "00-intro-setup.html#computing-requirements",
    "href": "00-intro-setup.html#computing-requirements",
    "title": "Introduction and System Setup",
    "section": "Computing requirements",
    "text": "Computing requirements\nYou will require the following software installed and configured for the workshop. Please have this set up and ready to go before we start.\nYou will need:\n\nA laptop computer, preferably with administrative privileges\nR and RStudio\nSeveral R packages\nQGIS (optional)\n\n\nInstall R and RStudio\nYou will need:\n\nR version &gt;= 4.2.0\nRStudio &gt;= 2023.03.1\n\n\nInstall R\nDownload and install R for your operating system from https://cloud.r-project.org/.\n\nWindows with no admin rights:\nIf you do not have administrator rights, the installer will default to installing somewhere in your user folder (e.g., C:/Users/username/AppData/Local/Programs/). If you prefer, you can change the location to another folder that you have write access to, just make sure it is on your C:/ drive.\n\n\n\nInstall R Studio\nDownload and install RStudio Desktop from https://posit.co/download/rstudio-desktop/. This page should automatically offer you the version suitable for your operating system, but you can scroll down to find versions for all operating systems.\n\nWindows with admin rights:\nDownload the .zip archive for Windows under “Zip/Tarballs”. Create a folder called RStudio in a location on your C:/ drive, where you have write access (e.g. C:/Users/username/AppData/Local/Programs/RStudio), and extract the zip file into this folder. Find the RStudio program in this folder: it is named rstudio.exe, but the file extension will typically be hidden, so look for rstudio. Right-click this file to create a shortcut and drag it to your desktop/task bar. Use the shortcut to open RStudio.\n\n\n\n\nInstall packages\nIn R, install the necessary packages by running:\n\ninstall.packages(\n  c(\"tidyverse\", \"sf\", \"terra\", \"mapview\", \"bcdata\", \"bcmaps\", \"readxl\", \"ggplot2\", \"usethis\")\n)\n\n\n\nQGIS (optional)\nQGIS is a free and open-source geographic information system (GIS) that allows you to create, edit, visualize, analyze and publish geospatial information.\nDownload and install from the QGIS website."
  },
  {
    "objectID": "00-intro-setup.html#course-materials-and-data",
    "href": "00-intro-setup.html#course-materials-and-data",
    "title": "Introduction and System Setup",
    "section": "Course materials and data",
    "text": "Course materials and data\nWe have created an empty RStudio project containing the course data.\nThe easiest way to get started to run:\n\nusethis::use_course(\"https://tinyurl.com/3dmxtxaz\")\n\nThis will download the course materials, save them in a logical spot on your computer, and open the project in a new RStudio session.\nAlternatively, you can download the zip file from here, decompress the zip file somewhere convenient on your C:/ drive. Then, navigate to the newly created folder (called r-telemetry-workshop), and double click on the r-telemetry-workshop.Rproj file to open the project in RStudio."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Telemetry Workshop, November 2023",
    "section": "",
    "text": "Welcome!\nWe are happy to be working with you to step through the process to use R to clean, summarise and analyse your telemetry data. This course will be for basic to medium R coders, but never fear, we will have material to follow along with and to take home so don’t worry!\nEvent details\n\nWhere: Cedar Boardroom, 2000 South Ospika Blvd, Prince George.\nWhen: November 15th, 16th, 17th 2023\nTimes: The course will be two days (8:30 - 4:30pm, 15th and 16th November) followed by a half day hackathon (8:30 - 12:30, 17th November). We will have breaks for coffee and lunch. Please bring your own food and snacks.\nWhat to bring: Bring your laptop with installed versions of R and Rstudio. For setup instructions go to setup instructions. Help will be available for the first 30 minutes before the course starts (8:30 - 9:00am) to help trouble shoot any computer problems\n\nWhat is a hackathon? A hackathon is an opportunity to work with your peers on common problems. This might be talking through options, paired coding or splitting up to separate tasks. We will provide more input during the course to help you best use this time.\nIf you have a dataset you would like to work on, please bring it with you and you can use this for the hackathon.\nThank-you\nThank-you to Ministry of Forests, Government of British Columbia for sponsoring this workshop."
  },
  {
    "objectID": "bonus-kde.html",
    "href": "bonus-kde.html",
    "title": "Generating Kernel Density Estimates",
    "section": "",
    "text": "In this module we will use caribou data to create home range estimates using kernal density and minimum convex polygons.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n#install.packages(\"ks\")\nlibrary(ks)\nlibrary(mapview)\n\n\nscott &lt;- st_read(\"clean_data/caribou.gpkg\") %&gt;% \n  st_transform(3005)\n\nReading layer `caribou' from data source \n  `/Users/andy/dev/r-telemetry-workshop-nov-2023/clean_data/caribou.gpkg' \n  using driver `GPKG'\nSimple feature collection with 17092 features and 15 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1145290 ymin: 1143000 xmax: 1247842 ymax: 1227464\nProjected CRS: NAD83 / BC Albers\n\nspt &lt;- scott %&gt;% \n  st_coordinates(scott)\n  \nx &lt;- spt\n\n\n\nst_boundary(scott) \n\nSimple feature collection with 17092 features and 15 fields (with 17092 geometries empty)\nGeometry type: GEOMETRYCOLLECTION\nDimension:     XY\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: NAD83 / BC Albers\nFirst 10 features:\n   location.long location.lat  herd tag.local.identifier animal.id animal.sex\n1      -123.6036     55.90000 Scott               car170 SC_car170          f\n2      -123.5987     55.87343 Scott               car170 SC_car170          f\n3      -123.5903     55.87470 Scott               car170 SC_car170          f\n4      -123.4915     55.83741 Scott               car170 SC_car170          f\n5      -123.4740     55.87877 Scott               car170 SC_car170          f\n6      -123.4412     55.85412 Scott               car170 SC_car170          f\n7      -123.4325     55.91398 Scott               car171 SC_car171          f\n8      -123.4310     55.87820 Scott               car170 SC_car170          f\n9      -123.4280     55.87681 Scott               car170 SC_car170          f\n10     -123.4252     55.87287 Scott               car170 SC_car170          f\n   animal.reproductive.condition tag.manufacturer.name   tag.model\n1                   with calf: N                   ATS GPS Iridium\n2                   with calf: N                   ATS GPS Iridium\n3                   with calf: N                   ATS GPS Iridium\n4                   with calf: N                   ATS GPS Iridium\n5                   with calf: N                   ATS GPS Iridium\n6                   with calf: N                   ATS GPS Iridium\n7                   with calf: Y                   ATS GPS Iridium\n8                   with calf: N                   ATS GPS Iridium\n9                   with calf: N                   ATS GPS Iridium\n10                  with calf: N                   ATS GPS Iridium\n             date_time year month day hour minute                     geom\n1  2013-09-23 03:01:00 2013     9  23   10      1 GEOMETRYCOLLECTION EMPTY\n2  2013-10-09 03:01:00 2013    10   9   10      1 GEOMETRYCOLLECTION EMPTY\n3  2013-10-30 03:01:00 2013    10  30   10      1 GEOMETRYCOLLECTION EMPTY\n4  2014-08-11 03:01:00 2014     8  11   10      1 GEOMETRYCOLLECTION EMPTY\n5  2013-11-09 18:01:00 2013    11  10    2      1 GEOMETRYCOLLECTION EMPTY\n6  2014-08-07 03:01:00 2014     8   7   10      1 GEOMETRYCOLLECTION EMPTY\n7  2014-09-18 19:02:00 2014     9  19    2      2 GEOMETRYCOLLECTION EMPTY\n8  2013-12-06 10:02:00 2013    12   6   18      2 GEOMETRYCOLLECTION EMPTY\n9  2014-07-08 11:01:00 2014     7   8   18      1 GEOMETRYCOLLECTION EMPTY\n10 2013-11-23 18:01:00 2013    11  24    2      1 GEOMETRYCOLLECTION EMPTY\n\n# minimujm convex polygons\n\n\npts1 &lt;- st_as_sf(x = scott, coords = c('location.long', 'location.lat'))\nmy_hull &lt;- st_convex_hull(st_union(pts1))\nplot(my_hull)\nplot(pts1[1], cex=2, col=\"blue\", add = T)\n\n\n\n# \n# \n# set.seed(8192) \n# samp&lt;-200 \n# mus&lt;-rbind(c(-2,2),c(0,0),c(2,-2)) \n# Sigmas&lt;-rbind(diag(2),matrix(c(0.8,-0.72,-0.72,0.8),nrow=2),diag(2)) \n# cwt&lt;-3/11 \n# props&lt;-c((1-cwt)/2,cwt,(1-cwt)/2) \n# x&lt;-rmvnorm.mixt(n=samp,mus=mus,Sigmas=Sigmas,props=props)\n\n\n# Kernal density \n\n# using = h ref \n\nHpi1 &lt;- Hpi(x=x)\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nHpi2 &lt;- Hpi.diag(x = x)\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nfhat.pi1 &lt;-kde(x=x,H=Hpi1) \n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\n\nWarning in quantile.default(dobs, prob = (100 - cont)/100): partial argument\nmatch of 'prob' to 'probs'\n\nfhat.pi2 &lt;-kde(x=x,H=Hpi2)\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\n\nWarning in quantile.default(dobs, prob = (100 - cont)/100): partial argument\nmatch of 'prob' to 'probs'\n\nplot(fhat.pi1)\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\nplot(fhat.pi1)\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'"
  },
  {
    "objectID": "bonus-kde.html#overview",
    "href": "bonus-kde.html#overview",
    "title": "Generating Kernel Density Estimates",
    "section": "",
    "text": "In this module we will use caribou data to create home range estimates using kernal density and minimum convex polygons.\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE\n\n#install.packages(\"ks\")\nlibrary(ks)\nlibrary(mapview)\n\n\nscott &lt;- st_read(\"clean_data/caribou.gpkg\") %&gt;% \n  st_transform(3005)\n\nReading layer `caribou' from data source \n  `/Users/andy/dev/r-telemetry-workshop-nov-2023/clean_data/caribou.gpkg' \n  using driver `GPKG'\nSimple feature collection with 17092 features and 15 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1145290 ymin: 1143000 xmax: 1247842 ymax: 1227464\nProjected CRS: NAD83 / BC Albers\n\nspt &lt;- scott %&gt;% \n  st_coordinates(scott)\n  \nx &lt;- spt\n\n\n\nst_boundary(scott) \n\nSimple feature collection with 17092 features and 15 fields (with 17092 geometries empty)\nGeometry type: GEOMETRYCOLLECTION\nDimension:     XY\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: NAD83 / BC Albers\nFirst 10 features:\n   location.long location.lat  herd tag.local.identifier animal.id animal.sex\n1      -123.6036     55.90000 Scott               car170 SC_car170          f\n2      -123.5987     55.87343 Scott               car170 SC_car170          f\n3      -123.5903     55.87470 Scott               car170 SC_car170          f\n4      -123.4915     55.83741 Scott               car170 SC_car170          f\n5      -123.4740     55.87877 Scott               car170 SC_car170          f\n6      -123.4412     55.85412 Scott               car170 SC_car170          f\n7      -123.4325     55.91398 Scott               car171 SC_car171          f\n8      -123.4310     55.87820 Scott               car170 SC_car170          f\n9      -123.4280     55.87681 Scott               car170 SC_car170          f\n10     -123.4252     55.87287 Scott               car170 SC_car170          f\n   animal.reproductive.condition tag.manufacturer.name   tag.model\n1                   with calf: N                   ATS GPS Iridium\n2                   with calf: N                   ATS GPS Iridium\n3                   with calf: N                   ATS GPS Iridium\n4                   with calf: N                   ATS GPS Iridium\n5                   with calf: N                   ATS GPS Iridium\n6                   with calf: N                   ATS GPS Iridium\n7                   with calf: Y                   ATS GPS Iridium\n8                   with calf: N                   ATS GPS Iridium\n9                   with calf: N                   ATS GPS Iridium\n10                  with calf: N                   ATS GPS Iridium\n             date_time year month day hour minute                     geom\n1  2013-09-23 03:01:00 2013     9  23   10      1 GEOMETRYCOLLECTION EMPTY\n2  2013-10-09 03:01:00 2013    10   9   10      1 GEOMETRYCOLLECTION EMPTY\n3  2013-10-30 03:01:00 2013    10  30   10      1 GEOMETRYCOLLECTION EMPTY\n4  2014-08-11 03:01:00 2014     8  11   10      1 GEOMETRYCOLLECTION EMPTY\n5  2013-11-09 18:01:00 2013    11  10    2      1 GEOMETRYCOLLECTION EMPTY\n6  2014-08-07 03:01:00 2014     8   7   10      1 GEOMETRYCOLLECTION EMPTY\n7  2014-09-18 19:02:00 2014     9  19    2      2 GEOMETRYCOLLECTION EMPTY\n8  2013-12-06 10:02:00 2013    12   6   18      2 GEOMETRYCOLLECTION EMPTY\n9  2014-07-08 11:01:00 2014     7   8   18      1 GEOMETRYCOLLECTION EMPTY\n10 2013-11-23 18:01:00 2013    11  24    2      1 GEOMETRYCOLLECTION EMPTY\n\n# minimujm convex polygons\n\n\npts1 &lt;- st_as_sf(x = scott, coords = c('location.long', 'location.lat'))\nmy_hull &lt;- st_convex_hull(st_union(pts1))\nplot(my_hull)\nplot(pts1[1], cex=2, col=\"blue\", add = T)\n\n\n\n# \n# \n# set.seed(8192) \n# samp&lt;-200 \n# mus&lt;-rbind(c(-2,2),c(0,0),c(2,-2)) \n# Sigmas&lt;-rbind(diag(2),matrix(c(0.8,-0.72,-0.72,0.8),nrow=2),diag(2)) \n# cwt&lt;-3/11 \n# props&lt;-c((1-cwt)/2,cwt,(1-cwt)/2) \n# x&lt;-rmvnorm.mixt(n=samp,mus=mus,Sigmas=Sigmas,props=props)\n\n\n# Kernal density \n\n# using = h ref \n\nHpi1 &lt;- Hpi(x=x)\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nHpi2 &lt;- Hpi.diag(x = x)\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nfhat.pi1 &lt;-kde(x=x,H=Hpi1) \n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\n\nWarning in quantile.default(dobs, prob = (100 - cont)/100): partial argument\nmatch of 'prob' to 'probs'\n\nfhat.pi2 &lt;-kde(x=x,H=Hpi2)\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\n\nWarning in seq.default(a[id], b[id], length = bgridsize[id]): partial argument\nmatch of 'length' to 'length.out'\n\n\nWarning in quantile.default(dobs, prob = (100 - cont)/100): partial argument\nmatch of 'prob' to 'probs'\n\nplot(fhat.pi1)\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\nplot(fhat.pi1)\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'level' to 'levels'\n\n\nWarning in contour.default(fhat$eval.points[[1]], fhat$eval.points[[2]], :\npartial argument match of 'label' to 'labels'"
  },
  {
    "objectID": "05-vector-bcdata.html",
    "href": "05-vector-bcdata.html",
    "title": "Getting B.C. Open Data with R",
    "section": "",
    "text": "The B.C. Government makes a huge amount of open data available, both spatial and non-spatial, and documents it in the B.C. Data Catalogue.\nThe {bcdata} package allows you to interact with the catalogue, and download data, directly from within R.\nWe are going to use {bcdata} to get some data for our telemetry analysis:\nlibrary(bcdata)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(mapview)"
  },
  {
    "objectID": "05-vector-bcdata.html#create-an-area-of-interest-aoi",
    "href": "05-vector-bcdata.html#create-an-area-of-interest-aoi",
    "title": "Getting B.C. Open Data with R",
    "section": "Create an Area of Interest (AOI)",
    "text": "Create an Area of Interest (AOI)\nWe’ll read in the caribou data created in the previous module, and create an area of interest (a bounding box) around it, so we can use that to spatially subset the covariate data we will be using.\n\nscott &lt;- read_sf(\"clean_data/scott_herd_subset.gpkg\")\n\nscott_bbox &lt;- st_bbox(scott)\n\n## Round to nearest 100m outside the box to align with raster grids\nscott_bbox[\"xmin\"] &lt;- floor(scott_bbox[\"xmin\"] / 100) * 100\nscott_bbox[\"ymin\"] &lt;- floor(scott_bbox[\"ymin\"] / 100) * 100\nscott_bbox[\"xmax\"] &lt;- ceiling(scott_bbox[\"xmax\"] / 100) * 100\nscott_bbox[\"ymax\"] &lt;- ceiling(scott_bbox[\"ymax\"] / 100) * 100\n\nscott_aoi &lt;- st_as_sfc(scott_bbox)\n\n\nwrite_sf(scott_aoi, file.path(\"clean_data\", \"scott_aoi.gpkg\"))\n\nFirst, let’s open the B.C. Data Catalogue in our browser:\n\nbcdc_browse()\n\nIf you search for “BEC”, one of the first hits should be the BEC Map record. If you click on the “Share” (  ) button you will get a url: https://catalogue.data.gov.bc.ca/dataset/f358a53b-ffde-4830-a325-a5a03ff672c3. The last bit of the url (f358a53b-ffde-4830-a325-a5a03ff672c3) is the unique identifier for the record, and we can use that ID to query the dataset with {bcdata}.\nLet’s find out about the record:\n\nbcdc_get_record(\"f358a53b-ffde-4830-a325-a5a03ff672c3\")\n\nB.C. Data Catalogue Record: BEC Map\nName: bec-map (ID: f358a53b-ffde-4830-a325-a5a03ff672c3)\nPermalink:\n https://catalogue.data.gov.bc.ca/dataset/f358a53b-ffde-4830-a325-a5a03ff672c3\nLicence: Open Government Licence - British Columbia\nDescription: The current and most detailed version of the approved\n corporate provincial digital Biogeoclimatic Ecosystem Classification\n (BEC) Zone/Subzone/Variant/Phase map (version 12, September 2, 2021).\n Use this version when performing GIS analysis regardless of scale.\n This mapping is deliberately extended across the ocean, lakes,\n glaciers, etc to facilitate intersection with a terrestrial landcover\n layer of your choice\nAvailable Resources (1):\n 1. WMS getCapabilities request (wms)\nAccess the full 'Resources' data frame using:\n bcdc_tidy_resources('f358a53b-ffde-4830-a325-a5a03ff672c3')\nQuery and filter this data using:\n bcdc_query_geodata('f358a53b-ffde-4830-a325-a5a03ff672c3')"
  },
  {
    "objectID": "05-vector-bcdata.html#bec",
    "href": "05-vector-bcdata.html#bec",
    "title": "Getting B.C. Open Data with R",
    "section": "BEC",
    "text": "BEC\nbcdc_query_geodata() by itself does not download the data - it retrieves the first few records and shows them to us, as well as some helpful information about the data:\n\nbcdc_query_geodata(\"f358a53b-ffde-4830-a325-a5a03ff672c3\")\n\nQuerying 'bec-map' record\n• Using collect() on this object will return 15666 features and 20 fields\n• Accessing this record requires pagination and will make 16 separate\n• requests to the WFS. See ?bcdc_options\n• At most six rows of the record are printed here\n────────────────────────────────────────────────────────────────────────────────\nSimple feature collection with 6 features and 20 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 575312.5 ymin: 1553588 xmax: 780612.5 ymax: 1668388\nProjected CRS: NAD83 / BC Albers\n# A tibble: 6 × 21\n  id          FEATURE_CLASS_SKEY ZONE  SUBZONE VARIANT PHASE NATURAL_DISTURBANCE\n  &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;              \n1 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n2 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n3 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n4 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n5 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n6 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n# ℹ 14 more variables: MAP_LABEL &lt;chr&gt;, BGC_LABEL &lt;chr&gt;, ZONE_NAME &lt;chr&gt;,\n#   SUBZONE_NAME &lt;chr&gt;, VARIANT_NAME &lt;chr&gt;, PHASE_NAME &lt;chr&gt;,\n#   NATURAL_DISTURBANCE_NAME &lt;chr&gt;, FEATURE_AREA_SQM &lt;int&gt;,\n#   FEATURE_LENGTH_M &lt;int&gt;, FEATURE_AREA &lt;int&gt;, FEATURE_LENGTH &lt;int&gt;,\n#   OBJECTID &lt;int&gt;, SE_ANNO_CAD_DATA &lt;chr&gt;, geometry &lt;POLYGON [m]&gt;\n\n\nYou can use dplyr verbs filter() and select() to cut down the amount of data you need to download from the web service. filter() can take logical predicates such as ==, &gt;, %in% etc., as well as geometry predicates such as INTERSECTS(), OVERLAPS(), WITHIN() etc. See this vignette for details.\nOnce your query is complete you call collect() to tell the server to execute the query and send you the data:\n\nbec &lt;- bcdc_query_geodata(\"f358a53b-ffde-4830-a325-a5a03ff672c3\") |&gt;\n    filter(INTERSECTS(scott_aoi)) |&gt; \n    select(MAP_LABEL) |&gt; \n    collect()\n\nmapview(bec, zcol = \"MAP_LABEL\") + mapview(st_as_sf(scott_aoi))\n\n\n\n\n\n\nYou can see that filtering using the INTERSECTS() function does a good job of only downloading the features that intersect the AOI, but it doesn’t actually clip them to the AOI. We can do that with sf::st_intersection(). Also, there are some columns that are “sticky” - even if you don’t select them in a select() statement before you call collect, they come along anyway. We can do a final select() once the data is downloaded:\n\nbec &lt;- st_intersection(bec, scott_aoi) |&gt; \n  select(MAP_LABEL)\n\nmapview(bec)\n\n\n\n\n\n\nAnd now we can write our BEC data to a file to use later in the analysis.\n\nwrite_sf(bec, file.path(\"clean_data\", \"bec.gpkg\"))"
  },
  {
    "objectID": "05-vector-bcdata.html#vri",
    "href": "05-vector-bcdata.html#vri",
    "title": "Getting B.C. Open Data with R",
    "section": "VRI",
    "text": "VRI\nSee a list of VRI codes.\n\nvri &lt;- bcdc_query_geodata(\"2ebb35d8-c82f-4a17-9c96-612ac3532d55\") |&gt; \n  filter(INTERSECTS(scott_aoi)) |&gt; \n  select(PROJ_AGE_CLASS_CD_1, BCLCS_LEVEL_4, CROWN_CLOSURE_CLASS_CD) |&gt;  \n  collect() |&gt; \n  st_intersection(scott_aoi)\n\nWe want to split this into separate files for different variables:\n\nConiferous-leading stands\nStands with age greater than 40 yrs (age class &gt;=3)\nCrown closure\n\n\n# Tree coniferous leading - select coniferous leading vri plots\nvri_conif &lt;- vri |&gt;  \n    mutate(conif = BCLCS_LEVEL_4) |&gt; \n    filter(conif == \"TC\") |&gt; \n    select(conif)\n\n\nwrite_sf(vri_conif, file.path(\"clean_data\",\"vri_conif.gpkg\"))\n\n\n# Age class greater than 40 years\nvri_ageclass &lt;- vri |&gt; \n    mutate(age_class = as.numeric(PROJ_AGE_CLASS_CD_1)) |&gt; \n    filter(age_class &gt;= 3) |&gt; \n    select(age_class)\n\n\nwrite_sf(vri_ageclass, file.path(\"clean_data\", \"vri_ageclass.gpkg\"))\n\n\n# Crown closure class \nvri_cc &lt;- vri |&gt; \n    mutate(cc_class = as.numeric(CROWN_CLOSURE_CLASS_CD)) |&gt; \n    select(cc_class)\n\n\nwrite_sf(vri_cc, file.path(\"clean_data\", \"vri_cc.gpkg\"))"
  },
  {
    "objectID": "05-vector-bcdata.html#cutblocks",
    "href": "05-vector-bcdata.html#cutblocks",
    "title": "Getting B.C. Open Data with R",
    "section": "Cutblocks",
    "text": "Cutblocks\nTo get the cutblocks, we filter to our AOI, and also choose those blocks that have a harvest year in the last 30 years.\nWe can get information about the columns in a given dataset with bcdc_describe_feature():\n\nbcdc_describe_feature(\"b1b647a6-f271-42e0-9cd0-89ec24bce9f7\")\n\n# A tibble: 13 × 5\n   col_name                sticky remote_col_type local_col_type column_comments\n   &lt;chr&gt;                   &lt;lgl&gt;  &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;          \n 1 id                      TRUE   xsd:string      character      &lt;NA&gt;           \n 2 VEG_CONSOLIDATED_CUT_B… TRUE   xsd:decimal     numeric        VEG_CONSOLIDAT…\n 3 OPENING_ID              FALSE  xsd:decimal     numeric        OPENING_ID is …\n 4 HARVEST_YEAR            FALSE  xsd:decimal     numeric        HARVEST_YEAR i…\n 5 DISTURBANCE_START_DATE  FALSE  xsd:date        date           DISTURBANCE_ST…\n 6 DISTURBANCE_END_DATE    FALSE  xsd:date        date           DISTURBANCE_EN…\n 7 DATA_SOURCE             FALSE  xsd:string      character      DATA_SOURCE is…\n 8 AREA_HA                 FALSE  xsd:decimal     numeric        AREA_HA is the…\n 9 FEATURE_AREA_SQM        FALSE  xsd:decimal     numeric        FEATURE_AREA_S…\n10 FEATURE_LENGTH_M        FALSE  xsd:decimal     numeric        FEATURE_LENGTH…\n11 SHAPE                   FALSE  gml:GeometryPr… sfc geometry   SHAPE is the c…\n12 OBJECTID                TRUE   xsd:decimal     numeric        OBJECTID is a …\n13 SE_ANNO_CAD_DATA        FALSE  xsd:hexBinary   numeric        SE_ANNO_CAD_DA…\n\ncutblocks &lt;- bcdc_query_geodata(\"b1b647a6-f271-42e0-9cd0-89ec24bce9f7\") |&gt;\n  filter(\n    INTERSECTS(scott_aoi), \n    HARVEST_YEAR &gt;= 1993) |&gt;\n  select(HARVEST_YEAR) |&gt;\n  collect() |&gt; \n  st_intersection(scott_aoi)\n\nmapview(cutblocks, zcol = \"HARVEST_YEAR\")\n\n\n\n\n\n\n\nwrite_sf(cutblocks, file.path(\"clean_data\", \"cutblocks.gpkg\"))"
  },
  {
    "objectID": "05-vector-bcdata.html#water-bodies",
    "href": "05-vector-bcdata.html#water-bodies",
    "title": "Getting B.C. Open Data with R",
    "section": "Water bodies",
    "text": "Water bodies\n\nSearching the catalogue for the BC Freshwater Atlas\nWe can search the catalogue for data using keywords, with bcdc_search(). Control the number of results returned with n.\n\nbcdc_search(\"freshwater atlas\", n = 20)\n\nList of B.C. Data Catalogue Records\nNumber of records: 20\nTitles:\n1: Freshwater Atlas Lakes (multiple, html, pdf, wms, kml)\n ID: cb1e3aba-d3fe-4de1-a2d4-b8b6650fb1f6\n Name: freshwater-atlas-lakes\n2: Freshwater Atlas Rivers (multiple, fgdb, pdf, wms, kml)\n ID: f7dac054-efbf-402f-ab62-6fc4b32a619e\n Name: freshwater-atlas-rivers\n3: Freshwater Atlas Obstructions (multiple, html, pdf, wms, kml)\n ID: 64797286-3ca5-4202-9064-a7f790321e9e\n Name: freshwater-atlas-obstructions\n4: Freshwater Atlas Watersheds (multiple, html, pdf, wms, kml, fgdb)\n ID: 3ee497c4-57d7-47f8-b030-2e0c03f8462a\n Name: freshwater-atlas-watersheds\n5: Freshwater Atlas Coastlines (multiple, html, pdf, wms, kml)\n ID: 87b1d6a7-d4d1-4c25-a879-233becdbffed\n Name: freshwater-atlas-coastlines\n6: Freshwater Atlas Glaciers (multiple, html, pdf, wms, kml)\n ID: 8f2aee65-9f4c-4f72-b54c-0937dbf3e6f7\n Name: freshwater-atlas-glaciers\n7: Freshwater Atlas Wetlands (multiple, html, pdf, wms, kml)\n ID: 93b413d8-1840-4770-9629-641d74bd1cc6\n Name: freshwater-atlas-wetlands\n8: Freshwater Atlas Islands (multiple, html, pdf, wms, kml)\n ID: 4483aeea-df26-4b83-a565-934c769e74de\n Name: freshwater-atlas-islands\n9: Freshwater Atlas Watershed Boundaries (multiple, html, pdf, wms,\n kml, fgdb)\n ID: ab758580-809d-4e11-bb2c-df02ac5465c9\n Name: freshwater-atlas-watershed-boundaries\n10: Freshwater Atlas Linear Boundaries (multiple, html, pdf, wms, kml,\n fgdb)\n ID: 2af1388e-d5f7-46dc-a6e2-f85415ddbd1c\n Name: freshwater-atlas-linear-boundaries\n11: Freshwater Atlas Watershed Groups (multiple, html, pdf, wms, kml)\n ID: 51f20b1a-ab75-42de-809d-bf415a0f9c62\n Name: freshwater-atlas-watershed-groups\n12: Freshwater Atlas Manmade Waterbodies (multiple, html, pdf, wms,\n kml)\n ID: 055fd71e-b771-4d47-a863-8a54f91a954c\n Name: freshwater-atlas-manmade-waterbodies\n13: Freshwater Atlas Named Watersheds (multiple, html, pdf, wms, kml)\n ID: ea63ea04-eab0-4b83-8729-f8a93ac688a1\n Name: freshwater-atlas-named-watersheds\n14: Freshwater Atlas Assessment Watersheds (multiple, html, pdf, wms,\n kml)\n ID: 97d8ef37-b8d2-4c3b-b772-6b25c1db13d0\n Name: freshwater-atlas-assessment-watersheds\n15: Freshwater Atlas Stream Directions (multiple, html, pdf, wms, kml)\n ID: d7165359-52ef-41d0-b762-c53e3468ff3f\n Name: freshwater-atlas-stream-directions\n16: Freshwater Atlas Stream Network (multiple, html, pdf, wms, kml,\n fgdb)\n ID: 92344413-8035-4c08-b996-65a9b3f62fca\n Name: freshwater-atlas-stream-network\n17: Freshwater Atlas Edge Type Codes (multiple, html, pdf)\n ID: 509cbf74-7ee7-44d3-a88d-4e088ea67325\n Name: freshwater-atlas-edge-type-codes\n18: Freshwater Atlas Waterbody Type Codes (multiple, html, pdf)\n ID: ade4f36a-1fd4-4583-8253-2b2a1bbe34ff\n Name: freshwater-atlas-waterbody-type-codes\n19: Freshwater Atlas Watershed Type Codes (multiple, html, pdf)\n ID: f7efa3ea-bf1c-4c4f-bb33-ba841aa076c0\n Name: freshwater-atlas-watershed-type-codes\n20: Freshwater Atlas Named Point Features (multiple, html, pdf, wms,\n kml)\n ID: db43a358-273c-4c2e-8a5c-cc28eaaffaa7\n Name: freshwater-atlas-named-point-features\n\nAccess a single record by calling `bcdc_get_record(ID)` with the ID\n from the desired record.\n\n\n\nlakes &lt;- bcdc_query_geodata(\"cb1e3aba-d3fe-4de1-a2d4-b8b6650fb1f6\") |&gt;\n  filter(INTERSECTS(scott_aoi)) |&gt;\n  select(id, WATERBODY_TYPE, AREA_HA) |&gt;\n  collect()\n\nwetlands &lt;- bcdc_query_geodata(\"93b413d8-1840-4770-9629-641d74bd1cc6\") |&gt;\n  filter(INTERSECTS(scott_aoi)) |&gt;\n  select(id, WATERBODY_TYPE, AREA_HA) |&gt;\n  collect()\n\n# Combine the datasets into one, select only the columns we want, and \n# clip to aoi\nwater &lt;- bind_rows(lakes, wetlands) |&gt; \n  select(id, WATERBODY_TYPE, AREA_HA) |&gt; \n  st_intersection(scott_aoi)\n\n\nwrite_sf(wetlands, file.path(\"clean_data\", \"water.gpkg\"))"
  },
  {
    "objectID": "05-vector-bcdata.html#stream-index",
    "href": "05-vector-bcdata.html#stream-index",
    "title": "Getting B.C. Open Data with R",
    "section": "Stream Index",
    "text": "Stream Index\n\nstreams_cols &lt;- bcdc_describe_feature(\"92344413-8035-4c08-b996-65a9b3f62fca\")\nprint(streams_cols, n = 20)\n\n# A tibble: 28 × 5\n   col_name                sticky remote_col_type local_col_type column_comments\n   &lt;chr&gt;                   &lt;lgl&gt;  &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;          \n 1 id                      TRUE   xsd:string      character       &lt;NA&gt;          \n 2 LINEAR_FEATURE_ID       TRUE   xsd:decimal     numeric        \"A unique nume…\n 3 WATERSHED_GROUP_ID      TRUE   xsd:decimal     numeric        \"An automatica…\n 4 EDGE_TYPE               TRUE   xsd:decimal     numeric        \"A 4 digit num…\n 5 BLUE_LINE_KEY           FALSE  xsd:decimal     numeric        \"Uniquely iden…\n 6 WATERSHED_KEY           FALSE  xsd:decimal     numeric        \"A key that id…\n 7 FWA_WATERSHED_CODE      FALSE  xsd:string      character      \"A 143 charact…\n 8 LOCAL_WATERSHED_CODE    FALSE  xsd:string      character      \"The 143 chara…\n 9 WATERSHED_GROUP_CODE    TRUE   xsd:string      character      \"The watershed…\n10 DOWNSTREAM_ROUTE_MEASU… FALSE  xsd:decimal     numeric        \"The distance,…\n11 LENGTH_METRE            TRUE   xsd:decimal     numeric        \"The length in…\n12 FEATURE_SOURCE          TRUE   xsd:string      character      \"The source of…\n13 GNIS_ID                 FALSE  xsd:decimal     numeric        \"The BCGNIS  (…\n14 GNIS_NAME               FALSE  xsd:string      character      \"The BCGNIS  (…\n15 LEFT_RIGHT_TRIBUTARY    FALSE  xsd:string      character      \"Describes whi…\n16 STREAM_ORDER            TRUE   xsd:decimal     numeric        \"The calculate…\n17 STREAM_MAGNITUDE        TRUE   xsd:decimal     numeric        \"The calculate…\n18 WATERBODY_KEY           FALSE  xsd:decimal     numeric        \"The waterbody…\n19 BLUE_LINE_KEY_50K       FALSE  xsd:decimal     numeric        \"The best matc…\n20 WATERSHED_CODE_50K      FALSE  xsd:string      character      \"The hierarchi…\n# ℹ 8 more rows\n\n\nLet’s use the STREAM_ORDER column to just get streams of order 3 and 4 and still intersect with our AOI.\n\nstreams &lt;- bcdc_query_geodata(\"92344413-8035-4c08-b996-65a9b3f62fca\") |&gt;\n  filter(\n    INTERSECTS(scott_aoi), \n    STREAM_ORDER &gt;= 3\n  ) |&gt;\n  select(id, STREAM_ORDER) |&gt;\n  collect() |&gt; \n  select(id, STREAM_ORDER) |&gt;\n  st_zm() |&gt; \n  st_intersection(scott_aoi)\n\nmapview(streams)\n\n\n\n\n\n\n\nwrite_sf(streams, file.path(\"clean_data\", \"streams.gpkg\"))"
  },
  {
    "objectID": "05-vector-bcdata.html#roads",
    "href": "05-vector-bcdata.html#roads",
    "title": "Getting B.C. Open Data with R",
    "section": "Roads",
    "text": "Roads\n\nroads &lt;- bcdc_query_geodata(\"bb060417-b6e6-4548-b837-f9060d94743e\") |&gt; \n  filter(INTERSECTS(scott_aoi))  |&gt; \n  select(id, ROAD_CLASS, ROAD_SURFACE) |&gt; \n  collect() |&gt; \n  select(ROAD_SURFACE, ROAD_CLASS) |&gt; \n  st_intersection(scott_aoi) |&gt; # clip roads so all inside aoi\n  st_cast(\"MULTILINESTRING\")\n\n\nwrite_sf(roads, file.path(\"clean_data\", \"roads.gpkg\"))"
  },
  {
    "objectID": "07-processing-vector-data.html",
    "href": "07-processing-vector-data.html",
    "title": "Processing Base Vector Data",
    "section": "",
    "text": "In this module we will post-process the vector data we prepared earlier to generate some additional covariates using the terra package. We will also prepare all data for the analysis. This includes\n\ngenerating a road density layer\ncalculating distance to water metrics\nconverting our prepared vector to raster\nbuilding a raster stack with all covariate data"
  },
  {
    "objectID": "07-processing-vector-data.html#overview",
    "href": "07-processing-vector-data.html#overview",
    "title": "Processing Base Vector Data",
    "section": "",
    "text": "In this module we will post-process the vector data we prepared earlier to generate some additional covariates using the terra package. We will also prepare all data for the analysis. This includes\n\ngenerating a road density layer\ncalculating distance to water metrics\nconverting our prepared vector to raster\nbuilding a raster stack with all covariate data"
  },
  {
    "objectID": "07-processing-vector-data.html#calculate-density-of-road-network.",
    "href": "07-processing-vector-data.html#calculate-density-of-road-network.",
    "title": "Processing Base Vector Data",
    "section": "1. Calculate density of road network.",
    "text": "1. Calculate density of road network.\nOften we want to post-process our raw vector data to create more meaningful covariates in our analysis. In this case we are interested in the density of roads and how this might impact our caribou habitat choice.\n\n# load in the libraries needed\n\nlibrary(bcdata)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(terra)\n\n# read in raster template\ntemplate &lt;- rast(file.path(\"clean_data\", \"template.tif\"))\n\nWe can assume that the type of road will have a different impact on Caribou avoidance or predator use. We can assign a value based on the road type. In this case we will assign a higher value to roads used more frequently.\n\n# read in roads\nroads &lt;- read_sf(file.path(\"clean_data\", \"roads.gpkg\"))\n\n# assign a value to the roads based on estimated speed of travel or use\n\nroads &lt;- roads %&gt;% \n  mutate(rd_value = case_when(\n            ROAD_SURFACE == \"loose\" ~ 25,\n            ROAD_SURFACE == \"overgrown\" ~ 5,\n            ROAD_SURFACE == \"rough\" ~ 10,\n            ROAD_SURFACE == \"unknown\" ~ 7.5)) \n\nNow we can convert this to a raster using the template we created already\n\n# convert roads to a raster \nrroads &lt;- rasterize(roads, template, field = \"rd_value\" )\n\nplot(rroads)\n\n\n\n\nAs the impact of the roads is likely to impact a wider influence than a single pixal we can use a moving window analysis to expand the influence of roads for a given value.\n\n# create a moving window \nrrdens &lt;- focal(rroads, w=9, fun=\"sum\", na.rm=TRUE)\n\nplot(rrdens)\n\n\n\n\n\nwriteRaster(rrdens, file.path(\"clean_data\", \"road_density.tif\"), overwrite = TRUE)"
  },
  {
    "objectID": "07-processing-vector-data.html#calculate-distance-from-water",
    "href": "07-processing-vector-data.html#calculate-distance-from-water",
    "title": "Processing Base Vector Data",
    "section": "2. Calculate distance from water",
    "text": "2. Calculate distance from water\nWhile density provides a measure of impact we may also be interested in capturing the distance of fixes to caribou points. In this case we will explore the distance to water bodies within the study areas.\n\n# read in water \nwater &lt;- read_sf(file.path(\"clean_data\", \"water.gpkg\"))\n\n# calculate the distance to water for each pixel in the raster\n# be patient - this might take some time. \nwater_dis &lt;- distance(template, water, unit = \"km\")\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\nplot(water_dis)\n\n\n\n# what sort of values are we seeing?\nrange(water_dis)\n\nclass       : SpatRaster \ndimensions  : 1484, 1616, 2  (nrow, ncol, nlyr)\nresolution  : 25, 25  (x, y)\nextent      : 1145200, 1185600, 1190400, 1227500  (xmin, xmax, ymin, ymax)\ncoord. ref. : NAD83 / BC Albers (EPSG:3005) \nsource(s)   : memory\nnames       : range_min, range_max \nmin values  :  0.000000,  0.000000 \nmax values  :  8.300147,  8.300147 \n\nsort(unique(values(water_dis)))\n\n   [1] 0.000000e+00 8.354639e-05 1.793377e-04 3.220093e-04 4.024488e-04\n   [6] 4.224121e-04 5.609028e-04 5.714237e-04 6.075566e-04 6.288163e-04\n  [11] 6.344265e-04 6.564031e-04 7.123651e-04 7.503013e-04 7.557943e-04\n  [16] 7.702110e-04 7.889899e-04 8.578584e-04 8.730006e-04 8.744055e-04\n  [21] 8.839870e-04 8.920252e-04 1.009363e-03 1.033941e-03 1.045970e-03\n  [26] 1.053091e-03 1.054326e-03 1.055127e-03 1.098221e-03 1.140004e-03\n  [31] 1.147728e-03 1.172200e-03 1.244563e-03 1.271417e-03 1.299894e-03\n  [36] 1.304980e-03 1.333810e-03 1.335460e-03 1.353876e-03 1.373854e-03\n  [41] 1.377376e-03 1.412947e-03 1.415191e-03 1.427648e-03 1.489975e-03\n  [46] 1.490684e-03 1.507176e-03 1.509589e-03 1.544592e-03 1.548663e-03\n  [51] 1.562744e-03 1.581749e-03 1.600235e-03 1.603881e-03 1.619275e-03\n  [56] 1.696763e-03 1.697000e-03 1.711516e-03 1.726444e-03 1.732980e-03\n  [61] 1.749876e-03 1.751582e-03 1.758986e-03 1.771955e-03 1.777293e-03\n  [66] 1.784081e-03 1.797911e-03 1.798423e-03 1.816746e-03 1.824902e-03\n  [71] 1.828142e-03 1.830018e-03 1.863127e-03 1.883328e-03 1.908015e-03\n  [76] 1.942268e-03 1.946987e-03 1.958886e-03 1.965749e-03 1.994803e-03\n  [81] 2.017419e-03 2.023637e-03 2.038138e-03 2.059626e-03 2.072752e-03\n  [86] 2.083797e-03 2.095360e-03 2.137215e-03 2.157780e-03 2.165196e-03\n  [91] 2.184712e-03 2.193500e-03 2.195624e-03 2.199508e-03 2.200848e-03\n  [96] 2.207899e-03 2.228489e-03 2.229763e-03 2.233271e-03 2.243064e-03\n [101] 2.243244e-03 2.257163e-03 2.265657e-03 2.268274e-03 2.281968e-03\n [106] 2.282548e-03 2.294612e-03 2.300563e-03 2.319897e-03 2.326579e-03\n [111] 2.356661e-03 2.366370e-03 2.373706e-03 2.391651e-03 2.401961e-03\n [116] 2.407914e-03 2.419101e-03 2.440518e-03 2.447606e-03 2.462102e-03\n [121] 2.466399e-03 2.472272e-03 2.475359e-03 2.492332e-03 2.509293e-03\n [126] 2.515053e-03 2.519773e-03 2.525552e-03 2.532133e-03 2.552744e-03\n [131] 2.566130e-03 2.569193e-03 2.579917e-03 2.594811e-03 2.604658e-03\n [136] 2.604796e-03 2.621392e-03 2.632892e-03 2.638829e-03 2.644531e-03\n [141] 2.651217e-03 2.653218e-03 2.658125e-03 2.658895e-03 2.662570e-03\n [146] 2.663941e-03 2.690360e-03 2.692390e-03 2.699568e-03 2.716192e-03\n [151] 2.725308e-03 2.734154e-03 2.736955e-03 2.747360e-03 2.747803e-03\n [156] 2.752704e-03 2.767192e-03 2.771250e-03 2.771352e-03 2.783107e-03\n [161] 2.790617e-03 2.791484e-03 2.817611e-03 2.820192e-03 2.838268e-03\n [166] 2.845500e-03 2.849823e-03 2.868013e-03 2.868419e-03 2.874502e-03\n [171] 2.874896e-03 2.880615e-03 2.889031e-03 2.893345e-03 2.895117e-03\n [176] 2.903460e-03 2.951179e-03 2.970401e-03 2.985316e-03 2.993279e-03\n [181] 2.997519e-03 2.997917e-03 3.002666e-03 3.006783e-03 3.007862e-03\n [186] 3.010211e-03 3.018968e-03 3.023598e-03 3.035654e-03 3.041677e-03\n [191] 3.063402e-03 3.063887e-03 3.064291e-03 3.069804e-03 3.080199e-03\n [196] 3.090484e-03 3.111444e-03 3.111809e-03 3.113649e-03 3.122893e-03\n [201] 3.124367e-03 3.125150e-03 3.128182e-03 3.136189e-03 3.142163e-03\n [206] 3.144322e-03 3.147241e-03 3.148177e-03 3.156844e-03 3.169712e-03\n [211] 3.194259e-03 3.199128e-03 3.202962e-03 3.222417e-03 3.240783e-03\n [216] 3.241319e-03 3.243937e-03 3.246958e-03 3.252205e-03 3.255983e-03\n [221] 3.270986e-03 3.277836e-03 3.279085e-03 3.302005e-03 3.310134e-03\n [226] 3.318525e-03 3.330970e-03 3.336805e-03 3.342081e-03 3.347872e-03\n [231] 3.353258e-03 3.359136e-03 3.365397e-03 3.371317e-03 3.378416e-03\n [236] 3.383850e-03 3.388801e-03 3.394205e-03 3.417285e-03 3.420898e-03\n [241] 3.433480e-03 3.434105e-03 3.437936e-03 3.459021e-03 3.462029e-03\n [246] 3.470122e-03 3.479474e-03 3.486615e-03 3.493009e-03 3.493487e-03\n [251] 3.507147e-03 3.515921e-03 3.538122e-03 3.540929e-03 3.546491e-03\n [256] 3.547894e-03 3.556438e-03 3.562731e-03 3.584079e-03 3.588258e-03\n [261] 3.590476e-03 3.613673e-03 3.622335e-03 3.625056e-03 3.659484e-03\n [266] 3.664595e-03 3.670111e-03 3.673852e-03 3.678151e-03 3.684249e-03\n [271] 3.696326e-03 3.703199e-03 3.716291e-03 3.726162e-03 3.726189e-03\n [276] 3.726559e-03 3.732010e-03 3.733591e-03 3.736480e-03 3.739752e-03\n [281] 3.747666e-03 3.757735e-03 3.766695e-03 3.782857e-03 3.797917e-03\n [286] 3.798943e-03 3.799326e-03 3.812694e-03 3.820500e-03 3.821368e-03\n [291] 3.834968e-03 3.850022e-03 3.854158e-03 3.855806e-03 3.860219e-03\n [296] 3.861653e-03 3.862991e-03 3.865464e-03 3.867229e-03 3.870314e-03\n [301] 3.875855e-03 3.886846e-03 3.889548e-03 3.893839e-03 3.897951e-03\n [306] 3.900096e-03 3.910924e-03 3.914663e-03 3.916752e-03 3.920419e-03\n [311] 3.921395e-03 3.929867e-03 3.929880e-03 3.930402e-03 3.933165e-03\n [316] 3.933635e-03 3.954659e-03 3.959268e-03 3.960532e-03 3.964720e-03\n [321] 3.971234e-03 3.973511e-03 3.981543e-03 3.983091e-03 3.983737e-03\n [326] 3.987534e-03 3.996677e-03 3.997315e-03 4.009712e-03 4.010660e-03\n [331] 4.014388e-03 4.014889e-03 4.034883e-03 4.038856e-03 4.042111e-03\n [336] 4.050406e-03 4.071585e-03 4.074377e-03 4.077882e-03 4.091428e-03\n [341] 4.092184e-03 4.101040e-03 4.102097e-03 4.109924e-03 4.120061e-03\n [346] 4.126378e-03 4.129176e-03 4.129625e-03 4.129721e-03 4.139676e-03\n [351] 4.170028e-03 4.170557e-03 4.171197e-03 4.172227e-03 4.184331e-03\n [356] 4.188188e-03 4.206571e-03 4.206856e-03 4.208908e-03 4.210354e-03\n [361] 4.227391e-03 4.235904e-03 4.237400e-03 4.260735e-03 4.265175e-03\n [366] 4.278380e-03 4.283647e-03 4.286276e-03 4.299639e-03 4.306281e-03\n [371] 4.307327e-03 4.320317e-03 4.326617e-03 4.328133e-03 4.330663e-03\n [376] 4.340342e-03 4.343630e-03 4.359019e-03 4.372601e-03 4.375643e-03\n [381] 4.388237e-03 4.389198e-03 4.407658e-03 4.411906e-03 4.412936e-03\n [386] 4.415641e-03 4.416461e-03 4.420811e-03 4.430859e-03 4.431489e-03\n [391] 4.432926e-03 4.442103e-03 4.443537e-03 4.444556e-03 4.448439e-03\n [396] 4.452714e-03 4.462208e-03 4.474372e-03 4.474574e-03 4.486140e-03\n [401] 4.491237e-03 4.493072e-03 4.493891e-03 4.499716e-03 4.499903e-03\n [406] 4.501368e-03 4.525541e-03 4.539310e-03 4.544459e-03 4.550125e-03\n [411] 4.569006e-03 4.571772e-03 4.580308e-03 4.580466e-03 4.591896e-03\n [416] 4.595098e-03 4.629865e-03 4.632274e-03 4.633439e-03 4.637526e-03\n [421] 4.640692e-03 4.654033e-03 4.664715e-03 4.665893e-03 4.668765e-03\n [426] 4.674487e-03 4.677483e-03 4.681605e-03 4.687017e-03 4.687048e-03\n [431] 4.694230e-03 4.701868e-03 4.702362e-03 4.702479e-03 4.708167e-03\n [436] 4.716630e-03 4.721616e-03 4.737822e-03 4.754409e-03 4.754991e-03\n [441] 4.755112e-03 4.757641e-03 4.772763e-03 4.779536e-03 4.788908e-03\n [446] 4.798245e-03 4.804099e-03 4.812589e-03 4.816322e-03 4.817939e-03\n [451] 4.829073e-03 4.829959e-03 4.846631e-03 4.864753e-03 4.865155e-03\n [456] 4.875025e-03 4.875500e-03 4.883371e-03 4.886790e-03 4.921058e-03\n [461] 4.921883e-03 4.926136e-03 4.931444e-03 4.931534e-03 4.934348e-03\n [466] 4.935194e-03 4.946282e-03 4.946850e-03 4.962570e-03 4.963935e-03\n [471] 4.964760e-03 4.977622e-03 4.989801e-03 4.995859e-03 5.000693e-03\n [476] 5.002676e-03 5.003470e-03 5.005444e-03 5.006569e-03 5.007010e-03\n [481] 5.016636e-03 5.018864e-03 5.023911e-03 5.028331e-03 5.033630e-03\n [486] 5.042306e-03 5.053342e-03 5.054466e-03 5.063227e-03 5.075914e-03\n [491] 5.080303e-03 5.084856e-03 5.086594e-03 5.092584e-03 5.104422e-03\n [496] 5.107407e-03 5.116234e-03 5.119938e-03 5.127084e-03 5.132095e-03\n [501] 5.135058e-03 5.138009e-03 5.145247e-03 5.147774e-03 5.154867e-03\n [506] 5.156467e-03 5.156498e-03 5.167759e-03 5.172478e-03 5.172593e-03\n [511] 5.192407e-03 5.192771e-03 5.201498e-03 5.207882e-03 5.233647e-03\n [516] 5.242128e-03 5.248218e-03 5.249958e-03 5.251565e-03 5.252594e-03\n [521] 5.253464e-03 5.256126e-03 5.263123e-03 5.269066e-03 5.271495e-03\n [526] 5.272617e-03 5.273448e-03 5.279970e-03 5.288764e-03 5.293299e-03\n [531] 5.294304e-03 5.305228e-03 5.305794e-03 5.316781e-03 5.317245e-03\n [536] 5.326781e-03 5.338802e-03 5.355155e-03 5.359647e-03 5.361585e-03\n [541] 5.365754e-03 5.372943e-03 5.376568e-03 5.378194e-03 5.386397e-03\n [546] 5.395164e-03 5.410261e-03 5.415049e-03 5.419007e-03 5.420604e-03\n [551] 5.421561e-03 5.434080e-03 5.438715e-03 5.455568e-03 5.457534e-03\n [556] 5.459840e-03 5.486379e-03 5.497892e-03 5.502652e-03 5.502828e-03\n [561] 5.504778e-03 5.509835e-03 5.516308e-03 5.528804e-03 5.529845e-03\n [566] 5.531652e-03 5.542693e-03 5.543705e-03 5.547208e-03 5.548225e-03\n [571] 5.549682e-03 5.550677e-03 5.553966e-03 5.554029e-03 5.557330e-03\n [576] 5.573477e-03 5.586591e-03 5.594544e-03 5.596005e-03 5.598295e-03\n [581] 5.603531e-03 5.611717e-03 5.619624e-03 5.621065e-03 5.628705e-03\n [586] 5.629540e-03 5.635641e-03 5.636159e-03 5.638487e-03 5.642061e-03\n [591] 5.644220e-03 5.646539e-03 5.653770e-03 5.656074e-03 5.656178e-03\n [596] 5.660141e-03 5.667342e-03 5.679695e-03 5.687433e-03 5.691241e-03\n [601] 5.692591e-03 5.694889e-03 5.696151e-03 5.700014e-03 5.702405e-03\n [606] 5.702750e-03 5.703097e-03 5.724378e-03 5.726828e-03 5.736788e-03\n [611] 5.745819e-03 5.753113e-03 5.760085e-03 5.761195e-03 5.800370e-03\n [616] 5.808646e-03 5.811319e-03 5.816583e-03 5.825194e-03 5.832552e-03\n [621] 5.836536e-03 5.838528e-03 5.838925e-03 5.840812e-03 5.843527e-03\n [626] 5.847588e-03 5.854760e-03 5.855551e-03 5.863455e-03 5.875914e-03\n [631] 5.887930e-03 5.890769e-03 5.903275e-03 5.903594e-03 5.904011e-03\n [636] 5.906464e-03 5.911570e-03 5.919014e-03 5.924008e-03 5.929692e-03\n [641] 5.929719e-03 5.932041e-03 5.941347e-03 5.948219e-03 5.956917e-03\n [646] 5.967609e-03 5.968280e-03 5.972148e-03 5.979448e-03 5.988307e-03\n [651] 5.993519e-03 5.995633e-03 6.009366e-03 6.009944e-03 6.019413e-03\n [656] 6.024152e-03 6.026232e-03 6.033057e-03 6.035777e-03 6.037608e-03\n [661] 6.040878e-03 6.043685e-03 6.058144e-03 6.064370e-03 6.068721e-03\n [666] 6.072079e-03 6.074250e-03 6.077855e-03 6.079600e-03 6.079886e-03\n [671] 6.086333e-03 6.089765e-03 6.109800e-03 6.111919e-03 6.117663e-03\n [676] 6.120467e-03 6.121373e-03 6.122090e-03 6.125359e-03 6.128663e-03\n [681] 6.131502e-03 6.131809e-03 6.135002e-03 6.135716e-03 6.145981e-03\n [686] 6.146111e-03 6.146616e-03 6.152522e-03 6.153142e-03 6.160240e-03\n [691] 6.160823e-03 6.166398e-03 6.173531e-03 6.181670e-03 6.182679e-03\n [696] 6.185501e-03 6.198459e-03 6.207820e-03 6.213552e-03 6.215717e-03\n [701] 6.216397e-03 6.222170e-03 6.225450e-03 6.227248e-03 6.230736e-03\n [706] 6.233973e-03 6.239352e-03 6.261736e-03 6.267179e-03 6.293642e-03\n [711] 6.303046e-03 6.308030e-03 6.308450e-03 6.309780e-03 6.313276e-03\n [716] 6.321679e-03 6.325931e-03 6.333675e-03 6.336632e-03 6.337793e-03\n [721] 6.339010e-03 6.339742e-03 6.341742e-03 6.344625e-03 6.346180e-03\n [726] 6.348110e-03 6.353173e-03 6.362796e-03 6.369266e-03 6.370257e-03\n [731] 6.371161e-03 6.379339e-03 6.388196e-03 6.390939e-03 6.402470e-03\n [736] 6.405701e-03 6.414791e-03 6.421667e-03 6.425545e-03 6.432177e-03\n [741] 6.434633e-03 6.435975e-03 6.446992e-03 6.448525e-03 6.451143e-03\n [746] 6.457796e-03 6.460698e-03 6.465472e-03 6.469784e-03 6.470558e-03\n [751] 6.478557e-03 6.482097e-03 6.490151e-03 6.499970e-03 6.502250e-03\n [756] 6.503422e-03 6.505117e-03 6.512472e-03 6.518337e-03 6.518882e-03\n [761] 6.519480e-03 6.523834e-03 6.528698e-03 6.537941e-03 6.540419e-03\n [766] 6.544824e-03 6.546601e-03 6.549276e-03 6.551261e-03 6.553332e-03\n [771] 6.566197e-03 6.567777e-03 6.570223e-03 6.570261e-03 6.570650e-03\n [776] 6.571472e-03 6.575019e-03 6.575850e-03 6.584042e-03 6.585859e-03\n [781] 6.591674e-03 6.593350e-03 6.596046e-03 6.597836e-03 6.615148e-03\n [786] 6.631314e-03 6.632460e-03 6.639149e-03 6.639655e-03 6.641649e-03\n [791] 6.642952e-03 6.650245e-03 6.650335e-03 6.655581e-03 6.661900e-03\n [796] 6.670139e-03 6.680212e-03 6.689814e-03 6.694441e-03 6.700264e-03\n [801] 6.703687e-03 6.705405e-03 6.705588e-03 6.711568e-03 6.717603e-03\n [806] 6.719842e-03 6.726440e-03 6.728682e-03 6.740810e-03 6.755425e-03\n [811] 6.756204e-03 6.772967e-03 6.784129e-03 6.791321e-03 6.794375e-03\n [816] 6.799471e-03 6.800284e-03 6.805160e-03 6.810341e-03 6.812335e-03\n [821] 6.818400e-03 6.820041e-03 6.821388e-03 6.825063e-03 6.846836e-03\n [826] 6.847765e-03 6.852849e-03 6.856924e-03 6.862610e-03 6.862668e-03\n [831] 6.879293e-03 6.881389e-03 6.885650e-03 6.891680e-03 6.895520e-03\n [836] 6.900134e-03 6.906680e-03 6.906979e-03 6.914652e-03 6.915172e-03\n [841] 6.917122e-03 6.917592e-03 6.918726e-03 6.936038e-03 6.936902e-03\n [846] 6.936945e-03 6.938281e-03 6.938419e-03 6.954272e-03 6.961605e-03\n [851] 6.967464e-03 6.968449e-03 6.973530e-03 6.987675e-03 6.988432e-03\n [856] 6.991744e-03 6.993030e-03 6.997985e-03 7.002068e-03 7.006521e-03\n [861] 7.020000e-03 7.025589e-03 7.027044e-03 7.042377e-03 7.046225e-03\n [866] 7.056717e-03 7.057602e-03 7.061737e-03 7.062236e-03 7.063946e-03\n [871] 7.065564e-03 7.071559e-03 7.071817e-03 7.078636e-03 7.080037e-03\n [876] 7.092354e-03 7.095065e-03 7.096544e-03 7.103163e-03 7.103922e-03\n [881] 7.108508e-03 7.119262e-03 7.126635e-03 7.128815e-03 7.137040e-03\n [886] 7.147906e-03 7.155420e-03 7.174085e-03 7.175783e-03 7.187558e-03\n [891] 7.189155e-03 7.191035e-03 7.197557e-03 7.202090e-03 7.208076e-03\n [896] 7.213772e-03 7.228349e-03 7.231527e-03 7.232217e-03 7.233614e-03\n [901] 7.235598e-03 7.243967e-03 7.244984e-03 7.256360e-03 7.271464e-03\n [906] 7.276991e-03 7.277210e-03 7.283058e-03 7.291282e-03 7.300223e-03\n [911] 7.307113e-03 7.311529e-03 7.316802e-03 7.319237e-03 7.340081e-03\n [916] 7.340439e-03 7.342483e-03 7.346702e-03 7.346936e-03 7.347403e-03\n [921] 7.382062e-03 7.386048e-03 7.390494e-03 7.399022e-03 7.399154e-03\n [926] 7.409632e-03 7.413118e-03 7.416084e-03 7.427206e-03 7.429620e-03\n [931] 7.442337e-03 7.451181e-03 7.453462e-03 7.454029e-03 7.463462e-03\n [936] 7.463603e-03 7.465653e-03 7.473028e-03 7.490374e-03 7.490666e-03\n [941] 7.496219e-03 7.504434e-03 7.508024e-03 7.513508e-03 7.515179e-03\n [946] 7.516536e-03 7.517366e-03 7.541369e-03 7.552679e-03 7.552778e-03\n [951] 7.556866e-03 7.565245e-03 7.565560e-03 7.566417e-03 7.570643e-03\n [956] 7.575738e-03 7.576890e-03 7.578585e-03 7.584605e-03 7.586912e-03\n [961] 7.589987e-03 7.599528e-03 7.602529e-03 7.604165e-03 7.606094e-03\n [966] 7.615647e-03 7.619632e-03 7.631057e-03 7.636951e-03 7.639892e-03\n [971] 7.640367e-03 7.640839e-03 7.652645e-03 7.660468e-03 7.662504e-03\n [976] 7.663872e-03 7.673625e-03 7.678838e-03 7.681175e-03 7.689035e-03\n [981] 7.691654e-03 7.702696e-03 7.703207e-03 7.705329e-03 7.705458e-03\n [986] 7.706295e-03 7.713302e-03 7.713857e-03 7.715643e-03 7.717300e-03\n [991] 7.717619e-03 7.721444e-03 7.722438e-03 7.725941e-03 7.738075e-03\n [996] 7.741438e-03 7.744331e-03 7.765709e-03 7.771911e-03 7.777849e-03\n [ reached getOption(\"max.print\") -- omitted 2392572 entries ]\n\nrange(unique(values(water_dis)))\n\n[1] 0.000000 8.300147"
  },
  {
    "objectID": "07-processing-vector-data.html#convert-base-vector-data-to-rasters",
    "href": "07-processing-vector-data.html#convert-base-vector-data-to-rasters",
    "title": "Processing Base Vector Data",
    "section": "3. Convert base vector data to rasters",
    "text": "3. Convert base vector data to rasters\nWe can also convert each vector layer we extracted into a raster.\nNote this can also be done at the vector stage, however predictive modelling will normally require a stacked covariate list so that you can predict from the built model\n\nbec &lt;- read_sf(file.path(\"clean_data\",\"bec.gpkg\"))\nvri_cc &lt;- read_sf(file.path(\"clean_data\",\"vri_cc.gpkg\"))\nvri_conif &lt;- read_sf(file.path(\"clean_data\",\"vri_conif.gpkg\"))\nvri_ageclass &lt;- read_sf(file.path(\"clean_data\",\"vri_ageclass.gpkg\"))\n#water\n#roads \n#cutblocks\n\n\n\n\n# convert to rasters\nvri_cc &lt;- rasterize(vri_cc, template, field = \"cc_class\" )\nvri_conif &lt;- rasterize(vri_conif, template, field = \"conif\" )\nvri_ageclass &lt;- rasterize(vri_ageclass, template, field = \"age_class\" )\n\n\n# stack into a set of rasters\nvect_stack &lt;- c(vri_cc, vri_conif, vri_ageclass, water_dis, rrdens)"
  },
  {
    "objectID": "07-processing-vector-data.html#lets-combine-our-already-created-rasters",
    "href": "07-processing-vector-data.html#lets-combine-our-already-created-rasters",
    "title": "Processing Base Vector Data",
    "section": "Lets combine our already created rasters",
    "text": "Lets combine our already created rasters\n\nrslope &lt;- rast(file.path(\"clean_data\",\"slope.tif\"))\naspect &lt;- rast(file.path(\"clean_data\",\"aspect.tif\"))\ntri  &lt;- rast(file.path(\"clean_data\",\"tri.tif\"))\ntrim_3005 &lt;- rast(file.path(\"clean_data\",\"dem.tif\"))\n\n\n\n# create a raster stack\nrast_stack &lt;- c(trim_3005, rslope, aspect, tri)"
  },
  {
    "objectID": "07-processing-vector-data.html#combine-raster-layers-into-a-stack",
    "href": "07-processing-vector-data.html#combine-raster-layers-into-a-stack",
    "title": "Processing Base Vector Data",
    "section": "4. Combine raster layers into a stack",
    "text": "4. Combine raster layers into a stack\n\n# lets combine both the raster stack and the vector stack \n\nall_stack &lt;- c(vect_stack, rast_stack)\n\nplot(all_stack)\n\n\n\n# we can write this out as a tif (raster object) \n#writeRaster(all_stack, file.path(\"clean_data\", \"rstack.tif\"), overwrite = T)\n\nOr we can write this out to a very small R object\n\nsaveRDS(all_stack, file.path(\"clean_data\", \"covars.RDS\")) # much faster"
  },
  {
    "objectID": "01-intro-motivation.html#welcome",
    "href": "01-intro-motivation.html#welcome",
    "title": "Introduction and Motivation",
    "section": "Welcome",
    "text": "Welcome"
  },
  {
    "objectID": "01-intro-motivation.html#agenda---day-1",
    "href": "01-intro-motivation.html#agenda---day-1",
    "title": "Introduction and Motivation",
    "section": "Agenda - Day 1",
    "text": "Agenda - Day 1\n\n\nSetup and troubleshooting (8:30 - 9:00)\nWorking with spatial data in R (9:00 - 10:30)\nBREAK (10:30 - 11:00)\nTelemetry data in R; clean, QA, and prepare data (Part I; 11:00 - 12:30)\nLUNCH (12:30 - 1:30)\nTelemetry data in R; clean, QA, and prepare data (Part II; 1:30 - 3:00)\nBREAK (3:00 - 3:30)\nVisualizing spatial data in R (3:30 - 4:30)"
  },
  {
    "objectID": "01-intro-motivation.html#agenda---day-2",
    "href": "01-intro-motivation.html#agenda---day-2",
    "title": "Introduction and Motivation",
    "section": "Agenda - Day 2",
    "text": "Agenda - Day 2\n\n\nSetup and troubleshooting (8:30 - 9:00)\nRetrieving spatial data from the B.C. Data Catalogue (9:00 - 10:30)\nBREAK (10:30 - 11:00)\nGetting and working with raster data (Part I; 11:00 - 12:30)\nLUNCH (12:30 - 1:30)\nPreparing base data for telemetry analysis (Part II; 1:30 - 3:00)\nBREAK (3:00 - 3:30)\nPreparing data for Resource Selection Function (RSF) analysis (3:30 - 4:30)"
  },
  {
    "objectID": "01-intro-motivation.html#code-of-conduct",
    "href": "01-intro-motivation.html#code-of-conduct",
    "title": "Introduction and Motivation",
    "section": "Code of Conduct",
    "text": "Code of Conduct"
  },
  {
    "objectID": "01-intro-motivation.html#housekeeping",
    "href": "01-intro-motivation.html#housekeeping",
    "title": "Introduction and Motivation",
    "section": "Housekeeping",
    "text": "Housekeeping\nWiFi:\nWashrooms:\nEtherpad: https://etherpad.andyteucher.ca/p/r-telemetry-pg"
  },
  {
    "objectID": "01-intro-motivation.html#motivation",
    "href": "01-intro-motivation.html#motivation",
    "title": "Introduction and Motivation",
    "section": "Motivation",
    "text": "Motivation\nThe Data Science Workflow"
  },
  {
    "objectID": "01-intro-motivation.html#motivation-1",
    "href": "01-intro-motivation.html#motivation-1",
    "title": "Introduction and Motivation",
    "section": "Motivation",
    "text": "Motivation\nSpatial Data Science with a GUI"
  },
  {
    "objectID": "01-intro-motivation.html#motivation-2",
    "href": "01-intro-motivation.html#motivation-2",
    "title": "Introduction and Motivation",
    "section": "Motivation",
    "text": "Motivation\nSpatial Data Science with R\n\n\n\nWorkshop Home"
  }
]