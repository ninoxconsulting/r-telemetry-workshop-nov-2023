[
  {
    "objectID": "04-spatial-data-viz.html",
    "href": "04-spatial-data-viz.html",
    "title": "Visualization of Spatial Data",
    "section": "",
    "text": "Quick recap of plot()\n\nPretty (and useful) maps with ggplot2\n\nVisualizing point density\nVisualize individual animal movement"
  },
  {
    "objectID": "04-spatial-data-viz.html#outline",
    "href": "04-spatial-data-viz.html#outline",
    "title": "Visualization of Spatial Data",
    "section": "",
    "text": "Quick recap of plot()\n\nPretty (and useful) maps with ggplot2\n\nVisualizing point density\nVisualize individual animal movement"
  },
  {
    "objectID": "04-spatial-data-viz.html#plot-sf-objects",
    "href": "04-spatial-data-viz.html#plot-sf-objects",
    "title": "Visualization of Spatial Data",
    "section": "\nplot sf objects",
    "text": "plot sf objects\nPreviously we used plot() to view our sf objects - this is a nice quick way to visualize our data for verification or glance at the different variables.\nFirst, read in the caribou data:\n\nlibrary(sf)\nlibrary(dplyr)\n\ncaribou &lt;- read_sf(\"clean_data/caribou.gpkg\")\n\nplot(caribou)\n\n\n\n\nYou can see that by default, this plots all of the attributes (up to 9). But we can plot just the points by extracting just the geometry with st_geometry():\n\nst_geometry(caribou) |&gt; \n  plot()\n\n\n\n\nOr, plot just one or a few variables of interest:\n\ncaribou |&gt; \n  select(herd, animal.sex) |&gt; # select just the herd and sex columns\n  plot(key.pos = 1) # put the legend at the bottom\n\n\n\n\nWe can also add a base map for context. Let’s use the bcmaps package to get a base map. bcmaps has a large collection of useful maps of B.C.:\n\nlibrary(bcmaps)\n\navailable_layers()\n\n# A tibble: 37 × 6\n   layer_name                title         record resource using_shortcuts local\n * &lt;chr&gt;                     &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;    &lt;lgl&gt;           &lt;lgl&gt;\n 1 airzones                  British Colu… e8eee… c495d08… TRUE            FALSE\n 2 bc_bound                  BC Boundary   b9bd9… f591698… FALSE           FALSE\n 3 bc_bound_hres             High Resolut… 30aeb… 3d72cf3… FALSE           FALSE\n 4 bc_cities                 BC Major Cit… b678c… 443dd85… TRUE            FALSE\n 5 bc_neighbours             Boundary of … b9bd9… f591698… FALSE           FALSE\n 6 bec                       British Colu… f358a… 3ec24cb… TRUE            FALSE\n 7 cded_raster               Get Canadian… &lt;NA&gt;   &lt;NA&gt;     FALSE           FALSE\n 8 cded_stars                Get Canadian… &lt;NA&gt;   &lt;NA&gt;     FALSE           FALSE\n 9 census_dissemination_area Current Cens… a091f… a7fa66d… TRUE            FALSE\n10 census_division           Current Cens… ef179… 36b530c… TRUE            FALSE\n# ℹ 27 more rows\n\n------------------------\nAll layers are downloaded from the internet and cached\non your hard drive at /Users/andy/Library/Caches/org.R-project.R/R/bcmaps.\n\n\nLet’s get the B.C. Natural resource boundaries:\n\nnr &lt;- nr_districts()\n\nnr_districts was updated on 2023-11-03\n\nst_geometry(nr) |&gt; \n  plot()\n\ncaribou |&gt; \n  st_transform(st_crs(nr)) |&gt; # transform so in the same crs as nr\n  select(herd) |&gt; # select just the herd column (called herd)\n  plot(add = TRUE)\n\n\n\n\nBut we can quickly get past the point where basic plotting lets us do what we want to do…"
  },
  {
    "objectID": "04-spatial-data-viz.html#ggplot2",
    "href": "04-spatial-data-viz.html#ggplot2",
    "title": "Visualization of Spatial Data",
    "section": "ggplot2",
    "text": "ggplot2\nggplot2 is a plotting package built on the theory of the “Grammar of Graphics”, where a plot is built up in layers:\n\nWe start with the data, then\nadd the graphical marks (points, lines, bars, etc. called “geom”s) we want to use to represent the data, and\nspecify “aesthetics” for how to map variables in our data to visual representations on the plot.\n\nLet’s start with a simple histogram of fixes over time:\n\nlibrary(ggplot2)\n\nggplot(data = caribou) + # start with data\n  geom_histogram( # specify the geom\n    aes( # specify aesthetics - how to map variables to visual representations\n      x = date_time\n    ), \n    position = \"dodge\" # make the bars side by side instead of stacked\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe can add a “fill” aesthetic to differentiate herds, i.e., group the bars by herd, and use a different fill colour for each herd:\n\ncaribou &lt;- transform_bc_albers(caribou)\n\nggplot(data = caribou) + # start with data\n  geom_histogram( # specify the geom\n    aes( # specify aesthetics - how to map variables to visual representations\n      x = date_time,\n      fill = herd\n    ), \n    position = \"dodge\" # make the bars side by side instead of stacked\n  )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe can use the same pattern to make a map:\n\nggplot(data = caribou) +\n  geom_sf() # x and y are inferred from the geometry column\n\n\n\n\n\nggplot(data = caribou) +\n  geom_sf(\n    aes(colour = herd, shape = animal.sex)\n  )\n\n\n\n\nWe have a lot of overlapping points, so the actual density of fixes is obscured. We can address this in several ways:\nTransparency\nWe can see the density of points better by making them partially transparent. We do this by setting the alpha parameter to a value between 0 and 1 (0 is fully transparent, 1 is fully opaque).\nNote that we are setting alpha to a constant value outside the aes() call, so the setting applies equally to all points (it is not mapped to a variable).\n\nggplot(data = caribou) +\n  geom_sf(\n    aes(colour = herd), \n    alpha = 0.1\n  ) + \n  scale_colour_viridis_d() + \n  theme_bw()\n\n\n\n\nWe can subset our plot into “small multiples”, or “facets” - making a small plot for each level of a variable in our data. For example, let’s look at the distribution of points in each month, still colouring the points by herd:\n\nggplot(data = caribou) +\n  geom_sf(\n    aes(colour = herd), \n    alpha = 0.1\n  ) + \n  scale_colour_viridis_d() + \n  facet_wrap(vars(month)) +\n  theme_bw()\n\n\n\n\nYour turn\nRead in the scott data (clean_data/scott_herd_subset.gpkg), and plot the fixes, coloured by reproductive condition, and faceted by season and year. Hint: see ?facet_grid.\nFor bonus points, remove the lat/long labels using the theme() function.\n\n\n\n\n\n\nsolution\n\n\n\n\n\n\nscott &lt;- read_sf(\"clean_data/scott_herd_subset.gpkg\")\n\nggplot(scott) +\n  geom_sf(aes(colour = animal.reproductive.condition), alpha = 0.3) + \n  facet_grid(season ~ year) + \n  theme(axis.text = element_blank())\n\n\n\n\nto get the seasons in a sensible order, set them to a factor and define the order of the levels:\n\nggplot(scott) +\n  geom_sf(aes(colour = animal.reproductive.condition), alpha = 0.3) + \n  facet_grid(\n    factor(season, levels = c(\"winter\", \"spring\", \"summer\", \"fall\")) ~ year\n  ) + \n  theme(axis.text = element_blank())\n\n\n\n\n\n\n\nBinning\nAnother way to look at the density of points is binning - sort of like a spatial histogram, but instead of using bar height to represent relative numbers, use colour. We can use geom_hex() to divide our space up into a hexagonal grid and colour the hexagons based on the number of points in each:\n\n# Start by extracting the X and Y coordinates as columns in our data set:\ncaribou &lt;- cbind(st_coordinates(caribou), caribou)\n\nhead(caribou)\n\nSimple feature collection with 6 features and 17 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1149513 ymin: 1207479 xmax: 1159827 ymax: 1214219\nProjected CRS: NAD83 / BC Albers\n        X       Y location.long location.lat  herd tag.local.identifier\n1 1149513 1214219     -123.6036     55.90000 Scott               car170\n2 1149921 1211265     -123.5987     55.87343 Scott               car170\n3 1150438 1211425     -123.5903     55.87470 Scott               car170\n4 1156753 1207479     -123.4915     55.83741 Scott               car170\n5 1157677 1212132     -123.4740     55.87877 Scott               car170\n6 1159827 1209456     -123.4412     55.85412 Scott               car170\n  animal.id animal.sex animal.reproductive.condition tag.manufacturer.name\n1 SC_car170          f                  with calf: N                   ATS\n2 SC_car170          f                  with calf: N                   ATS\n3 SC_car170          f                  with calf: N                   ATS\n4 SC_car170          f                  with calf: N                   ATS\n5 SC_car170          f                  with calf: N                   ATS\n6 SC_car170          f                  with calf: N                   ATS\n    tag.model           date_time year month day hour minute\n1 GPS Iridium 2013-09-23 03:01:00 2013     9  23   10      1\n2 GPS Iridium 2013-10-09 03:01:00 2013    10   9   10      1\n3 GPS Iridium 2013-10-30 03:01:00 2013    10  30   10      1\n4 GPS Iridium 2014-08-11 03:01:00 2014     8  11   10      1\n5 GPS Iridium 2013-11-09 18:01:00 2013    11  10    2      1\n6 GPS Iridium 2014-08-07 03:01:00 2014     8   7   10      1\n                     geom\n1 POINT (1149513 1214219)\n2 POINT (1149921 1211265)\n3 POINT (1150438 1211425)\n4 POINT (1156753 1207479)\n5 POINT (1157677 1212132)\n6 POINT (1159827 1209456)\n\nggplot(data = caribou) +\n  geom_hex(aes(x = X, y = Y)) + \n  scale_fill_viridis_c() + \n  coord_sf() +\n  theme_bw()\n\n\n\n\nWe can still use faceting to split out the two herds:\n\nggplot(data = caribou) +\n  geom_hex(aes(x = X, y = Y)) + \n  scale_fill_viridis_c() + \n  coord_sf() +\n  facet_wrap(vars(herd)) +\n  theme_bw()\n\n\n\n\nIf we want to add multiple layers to our map, we simply add multiple geoms, and add the name of the layer to the data argument in each:\n\nggplot() +\n  geom_sf(data = nr) + \n  geom_hex(aes(x = X, y = Y), data = caribou) + \n  scale_fill_viridis_c() + \n  facet_wrap(vars(herd)) +\n  theme_bw()\n\n\n\n\nWe can zoom in by specifying the plot limits in coord_sf()\n\nggplot() +\n  geom_sf(data = nr) + \n  geom_hex(aes(x = X, y = Y), data = caribou) + \n  scale_fill_viridis_c() + \n  coord_sf(\n    xlim = range(caribou$X) + c(-20000, 20000), # Add 20 km to all sides\n    ylim = range(caribou$Y) + c(-20000, 20000)\n  ) +\n  facet_wrap(vars(herd)) +\n  theme_bw()"
  },
  {
    "objectID": "04-spatial-data-viz.html#probability-density",
    "href": "04-spatial-data-viz.html#probability-density",
    "title": "Visualization of Spatial Data",
    "section": "Probability density",
    "text": "Probability density\nWe can also visualize a smooth probability density of fixes, using the ggdensity package:\n\nlibrary(ggdensity)\n\nggplot(data = caribou) +\n  geom_hdr(aes(x = X, y = Y, fill = herd)) + \n  coord_sf() + \n  theme_bw()\n\n\n\n\nThe regions have probabilities mapped to the alpha aesthetic by default - the probs legend shows the alhpa levels.\nNow let’s do a bit of work to make the plot look more finished:\n\nmonth_labels &lt;- setNames(month.name, 1:12)\n\nggplot(data = caribou) +\n  geom_hdr(aes(x = X, y = Y, fill = herd)) + \n  scale_fill_viridis_d(option = \"turbo\") + \n  scale_alpha_discrete(guide = \"none\") +\n  coord_sf() + \n  facet_wrap(\n    vars(month),\n    labeller = as_labeller(month_labels)\n    ) +\n  theme_bw() + \n  labs(\n    title = \"Probability density of caribou locations, by month\", \n    x = element_blank(), \n    y = element_blank(), \n    fill = \"Herd\"\n  )"
  },
  {
    "objectID": "04-spatial-data-viz.html#add-a-base-map-north-arrow-scale",
    "href": "04-spatial-data-viz.html#add-a-base-map-north-arrow-scale",
    "title": "Visualization of Spatial Data",
    "section": "Add a base map, north arrow, & scale",
    "text": "Add a base map, north arrow, & scale\nWe can use functions from the ggspatial package to add some nice touches to our map to make it pretty.\nWe can also customize the labels, legend, and theme elements\n\n# install.packages(c(\"ggspatial\", \"prettymapr\"))\nlibrary(ggspatial)\n\nggplot(caribou) +\n  # add background map\n  annotation_map_tile(zoom = 9) + \n  # add points\n  geom_sf(aes(colour = herd), alpha = 0.1) + \n  # set colour palette and increase legend alpha so you can see it\n  scale_colour_viridis_d(\n    option = \"turbo\", \n    guide = guide_legend(override.aes = list(alpha = 0.5))) + \n  # Add a North arrow and scale bar\n  annotation_north_arrow(style = north_arrow_nautical()) + \n  annotation_scale(location = \"tr\") + \n    coord_sf() + \n  # Facet by month, custom label the facets\n  facet_wrap(\n    vars(month),\n    labeller = as_labeller(month_labels)\n    ) +\n  # Add a title, remove X and Y labels\n  labs(\n    title = \"Probability density of caribou locations, by month\", \n    x = element_blank(), \n    y = element_blank(), \n    colour = \"Herd\"\n  ) + \n  # Set background of facet labels and legend to white, \n  # put legend on the bottom\n  theme(\n    strip.background = element_rect(fill = \"white\"),\n    legend.key = element_rect(fill = \"white\"),\n    legend.position = \"bottom\"\n  )\n\n\n\n\nWe can also look at the movement of just one or two animals:\n\nscott &lt;- read_sf(\"clean_data/scott_herd_subset.gpkg\")\n\nscott &lt;- cbind(st_coordinates(scott), scott)\n\nmvmt &lt;- scott |&gt; \n  filter(animal.id == \"SC_car171\")\n\nggplot(mvmt) + \n  geom_path(aes(x = X, y = Y, colour = date2)) +\n  scale_color_viridis_c(n.breaks = 6, trans = \"date\") + \n  coord_sf() + \n  labs(colour = \"Date\") + \n  theme_bw() + \n  theme(\n    axis.title = element_blank(),\n    legend.position = \"bottom\",\n    legend.key.width = unit(4, \"lines\")\n  )"
  },
  {
    "objectID": "04-spatial-data-viz.html#your-turn-1",
    "href": "04-spatial-data-viz.html#your-turn-1",
    "title": "Visualization of Spatial Data",
    "section": "Your turn:",
    "text": "Your turn:\nModify the previous map to make a faceted (small-multiples) map for two or more animals, and add a background map.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntop_animals &lt;- c(\"SC_car171\", \"SC_car168\")\n\nmvmt &lt;- scott |&gt; filter(animal.id %in% top_animals)\n\nggplot(mvmt) + \n  annotation_map_tile(zoom = 10) +\n  geom_path(aes(x = X, y = Y, colour = date2)) + \n  facet_wrap(vars(animal.id)) +\n  scale_color_viridis_c(n.breaks = 6, trans = \"date\") + \n  coord_sf() + \n  labs(colour = \"Date\") + \n  theme(\n    axis.title = element_blank(),\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    legend.position = \"bottom\",\n    legend.key.width = unit(4, \"lines\")\n  )"
  },
  {
    "objectID": "bonus-interactive-graphics.html",
    "href": "bonus-interactive-graphics.html",
    "title": "Interactive and animated maps",
    "section": "",
    "text": "We can visualize movement by animating their tracks, using the gganimate package.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(gganimate)\nlibrary(ggspatial)\n\nLet’s start with the map we made earlier of the paths of two animals, faceted by year:\n\nscott &lt;- read_sf(\"clean_data/scott_herd_subset.gpkg\")\n\nscott &lt;- cbind(st_coordinates(scott), scott)\n\ntop_animals &lt;- c(\"SC_car171\", \"SC_car168\")\n\nmvmt &lt;- scott |&gt; filter(animal.id %in% top_animals) |&gt; \n  mutate(doy = lubridate::yday(date2))\n\n\np &lt;- ggplot(mvmt) + \n  geom_path(aes(x = X, y = Y, colour = doy)) + \n  facet_grid(animal.id ~ year) +\n  scale_color_viridis_c() + \n  coord_sf() + \n  labs(colour = \"Day of Year\") + \n  theme_bw() + \n  theme(\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title = element_blank(),\n    legend.position = \"bottom\",\n    legend.key.width = unit(3, \"lines\")\n  )\n\nWe then add a transition to the plot that reveals each step along the path, and create an animated gif:\n\np_animate &lt;- p + \n  transition_reveal(along = doy) + \n  labs(title = \"Day of Year: {round(frame_along)}\")\n\nanimate(\n  p_animate, \n  width = 10, \n  height = 6, \n  units = \"in\", \n  res = 72, \n  fps = 10, \n  nframes = 300\n)"
  },
  {
    "objectID": "bonus-interactive-graphics.html#animated-maps",
    "href": "bonus-interactive-graphics.html#animated-maps",
    "title": "Interactive and animated maps",
    "section": "",
    "text": "We can visualize movement by animating their tracks, using the gganimate package.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(gganimate)\nlibrary(ggspatial)\n\nLet’s start with the map we made earlier of the paths of two animals, faceted by year:\n\nscott &lt;- read_sf(\"clean_data/scott_herd_subset.gpkg\")\n\nscott &lt;- cbind(st_coordinates(scott), scott)\n\ntop_animals &lt;- c(\"SC_car171\", \"SC_car168\")\n\nmvmt &lt;- scott |&gt; filter(animal.id %in% top_animals) |&gt; \n  mutate(doy = lubridate::yday(date2))\n\n\np &lt;- ggplot(mvmt) + \n  geom_path(aes(x = X, y = Y, colour = doy)) + \n  facet_grid(animal.id ~ year) +\n  scale_color_viridis_c() + \n  coord_sf() + \n  labs(colour = \"Day of Year\") + \n  theme_bw() + \n  theme(\n    axis.text = element_blank(),\n    axis.ticks = element_blank(),\n    axis.title = element_blank(),\n    legend.position = \"bottom\",\n    legend.key.width = unit(3, \"lines\")\n  )\n\nWe then add a transition to the plot that reveals each step along the path, and create an animated gif:\n\np_animate &lt;- p + \n  transition_reveal(along = doy) + \n  labs(title = \"Day of Year: {round(frame_along)}\")\n\nanimate(\n  p_animate, \n  width = 10, \n  height = 6, \n  units = \"in\", \n  res = 72, \n  fps = 10, \n  nframes = 300\n)"
  },
  {
    "objectID": "bonus-interactive-graphics.html#interactive-maps-with-leaflet",
    "href": "bonus-interactive-graphics.html#interactive-maps-with-leaflet",
    "title": "Interactive and animated maps",
    "section": "Interactive maps with {Leaflet}\n",
    "text": "Interactive maps with {Leaflet}\n\nLet’s use the file that has the caribou locations with the attributes from the base layers\n\nlibrary(dplyr)\nlibrary(leaflet)\n\nall_pts &lt;- read_sf(\"clean_data/allpts_att.gpkg\") |&gt; \n  st_transform(4326)\n\ncaribou &lt;- filter(all_pts, !is.na(animal.id))\nbackground &lt;- filter(all_pts, is.na(animal.id))\n\nanimal_pal &lt;- colorFactor(\n  palette = \"viridis\",\n  domain = unique(caribou$animal.id)\n)\n\nleaflet(caribou) |&gt; \n  addProviderTiles(providers$Esri.NatGeoWorldMap) |&gt; \n  addCircleMarkers(\n    data = background, \n    color = \"grey\", \n    radius = 1\n    ) |&gt; \n  addCircleMarkers(\n    color = ~ animal_pal(caribou$animal.id),\n    weight = 1,\n    radius = ~ elevation / 300,\n    popup = ~ paste0(\n      \"Animal: \", animal.id, \"&lt;br&gt;\",\n      \"Elevation: \", round(elevation)\n    )\n  ) |&gt; \n  addLegend(\n    \"bottomleft\", \n    pal = animal_pal,\n    values = ~ caribou$animal.id,\n    title = \"Animal ID\"\n  )"
  },
  {
    "objectID": "02-intro-spatial-data.html#outline",
    "href": "02-intro-spatial-data.html#outline",
    "title": "Introduction to Spatial Data with R",
    "section": "Outline",
    "text": "Outline\n\nIntroduction to Simple Features and the sf package\nCoordinate Reference Systems\nReading various spatial data formats into R\nBasic operations with spatial data"
  },
  {
    "objectID": "02-intro-spatial-data.html#learning-objectives",
    "href": "02-intro-spatial-data.html#learning-objectives",
    "title": "Introduction to Spatial Data with R",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nThe “simple features” representation of vector data\nUse and understand sf objects in R\nBasic understanding of Coordinate Reference Systems (CRS)\nUse mapview and sf to preview spatial data\nHow to do basic operations with spatial data using sf"
  },
  {
    "objectID": "02-intro-spatial-data.html#before-we-start",
    "href": "02-intro-spatial-data.html#before-we-start",
    "title": "Introduction to Spatial Data with R",
    "section": "Before we start",
    "text": "Before we start\n\nConfigure RStudio\nGet the course materials (if you haven’t already):\n\n\nusethis::use_course(\n  \"https://github.com/ninoxconsulting/r-telemetry-workshop-nov-2023/raw/main/r-telemetry-workshop.zip\"\n)\n\n&lt;etherpad.andyteucher.ca/p/r-telemetry-pg&gt;"
  },
  {
    "objectID": "02-intro-spatial-data.html#vector-simple-features",
    "href": "02-intro-spatial-data.html#vector-simple-features",
    "title": "Introduction to Spatial Data with R",
    "section": "Vector: Simple Features",
    "text": "Vector: Simple Features\n\n\nThe sf R package1\nReplaces\n\nsp\nrgdal\nrgeos\n\n\n2\n\n\n\n\nSimple Features is a standard specification (Open Geospatial Consortium) - agreed-upon way to represent vector spatial data\nrepresent all common vector geometry types : points, lines, polygons and their respective ‘multi’ versions\nsupports geometry collections, which can contain multiple geometry types in a single object.\nsf supersedes the sp ecosystem, which comprises sp , rgdal for data read/write and rgeos for spatial operations.\nrgdal and rgeos are now retired and removed from CRAN\n\n\nsf package: https://cran.r-project.org/package=sfGeocomputation with R, fig 2.2: https://geocompr.robinlovelace.net"
  },
  {
    "objectID": "02-intro-spatial-data.html#reading-previewing-spatial-data",
    "href": "02-intro-spatial-data.html#reading-previewing-spatial-data",
    "title": "Introduction to Spatial Data with R",
    "section": "Reading & previewing spatial data",
    "text": "Reading & previewing spatial data\n\n\n\nlibrary(sf)\nlibrary(mapview)\n\nairports &lt;- read_sf(\n  \"raw_data/bc_airports.gpkg\"\n)\n\nmapview(airports)"
  },
  {
    "objectID": "02-intro-spatial-data.html#mapview",
    "href": "02-intro-spatial-data.html#mapview",
    "title": "Introduction to Spatial Data with R",
    "section": "mapview",
    "text": "mapview\n\nmapview(airports, zcol = \"NUMBER_OF_RUNWAYS\")"
  },
  {
    "objectID": "02-intro-spatial-data.html#structure-of-an-sf-object",
    "href": "02-intro-spatial-data.html#structure-of-an-sf-object",
    "title": "Introduction to Spatial Data with R",
    "section": "Structure of an sf object",
    "text": "Structure of an sf object\n\nairports\n\nSimple feature collection with 455 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 406543.7 ymin: 367957.6 xmax: 1796645 ymax: 1689146\nProjected CRS: NAD83 / BC Albers\n# A tibble: 455 × 6\n   AIRPORT_NAME                   IATA_CODE LOCALITY ELEVATION NUMBER_OF_RUNWAYS\n   &lt;chr&gt;                          &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;             &lt;int&gt;\n 1 Terrace (Northwest Regional) … &lt;NA&gt;      Terrace     217.                   2\n 2 Victoria Harbour (Camel Point… &lt;NA&gt;      Victoria      4.57                 0\n 3 Victoria Inner Harbour Airpor… YWH       Victoria      0                    0\n 4 Victoria Harbour (Shoal Point… &lt;NA&gt;      Victoria      3.05                 0\n 5 Victoria (Royal Jubilee Hospi… &lt;NA&gt;      Saanich      15.6                  0\n 6 Victoria (General Hospital) H… &lt;NA&gt;      View Ro…     15.8                  0\n 7 Victoria (BC Hydro) Heliport   &lt;NA&gt;      Saanich      12.2                  0\n 8 San Juan Point (Coast Guard) … &lt;NA&gt;      Port Re…      7.62                 0\n 9 Shawnigan Lake Water Aerodrome &lt;NA&gt;      Shawnig…      0                    0\n10 Victoria International Airport YYJ       North S…     19.5                  3\n# ℹ 445 more rows\n# ℹ 1 more variable: geom &lt;POINT [m]&gt;\n\n\n\ngo through sf header info: - size (# of features and # of columns/attributes) - geometry type - dimension (XY - can have Z and M) - bbox - CRS"
  },
  {
    "objectID": "02-intro-spatial-data.html#key-features-of-an-sf-object",
    "href": "02-intro-spatial-data.html#key-features-of-an-sf-object",
    "title": "Introduction to Spatial Data with R",
    "section": "Key features of an sf object",
    "text": "Key features of an sf object\n\n\n\nst_geometry(airports)\n\nGeometry set for 455 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 406543.7 ymin: 367957.6 xmax: 1796645 ymax: 1689146\nProjected CRS: NAD83 / BC Albers\nFirst 5 geometries:\n\n\nPOINT (833323.9 1054950)\n\n\nPOINT (1193727 381604.1)\n\n\nPOINT (1194902 382257.7)\n\n\nPOINT (1193719 382179.3)\n\n\nPOINT (1198292 383563.6)\n\nst_bbox(airports)\n\n     xmin      ymin      xmax      ymax \n 406543.7  367957.6 1796645.0 1689145.9 \n\n\n\n\nst_crs(airports)\n\nCoordinate Reference System:\n  User input: NAD83 / BC Albers \n  wkt:\nPROJCRS[\"NAD83 / BC Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"British Columbia Albers\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",45,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-126,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",50,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",58.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Province-wide spatial data management.\"],\n        AREA[\"Canada - British Columbia.\"],\n        BBOX[48.25,-139.04,60.01,-114.08]],\n    ID[\"EPSG\",3005]]"
  },
  {
    "objectID": "02-intro-spatial-data.html#an-sf-object-is-a-data.frame",
    "href": "02-intro-spatial-data.html#an-sf-object-is-a-data.frame",
    "title": "Introduction to Spatial Data with R",
    "section": "An sf object is a data.frame",
    "text": "An sf object is a data.frame\n\nclass(airports)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nis.data.frame(airports)\n\n[1] TRUE\n\nsummary(airports)\n\n AIRPORT_NAME        IATA_CODE           LOCALITY           ELEVATION     \n Length:455         Length:455         Length:455         Min.   :   0.0  \n Class :character   Class :character   Class :character   1st Qu.:   0.0  \n Mode  :character   Mode  :character   Mode  :character   Median :   6.4  \n                                                          Mean   : 194.4  \n                                                          3rd Qu.: 307.1  \n                                                          Max.   :1277.4  \n NUMBER_OF_RUNWAYS            geom    \n Min.   :0.0000    POINT        :455  \n 1st Qu.:0.0000    epsg:3005    :  0  \n Median :0.0000    +proj=aea ...:  0  \n Mean   :0.3385                       \n 3rd Qu.:1.0000                       \n Max.   :3.0000"
  },
  {
    "objectID": "02-intro-spatial-data.html#coordinate-reference-systems",
    "href": "02-intro-spatial-data.html#coordinate-reference-systems",
    "title": "Introduction to Spatial Data with R",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\n\n\nGeographic\n\nspherical or ellipsoidal surface\nellipsoid defined by the datum\nlat/long, measured in angular distances (degrees/radians)\n\n\n\n\n\n\n\nProjected\n\nCartesian coordinates on a flat plane\norigin, x and y axes, linear unit of measurement (e.g., m)\n\n\n\n\n\ndefines how the spatial elements of the data relate to the surface of the Earth\nGeographic: - identify any location on the Earth’s surface using two values — longitude and latitude. - Longitude is location in the East-West direction in angular distance from the Prime Meridian plane. - Latitude is angular distance North or South of the equatorial plane. - Distances in geographic CRSs are therefore not measured in meters. This has important consequences\nProjected: - based on a geographic CRS - rely on map projections to convert the three-dimensional surface of the Earth into Easting and Northing (x and y) values in a projected CRS - often named based on a property they preserve: equal-area preserves area, azimuthal preserve direction, equidistant preserve distance, and conformal preserve local shape. - conic, cylindrical, planar"
  },
  {
    "objectID": "02-intro-spatial-data.html#crss-in-r-epsg-codes",
    "href": "02-intro-spatial-data.html#crss-in-r-epsg-codes",
    "title": "Introduction to Spatial Data with R",
    "section": "CRSs in R: EPSG codes",
    "text": "CRSs in R: EPSG codes\n\nst_crs(airports)$input\n\n[1] \"NAD83 / BC Albers\"\n\n\n\n\n\nBC Albers - B.C.’s standard projection\n\nEqual Area conic\nCentre: c(-126, 54)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://epsg.io/3005\n\n\nEPSG codes BC Albers"
  },
  {
    "objectID": "02-intro-spatial-data.html#wkt-well-known-text",
    "href": "02-intro-spatial-data.html#wkt-well-known-text",
    "title": "Introduction to Spatial Data with R",
    "section": "WKT (Well-Known Text)",
    "text": "WKT (Well-Known Text)\n\nst_crs(3005)\n\nCoordinate Reference System:\n  User input: EPSG:3005 \n  wkt:\nPROJCRS[\"NAD83 / BC Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"British Columbia Albers\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",45,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-126,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",50,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",58.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Province-wide spatial data management.\"],\n        AREA[\"Canada - British Columbia.\"],\n        BBOX[48.25,-139.04,60.01,-114.08]],\n    ID[\"EPSG\",3005]]"
  },
  {
    "objectID": "02-intro-spatial-data.html#your-turn",
    "href": "02-intro-spatial-data.html#your-turn",
    "title": "Introduction to Spatial Data with R",
    "section": "Your turn",
    "text": "Your turn\n\nRead in the electoral districts data in the raw_data folder:\n\nWhat type of geometry is it?\nWhat is the CRS?\nWhat is the EPSG code?"
  },
  {
    "objectID": "02-intro-spatial-data.html#solution",
    "href": "02-intro-spatial-data.html#solution",
    "title": "Introduction to Spatial Data with R",
    "section": "Solution",
    "text": "Solution\n\nelec_bc &lt;- read_sf(\"raw_data/bc_electoral_districts.shp\")\nst_geometry_type(elec_bc, by_geometry = FALSE)\n\n[1] POLYGON\n18 Levels: GEOMETRY POINT LINESTRING POLYGON MULTIPOINT ... TRIANGLE\n\nst_crs(elec_bc)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nhttps://epsg.io/4326"
  },
  {
    "objectID": "02-intro-spatial-data.html#basic-plotting",
    "href": "02-intro-spatial-data.html#basic-plotting",
    "title": "Introduction to Spatial Data with R",
    "section": "Basic plotting",
    "text": "Basic plotting\n\nplot(elec_bc)"
  },
  {
    "objectID": "02-intro-spatial-data.html#basic-plotting-1",
    "href": "02-intro-spatial-data.html#basic-plotting-1",
    "title": "Introduction to Spatial Data with R",
    "section": "Basic plotting",
    "text": "Basic plotting\n\n\nJust the shapes\n\nplot(st_geometry(elec_bc))\n\n\n\n\n\n\n\n\n\nA single column\n\nplot(elec_bc[\"ED_NAME\"])\n\n\n\n\n\n\n\n\n\n\n\n\nNotice strange orientation of BC - north is greatly exaggerated\nbecause in WGS84 (lat/long)\nglobal CRS centred at lon 0 (Greenwich) and lat 0 (equator)\ngood for web mapping, not good for BC"
  },
  {
    "objectID": "02-intro-spatial-data.html#transforming-coordinate-systems",
    "href": "02-intro-spatial-data.html#transforming-coordinate-systems",
    "title": "Introduction to Spatial Data with R",
    "section": "Transforming coordinate systems",
    "text": "Transforming coordinate systems\n\nelec_bc_albers &lt;- st_transform(elec_bc, 3005)\n\nOr, if you have another object in the CRS you want to use:\n\nelec_bc_albers &lt;- st_transform(elec_bc, st_crs(airports))\nst_crs(elec_bc_albers)\n\nCoordinate Reference System:\n  User input: NAD83 / BC Albers \n  wkt:\nPROJCRS[\"NAD83 / BC Albers\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"British Columbia Albers\",\n        METHOD[\"Albers Equal Area\",\n            ID[\"EPSG\",9822]],\n        PARAMETER[\"Latitude of false origin\",45,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-126,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",50,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",58.5,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",1000000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Province-wide spatial data management.\"],\n        AREA[\"Canada - British Columbia.\"],\n        BBOX[48.25,-139.04,60.01,-114.08]],\n    ID[\"EPSG\",3005]]"
  },
  {
    "objectID": "02-intro-spatial-data.html#your-turn-1",
    "href": "02-intro-spatial-data.html#your-turn-1",
    "title": "Introduction to Spatial Data with R",
    "section": "Your turn",
    "text": "Your turn\nLoad \"raw_data/ski_resorts.csv\" as an sf object\n\n\n\n\n\nfacility_name\nlocality\nlatitude\nlongitude\nelevation\n\n\n\n\nWapiti Ski Club\nElkford\n50.02168\n-114.9380\n1467\n\n\nSummit Lake Ski Area\nNakusp\n50.14546\n-117.6144\n1132\n\n\nSasquatch Mountain Resort\nHemlock Valley\n49.38011\n-121.9354\n1185\n\n\nApex Mountain Resort\nApex\n49.39042\n-119.9047\n1852\n\n\nSalmo Ski Hill\nSalmo\n49.18640\n-117.3015\n864\n\n\nRed Mountain Resort\nRossland\n49.10238\n-117.8194\n1150"
  },
  {
    "objectID": "02-intro-spatial-data.html#hints",
    "href": "02-intro-spatial-data.html#hints",
    "title": "Introduction to Spatial Data with R",
    "section": "Hints:",
    "text": "Hints:\nLoad \"raw_data/ski_resorts.csv\" as an sf object\n\nski_resorts &lt;- read.csv(\"raw_data/ski_resorts.csv\")\nski_resorts &lt;- st_as_sf(ski_resorts, ...)"
  },
  {
    "objectID": "02-intro-spatial-data.html#solution-1",
    "href": "02-intro-spatial-data.html#solution-1",
    "title": "Introduction to Spatial Data with R",
    "section": "Solution",
    "text": "Solution\n\nski_resorts &lt;- read.csv(\"raw_data/ski_resorts.csv\")\n\nski_resorts &lt;- st_as_sf(ski_resorts,\n                        coords = c(\"longitude\", \"latitude\"),\n                        crs = 4326)\n\nhead(ski_resorts)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -121.9354 ymin: 49.10238 xmax: -114.938 ymax: 50.14546\nGeodetic CRS:  WGS 84\n              facility_name       locality elevation                   geometry\n1           Wapiti Ski Club        Elkford      1467  POINT (-114.938 50.02168)\n2      Summit Lake Ski Area         Nakusp      1132 POINT (-117.6144 50.14546)\n3 Sasquatch Mountain Resort Hemlock Valley      1185 POINT (-121.9354 49.38011)\n4      Apex Mountain Resort           Apex      1852 POINT (-119.9047 49.39042)\n5            Salmo Ski Hill          Salmo       864  POINT (-117.3015 49.1864)\n6       Red Mountain Resort       Rossland      1150 POINT (-117.8194 49.10238)"
  },
  {
    "objectID": "02-intro-spatial-data.html#geometric-calculations",
    "href": "02-intro-spatial-data.html#geometric-calculations",
    "title": "Introduction to Spatial Data with R",
    "section": "Geometric calculations",
    "text": "Geometric calculations\n\n\nGeometric Measurements\n\nst_area()\nst_length()\nst_distance()\n\n\nGeometric Operations\n\nst_union()\nst_intersection()\nst_difference()\nst_sym_difference()"
  },
  {
    "objectID": "02-intro-spatial-data.html#geometry-predicates",
    "href": "02-intro-spatial-data.html#geometry-predicates",
    "title": "Introduction to Spatial Data with R",
    "section": "Geometry Predicates",
    "text": "Geometry Predicates\n\nUse with st_filter() or st_join()\n\n\n\n\nst_intersects(): touch or overlap\nst_disjoint(): !intersects\nst_touches(): touch\nst_crosses(): cross (don’t touch)\nst_within(): within\n\n\n\nst_contains(): contains\nst_overlaps(): overlaps\nst_covers(): cover\nst_covered_by(): covered by\nst_equals(): equals"
  },
  {
    "objectID": "02-intro-spatial-data.html#manipulating-geometries",
    "href": "02-intro-spatial-data.html#manipulating-geometries",
    "title": "Introduction to Spatial Data with R",
    "section": "Manipulating Geometries",
    "text": "Manipulating Geometries\n\n\n\nst_line_merge()\nst_segmentize()\nst_voronoi()\nst_centroid()\nst_convex_hull()\nst_triangulate()\n\n\n\nst_polygonize()\nst_split()\nst_buffer()\nst_make_valid()\nst_boundary()\n…"
  },
  {
    "objectID": "02-intro-spatial-data.html#your-turn-2",
    "href": "02-intro-spatial-data.html#your-turn-2",
    "title": "Introduction to Spatial Data with R",
    "section": "Your turn",
    "text": "Your turn\n\nCalculate the area of each electoral district\nCreate an sf object of only airports within the Nelson-Creston electoral district.\nPlot the ski resorts as circles, where the size of the circle is related to the elevation of the resort.\n\n\nst_area(elec_bc_albers)\nst_filter(airports, elec_bc_albers[elec_bc_albers$ED_NAME == “Nelson-Creston”, ])\nmapview(st_buffer(ski_resorts, dist = ski_resorts$elevation*10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkshop Home"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at gperkins@ninoxconsulting.ca or andy@andyteucher.ca. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by [Mozilla’s code of conduct enforcement ladder][https://github.com/mozilla/inclusion].\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at gperkins@ninoxconsulting.ca or andy@andyteucher.ca. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by [Mozilla’s code of conduct enforcement ladder][https://github.com/mozilla/inclusion].\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "06-raster-data.html",
    "href": "06-raster-data.html",
    "title": "Exploring Raster Data",
    "section": "",
    "text": "In this module we will explore raster data, and the available data sets which can be used in R for analysis. We will:\n\nprepare our study aoi to use with rasters\nextract DEM data through the bcmaps package\ngenerate DEM derived covariates\nprepare a raster stack with all spatial layers generated to date"
  },
  {
    "objectID": "06-raster-data.html#overview",
    "href": "06-raster-data.html#overview",
    "title": "Exploring Raster Data",
    "section": "",
    "text": "In this module we will explore raster data, and the available data sets which can be used in R for analysis. We will:\n\nprepare our study aoi to use with rasters\nextract DEM data through the bcmaps package\ngenerate DEM derived covariates\nprepare a raster stack with all spatial layers generated to date"
  },
  {
    "objectID": "06-raster-data.html#background-rasters",
    "href": "06-raster-data.html#background-rasters",
    "title": "Exploring Raster Data",
    "section": "Background: Rasters",
    "text": "Background: Rasters\nRaster data is information built on a standard grid. These can be characterized by the grid extent (xmin, xmax, ymin, ymax) and can have a coordinate system to orient them in space. Rasters are made up of cells or pixels, based on a resolution or cell size. Each cell contains a single value. The terra package contains many functions for manipulating and processing rasters.\nNote we will be using the BC Albers coordinate reference system (EPSG:3005). The advantages of this are 1) BCAlbers is an equal area projection across BC, 2) extent is marked using meters."
  },
  {
    "objectID": "06-raster-data.html#create-a-standard-raster-template.",
    "href": "06-raster-data.html#create-a-standard-raster-template.",
    "title": "Exploring Raster Data",
    "section": "1. Create a standard raster template.",
    "text": "1. Create a standard raster template.\nFor ease of calculations we will create a standard “template” raster which will form the basis for all our data analysis.\n\n# read in the libraries needed\nlibrary(dplyr)\nlibrary(sf)\nlibrary(bcmaps)\nlibrary(terra)\n\nRead in our AOI that we made and saved previously. We can then convert this to a raster with a resolution of given size. In this example we will select 25m.\n\n# read in the spatial file\nbou &lt;- read_sf(\"clean_data/scott_herd_subset.gpkg\")\n\n\n# read in the aoi previously created\naoi &lt;- read_sf(\"clean_data/scott_aoi.gpkg\")\n\nWe can now convert our polygon to a raster object.\n\n# create a template raster with a resolution of 25m\ntemplate &lt;- rast(aoi, resolution = 25, names = \"aoi\", vals = 0)\n\nexport the raster template for later processing\n\nwriteRaster(template, \"clean_data/template.tif\", overwrite = TRUE)"
  },
  {
    "objectID": "06-raster-data.html#extract-base-data-using-the-cded-data-set",
    "href": "06-raster-data.html#extract-base-data-using-the-cded-data-set",
    "title": "Exploring Raster Data",
    "section": "2. Extract base data using the CDED data set",
    "text": "2. Extract base data using the CDED data set\nNow we can use the bcmaps package to directly download digital elevation data from the Canadian Digital Elevation Model CDED. Within BC, this is largely equivalent to the TRIM DEM data set. We will use the cded_terra function.\n\n# Note this is downloaded in tiles which will be cached \ntrim_raw &lt;- bcmaps::cded_terra(aoi)\n\nchecking your existing tiles for mapsheet 93o are up to date\n\n\nchecking your existing tiles for mapsheet 94b are up to date\n\ntrim_raw # note this is in WGS so we need to convert to 3005\n\nclass       : SpatRaster \ndimensions  : 1659, 3170, 1  (nrow, ncol, nlyr)\nresolution  : 0.0002083333, 0.0002083333  (x, y)\nextent      : -123.6772, -123.0168, 55.67445, 56.02008  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 (EPSG:4269) \nsource      : file15b9172c84853.vrt \nname        : elevation \n\nres(trim_raw)\n\n[1] 0.0002083333 0.0002083333\n\n# reproject to match our raster template crs and extent\ntrim_3005 &lt;- project(trim_raw, template)\n\n\n# write out the individual raster\nwriteRaster(trim_3005, \"clean_data/dem.tif\", overwrite = TRUE)"
  },
  {
    "objectID": "06-raster-data.html#generate-dem-derived-covariates",
    "href": "06-raster-data.html#generate-dem-derived-covariates",
    "title": "Exploring Raster Data",
    "section": "3. Generate DEM derived covariates",
    "text": "3. Generate DEM derived covariates\nNow we can use the terrain functions within the terra package to generate some standard base layers derived from the DEM.\n\n# generate slope \nrslope &lt;- terrain(trim_3005, v = \"slope\", neighbors = 8, unit = \"degrees\") \n\n# generate aspect\naspect &lt;- terrain(trim_3005, v = \"aspect\", neighbors = 8,  unit = \"degrees\") \n\n# generate topographic roughness index\ntri &lt;- terrain(trim_3005, v = \"TRI\", neighbors = 8)\n\n# create a raster stack\nrstack &lt;- c(trim_3005, rslope, aspect, tri)\n\nplot(rstack)\n\n\n\n\nwrite our the individual rasters (.tif)\n\nwriteRaster(rslope, \"clean_data/slope.tif\", overwrite = TRUE) \n\n\nwriteRaster(aspect, \"clean_data/aspect.tif\", overwrite = TRUE) \n\n\nwriteRaster(tri, \"clean_data/tri.tif\", overwrite = TRUE)"
  },
  {
    "objectID": "06-raster-data.html#your-turn",
    "href": "06-raster-data.html#your-turn",
    "title": "Exploring Raster Data",
    "section": "Your Turn",
    "text": "Your Turn\n\nExplore what other covariates you can generate using the terra::terrain function. Hint use ?terrain to see the help file.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\n#There are a number of other options to select from including TPI, TRIriley, roughness, flowdir. \n#Use the ?terrain to find details on the parameters needed\n\n?terra::terrain\nroughness &lt;-  terrain(trim_3005, v = \"roughness\") \nplot(roughness)\n\n\n\n\n\nCompare outputs of aspect which the neighbours parameter is adjusted. What difference does this make to the output and the time to process?\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\n# generate aspect\naspect4 &lt;- terrain(trim_3005, v = \"aspect\", neighbors = 4,  unit = \"degrees\") \n\n# generate aspect\naspect8 &lt;- terrain(trim_3005, v = \"aspect\", neighbors = 8,  unit = \"degrees\") \n\n# use the window to check \nplot(aspect4)\nplot(aspect8)\n\n# Explore the change in metric if any\naspectrr &lt;- terrain(trim_3005, v = \"aspect\", neighbors = 8,  unit = \"radians\")"
  },
  {
    "objectID": "03-intro-telemetry-data.html#overview",
    "href": "03-intro-telemetry-data.html#overview",
    "title": "Introduction to Telemetry Data",
    "section": "Overview:",
    "text": "Overview:\nIn this module we will read in, clean, QA and summarize raw telemetry data. This includes:\n\nread in data as csv/xlsx and convert to spatial data\nQA data for missing values and poor coordinate precision\ngenerate tabular summaries\nstandardize fix rate of telemetry points for future analysis"
  },
  {
    "objectID": "03-intro-telemetry-data.html#caribou-data-set",
    "href": "03-intro-telemetry-data.html#caribou-data-set",
    "title": "Introduction to Telemetry Data",
    "section": "Caribou data set:",
    "text": "Caribou data set:\nIn this workshop we will use GPS telemetry data from Mountain Caribou (Rangifer terendus) herds in the Peace region of British Columbia. The full data set and metadata can be found on movebank, also see bonus content - How to download data from movebank. The data set used in this workshop has been modified for demonstration purposes and should not be used in analysis.\n1. Reading data into R.\nIn this workshop we will be providing two files (Mountain caribou in British Columbia-reference-data.csv and Mountain caribou.xlsx). The first task is to look at the data to make sense of what we have.\n\n# load libraries \nlibrary(readr)\nlibrary(sf)\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\n\n# Read in data files.\nref_raw &lt;- read_csv(\"raw_data/Mountain caribou in British Columbia-reference-data.csv\", \n                    name_repair = \"universal\") # use this to standardise the column names\n\nloc_raw &lt;- read_xlsx(\"raw_data/Mountain caribou.xlsx\")\n\nLets take a look….\n\n# Reference data \nhead(ref_raw)\n\n# A tibble: 6 × 26\n  tag.id  animal.id  animal.taxon      deploy.on.date deploy.off.date\n  &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;             &lt;time&gt;         &lt;time&gt;         \n1 151.51  HR_151.510 Rangifer tarandus    NA             NA          \n2 C04a    GR_C04     Rangifer tarandus    NA          59:00          \n3 C03     GR_C03     Rangifer tarandus    NA             NA          \n4 151.805 HR_151.805 Rangifer tarandus    NA             NA          \n5 151.76  HR_151.760 Rangifer tarandus    NA             NA          \n6 151.72  HR_151.720 Rangifer tarandus    NA             NA          \n# ℹ 21 more variables: animal.death.comments &lt;chr&gt;, animal.life.stage &lt;chr&gt;,\n#   animal.reproductive.condition &lt;chr&gt;, animal.sex &lt;chr&gt;,\n#   animal.taxon.detail &lt;chr&gt;, attachment.type &lt;chr&gt;,\n#   deploy.off.latitude &lt;dbl&gt;, deploy.off.longitude &lt;dbl&gt;,\n#   deploy.on.latitude &lt;dbl&gt;, deploy.on.longitude &lt;dbl&gt;,\n#   deploy.on.person &lt;chr&gt;, deployment.comments &lt;chr&gt;,\n#   deployment.end.comments &lt;chr&gt;, deployment.end.type &lt;chr&gt;, …\n\nnames(ref_raw)\n\n [1] \"tag.id\"                        \"animal.id\"                    \n [3] \"animal.taxon\"                  \"deploy.on.date\"               \n [5] \"deploy.off.date\"               \"animal.death.comments\"        \n [7] \"animal.life.stage\"             \"animal.reproductive.condition\"\n [9] \"animal.sex\"                    \"animal.taxon.detail\"          \n[11] \"attachment.type\"               \"deploy.off.latitude\"          \n[13] \"deploy.off.longitude\"          \"deploy.on.latitude\"           \n[15] \"deploy.on.longitude\"           \"deploy.on.person\"             \n[17] \"deployment.comments\"           \"deployment.end.comments\"      \n[19] \"deployment.end.type\"           \"deployment.id\"                \n[21] \"manipulation.type\"             \"study.site\"                   \n[23] \"tag.beacon.frequency\"          \"tag.manufacturer.name\"        \n[25] \"tag.model\"                     \"tag.serial.no\"                \n\nref_short &lt;- ref_raw |&gt; \n  select(tag.id, animal.id, deploy.on.date, animal.sex, animal.reproductive.condition,\n         deployment.end.type,tag.model, tag.manufacturer.name, tag.serial.no)\n\n# Location data\nhead(loc_raw)\n\n# A tibble: 6 × 14\n    event.id timestamp location.long location.lat   DOP FixType     herd \n       &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;\n1 2270202009 01:00.0           -124.         55.9     1 val. GPS-3D Scott\n2 2270202041 01:00.0           -124.         55.9     1 val. GPS-3D Scott\n3 2270202100 01:00.0           -124.         55.9     1 val. GPS-3D Scott\n4 2270202901 01:00.0           -123.         55.8     1 val. GPS-3D Scott\n5 2270202132 01:00.0           -123.         55.9     1 val. GPS-3D Scott\n6 2270202890 01:00.0           -123.         55.9     1 val. GPS-3D Scott\n# ℹ 7 more variables: study.specific.measurement &lt;chr&gt;, sensor.type &lt;chr&gt;,\n#   individual.taxon.canonical.name &lt;chr&gt;, tag.local.identifier &lt;chr&gt;,\n#   individual.local.identifier &lt;chr&gt;, study.name &lt;chr&gt;, date &lt;chr&gt;\n\n\nWe can combine these data sets and keep the columns of interest\n\n# create a new data set by joining on a common field\nall_data &lt;- left_join(loc_raw, ref_raw, by = c('tag.local.identifier' = 'tag.id') )\n\n# filter the columns of interest\nall_data &lt;- all_data |&gt; \n  select(event.id, location.long, location.lat, DOP, FixType, herd,\n         study.specific.measurement, sensor.type, tag.local.identifier, date, animal.id,\n         animal.sex, animal.reproductive.condition, tag.manufacturer.name, tag.model )\n\n2. Clean and QA the data\n2a. Data input errors and column formats\nLets check for NA or missing values.\n\nhead(all_data)\n\n# A tibble: 6 × 15\n  event.id location.long location.lat   DOP FixType herd  study.specific.measu…¹\n     &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;                 \n1   2.27e9         -124.         55.9     1 val. G… Scott Summer                \n2   2.27e9         -124.         55.9     1 val. G… Scott Summer                \n3   2.27e9         -124.         55.9     1 val. G… Scott Summer                \n4   2.27e9         -123.         55.8     1 val. G… Scott Summer                \n5   2.27e9         -123.         55.9     1 val. G… Scott Winter                \n6   2.27e9         -123.         55.9     1 val. G… Scott Summer                \n# ℹ abbreviated name: ¹​study.specific.measurement\n# ℹ 8 more variables: sensor.type &lt;chr&gt;, tag.local.identifier &lt;chr&gt;,\n#   date &lt;chr&gt;, animal.id &lt;chr&gt;, animal.sex &lt;chr&gt;,\n#   animal.reproductive.condition &lt;chr&gt;, tag.manufacturer.name &lt;chr&gt;,\n#   tag.model &lt;chr&gt;\n\n# check if there are NA's in the data \napply(all_data, 2, function(x) any(is.na(x)))\n\n                     event.id                 location.long \n                        FALSE                          TRUE \n                 location.lat                           DOP \n                         TRUE                         FALSE \n                      FixType                          herd \n                        FALSE                          TRUE \n   study.specific.measurement                   sensor.type \n                         TRUE                         FALSE \n         tag.local.identifier                          date \n                        FALSE                         FALSE \n                    animal.id                    animal.sex \n                        FALSE                         FALSE \nanimal.reproductive.condition         tag.manufacturer.name \n                         TRUE                         FALSE \n                    tag.model \n                        FALSE \n\n\nSeveral columns contain NA’s and need more exploration.\n\n# filter missing values for latitude, longitude and date.\ntdata &lt;- all_data |&gt; \n  filter(!is.na(date)) |&gt; # keep any rows which are not NA\n  filter(!is.na(location.long)) |&gt; \n  filter(!is.na(location.lat))\n\n# Herd\nunique(tdata$herd)\n\n[1] \"Scott\"      \"Burnt Pine\" NA          \n\n# two missing herd values which we can fill in (or delete)\n\ntdata &lt;- tdata |&gt; \n  mutate(herd = case_when(\n    animal.id == \"BP_car043\" ~ \"Burnt Pine\", \n    animal.id == \"SC_car170\" ~ \"Scott\",\n    .default = herd\n  ))\n\n## rerun the NA check to confirm \n\n# apply(tdata, 2, function(x) any(is.na(x)))\n\n2b. Dealing with date-times\nTimestamps can be confusing and problematic, especially when working with multiple programs and file types. One solution is to split the date into separate components (year, month, day, time).\nWe firstly need to format the raw data into a date format using lubridate package.\n\n# calculate time differences\ntdata &lt;- tdata  |&gt; \n  mutate(date_time = ymd_hms(date)) \n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `date_time = ymd_hms(date)`.\nCaused by warning:\n!  5 failed to parse.\n\n# owch we still have an error in this data set\n\n# lets see if we can find it..... \n\nhead(sort(unique(tdata$date)))\n\n[1] \"2003-12-12 13:03:29.000\" \"2003-12-13 09:03:10.000\"\n[3] \"2003-12-14 05:03:11.000\" \"2003-12-15 01:03:10.000\"\n[5] \"2003-12-15 21:04:00.000\" \"2003-12-16 17:03:10.000\"\n\ntail(sort(unique(tdata$date)))\n\n[1] \"2016-03-16 10:01:00.000\" \"2016-03-16 17:49:00.000\"\n[3] \"2016-03-16 19:49:00.000\" \"2016-03-17 17:53:00.000\"\n[5] \"2016-03-17 19:53:00.000\" \"NA\"                     \n\n# Note this is not the same format as an NA we searched for above. i.e. \"NA\" is not equal to NA. \n\n# lets remove the NA \n\ntdata &lt;- tdata  |&gt; \n  filter(date != \"NA\")\n\n# re-run the code to convert to a date format. \n\ntdata  &lt;- tdata  |&gt; \n  mutate(date_time = ymd_hms(date)) \n\n# lets take a look \n\nhead(tdata$date_time) \n\n[1] \"2013-09-23 10:01:00 UTC\" \"2013-10-09 10:01:00 UTC\"\n[3] \"2013-10-30 10:01:00 UTC\" \"2014-08-11 10:01:00 UTC\"\n[5] \"2013-11-10 02:01:00 UTC\" \"2014-08-07 10:01:00 UTC\"\n\n# Note the Universal Coordinated Time Zone\n\n\n# lets split this data format into something more useful \n\ntdata  &lt;- tdata  |&gt; \n  mutate(year = year(date_time )) |&gt; \n  mutate(month = month(date_time ),\n         day = day(date_time),\n         hour = hour(date_time),\n         minute = minute(date_time))\n\n2c. Spatial accuracy QA.\nNext we will review the spatial accuracy of the point data. We have two columns which contain accuracy metrics: DOP (Dilution of Precision), and Fix Type.\nNote: For quick review of spatial errors it is helpful to view the data in QGIS or built a quick map or vizualisation (this is covered later in the workshop).\n\n# review Latitudes and Longitude values \nrange(tdata$location.lat)\n\n[1]  55.2249 155.4764\n\nhist(tdata$location.lat)\n\n\n\n# Note some points are above 65 Latitude\nrange(tdata$location.long)\n\n[1] -123.6684  -22.0000\n\nhist(tdata$location.long)\n\n\n\n# Lets set an upper limit and filter values which are within the range we want.\ntdata &lt;- tdata |&gt; \n  filter(location.long &lt;= -100) |&gt; \n  filter(location.lat &lt;= 65)\n\n2d. Spatial Precision QA.\nTo refine the level of precision we can use the metrics DOP (Dilution of Precision) and Fixtype information which is associated with each location observation.\n\nrange (tdata$DOP)\n\n[1]  1.0 43.1\n\nhist(tdata$DOP)\n\n\n\n# We only want to keep fixes with a DOP less than 10m\nfdata &lt;- tdata |&gt; \n  filter(DOP &lt;= 10)\n\nThe Fixtype is generally classified into categories, based on the level of precision of the calculated location i.e. the number of satellite used to triangulate the fixes. These metrics will vary with the device type and model. More information will be available through the manufacturer website (For example: Lotek Iridium collars ).\n\n# check the number of records per fix type\nfdata |&gt; \n  group_by(FixType) |&gt; \n  summarise(count = n())\n\n# A tibble: 3 × 2\n  FixType     count\n  &lt;chr&gt;       &lt;int&gt;\n1 GPS-2D         92\n2 GPS-3D        291\n3 val. GPS-3D 16801\n\n# We will only keep the higher level of precision\nfdata &lt;- fdata |&gt; \n  filter(FixType != \"GPS-2D\")\n\n# see what the data looks like\nglimpse(fdata)\n\nRows: 17,092\nColumns: 21\n$ event.id                      &lt;dbl&gt; 2270202009, 2270202041, 2270202100, 2270…\n$ location.long                 &lt;dbl&gt; -123.6036, -123.5987, -123.5903, -123.49…\n$ location.lat                  &lt;dbl&gt; 55.90000, 55.87343, 55.87470, 55.83741, …\n$ DOP                           &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ FixType                       &lt;chr&gt; \"val. GPS-3D\", \"val. GPS-3D\", \"val. GPS-…\n$ herd                          &lt;chr&gt; \"Scott\", \"Scott\", \"Scott\", \"Scott\", \"Sco…\n$ study.specific.measurement    &lt;chr&gt; \"Summer\", \"Summer\", \"Summer\", \"Summer\", …\n$ sensor.type                   &lt;chr&gt; \"gps\", \"gps\", \"gps\", \"gps\", \"gps\", \"gps\"…\n$ tag.local.identifier          &lt;chr&gt; \"car170\", \"car170\", \"car170\", \"car170\", …\n$ date                          &lt;chr&gt; \"2013-09-23 10:01:00.000\", \"2013-10-09 1…\n$ animal.id                     &lt;chr&gt; \"SC_car170\", \"SC_car170\", \"SC_car170\", \"…\n$ animal.sex                    &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", …\n$ animal.reproductive.condition &lt;chr&gt; \"with calf: N\", \"with calf: N\", \"with ca…\n$ tag.manufacturer.name         &lt;chr&gt; \"ATS\", \"ATS\", \"ATS\", \"ATS\", \"ATS\", \"ATS\"…\n$ tag.model                     &lt;chr&gt; \"GPS Iridium\", \"GPS Iridium\", \"GPS Iridi…\n$ date_time                     &lt;dttm&gt; 2013-09-23 10:01:00, 2013-10-09 10:01:0…\n$ year                          &lt;dbl&gt; 2013, 2013, 2013, 2014, 2013, 2014, 2014…\n$ month                         &lt;dbl&gt; 9, 10, 10, 8, 11, 8, 9, 12, 7, 11, 6, 12…\n$ day                           &lt;int&gt; 23, 9, 30, 11, 10, 7, 19, 6, 8, 24, 20, …\n$ hour                          &lt;int&gt; 10, 10, 10, 10, 2, 10, 2, 18, 18, 2, 2, …\n$ minute                        &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 0, 0, 1…\n\n# lets check if this column is any use? \n# unique(fdata$sensor.type)\n\nThe final step is to remove redundant columns.\n\nfdata &lt;- fdata |&gt; \n  select(-FixType, -DOP, -date, -study.specific.measurement, -sensor.type, -event.id)\n\n3. Export cleaned data\n3a. Export as a table\n\nwrite_csv(fdata, \"clean_data/caribou.csv\")\n\n3b. Convert to spatial file and export\n\n# convert to a sf object and transform to BC Albers\nbou &lt;- st_as_sf(fdata, coords = c(\"location.long\", \"location.lat\"), \n                crs = 4326, remove = FALSE) |&gt; \n  st_transform(3005)\n\nexport as .gpkg\n\nwrite_sf(bou, \"clean_data/caribou.gpkg\")\n\nWe can also export to a .shp file\n\nwrite_sf(bou, \"clean_data/caribou.shp\")\n\n# note warning on names for shapefile\n\n4. Generating tabular summaries\nNow we have clean data to work with we can get to the fun data exploration part!\n\nbou = read.csv(\"clean_data/caribou.csv\")\n\n#or \n\nbou_sf = read_sf(\"clean_data/caribou.gpkg\")\n\nbou &lt;- st_drop_geometry(bou_sf)\n\nMany questions we can ask here:\n\nhow many herds do we have?\nhow many animals in each herd?\nwhat is the sex ratio of collared animals?\nwhat is the duration of each collar? Start and end years?\n\nLets start with the basics:\n\n# how many herds?\nno_herds = unique(bou$herd)\n\n#How many records per herd ?\nno_records &lt;- bou |&gt; \n  group_by(herd) |&gt; \n  summarise(count = n())\n\n# number of animal records per herd?\nno_animals_id &lt;- bou |&gt; \n  group_by(herd, animal.id) |&gt; \n  summarise(count = n())\n\n`summarise()` has grouped output by 'herd'. You can override using the\n`.groups` argument.\n\n# animals by sex by herd ?\nno_animals_sex &lt;- bou |&gt; \n  group_by(herd, animal.sex) |&gt; \n  summarise(count = n())\n\n`summarise()` has grouped output by 'herd'. You can override using the\n`.groups` argument.\n\n# type of collar by herd by collar model?\ncollar_type &lt;- bou |&gt; \n  group_by(herd, tag.manufacturer.name, tag.model) |&gt; \n  summarise(count = n())\n\n`summarise()` has grouped output by 'herd', 'tag.manufacturer.name'. You can\noverride using the `.groups` argument.\n\n\nThere is lots of data here, so to make it easy we will just use the Scott herd.\n\n# Filter the Scott herd only \nsbou &lt;- bou |&gt; \n  filter(herd == \"Scott\")\n\n# tidy the data by removing columns that are redundant\nsbou &lt;- sbou |&gt; select(-herd, -tag.local.identifier, -tag.manufacturer.name, -tag.model)\n\n# how many animals?\nno_sbou &lt;- unique(sbou$animal.id)\n\nQuestion: What is the spread of fixes per season?\nNext we format the date variable so we can filter by months and years. We can also assign fixes to seasons based on the following dates :\n\nSpring/calving (April,May)\nSummer (June to August)\nFall (September to November)\nWinter (December to March)\n\n\nsbou &lt;- sbou |&gt; \n  mutate(season = case_when(\n            month %in% c(4,5) ~ \"spring\",\n            month %in% c(6,7,8) ~ \"summer\",\n            month %in% c(9,10,11) ~ \"fall\",\n            month %in% c(12,1,2,3) ~ \"winter\")) \n  \n# check data spread\ncounts.per.season = sbou |&gt; \n  group_by(season, animal.id) |&gt; \n  summarise(count = n())\n\n`summarise()` has grouped output by 'season'. You can override using the\n`.groups` argument.\n\nggplot(counts.per.season, aes(x = season, y = count)) + \n  geom_bar(stat = \"identity\") + \n  facet_wrap(~animal.id) +\n  labs(x = \"season\", y = \"no.of.fixes\", title = \"Scott Herd\")\n\n\n\n\nThen we can save our plot\n\nggsave(\"caribou_fixes_per_id.jpeg\", width = 10, height = 10, units = \"cm\")\n\nQuestion: What is the duration of the collar data ?\nThe duration of the collars is also an important question. These can tell us not only the first and last fix, but also point to potential errors or issues we need to consider for future analysis.\nLets calculate the minimum, maximum and duration between fix times.\n\n# which years were the collars active?\nggplot(sbou, aes(year, fill = animal.id)) +\n  geom_bar(position = \"dodge\")\n\n\n\n# calculate the minimum and max dates per collar \ntable_max &lt;- sbou |&gt; \n  select(animal.id, date_time) |&gt; # select the columns of interest\n  slice_max(date_time, by = animal.id)  # select the max value of date_time, per animal.id\ncolnames(table_max)&lt;- c(\"animal.id\",\"max\")\n\ntable_min &lt;- sbou |&gt; \n  select(animal.id, date_time) |&gt; \n  slice_min(date_time, by = animal.id) \n\ncolnames(table_min)&lt;- c(\"animal.id\",\"min\")\n\n# merge the two tables together and calculate the duration \ndur &lt;- left_join(table_max, table_min, by = join_by(animal.id)) |&gt; \n  distinct() |&gt; \n  mutate(duration = max - min) |&gt; # calculate duration (default is days)\n  mutate(dur_days = round( duration, 1)) |&gt; # format the duration \n  mutate(dur_hrs = round(as.numeric(dur_days)*24,1)) |&gt; # convert to hours\n  mutate(year_start = year(min), \n         year_end = year(max))\n\n# plot total duration of collar data \nggplot(dur, aes(y=factor(animal.id))) +\n  geom_segment(aes(x=min, xend=max, y=factor(animal.id), yend=factor(animal.id)), linewidth = 3)+\n  xlab(\"Date\") + ylab(\"Tag\") \n\n\n\nggplot(sbou, aes(factor(month), fill = factor(year)))+\n  geom_bar(position = \"dodge\") +\n  facet_wrap(~animal.id)+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n5. Standardize fix interval times\nAnother data processing method we might need to consider is having a standard density or fix rate for all animals. This may not be required, depending on the research question being asked, but it is useful to assess measures of density.\n\nsbou &lt;- sbou |&gt; \n  arrange(animal.id, date_time)\n\nsbou_dur &lt;- sbou |&gt; \n    mutate(time = as.POSIXct(date_time, format = \"%y/%d/%m %H:%M:%S\")) |&gt; \n    group_by(animal.id) |&gt; \n    mutate(diff = difftime(time, lag(time),  units = c(\"hours\")), \n           diff = as.numeric(diff))\n\n# we can see a big range in the time intervals for the fixes\nrange(sbou_dur$diff, na.rm = TRUE)\n\n[1]   0.01666667 111.75000000\n\n# most fall in the less than than 10 \nhist(sbou_dur$diff)\n\n\n\n# lets look at the individual animals\nggplot(sbou_dur, aes(diff)) + \n  geom_histogram(bins=30) +\n  facet_grid(.~animal.id)\n\nWarning: Removed 4 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n# much of the problem is with the SC_car171 individual \nggplot(sbou_dur, aes(y = diff, x = date_time)) + \n  geom_point() +\n  facet_wrap(.~animal.id, scales = \"free\")\n\nWarning: Removed 4 rows containing missing values (`geom_point()`).\n\n\n\n\n\nThe number of fixes are relatively steady throughout the years for all individuals except SC_car171? Something looks strange here as there are large spikes in October 2015 and February 2016. Potential mortality signals?\nTo create a standardized fix per day, lets take the first fix per day. This could be based on a number of factors, depending on our research question we want to ask. We can use Julian date to filter all fixes.\n\n# we can subset base on a Julian date\nsbou_dur &lt;- sbou_dur |&gt; \n  mutate(\n    date2=as.Date(date_time, format = '%Y-%m-%d'), # convert to date type\n    jdate=julian(date2)               # create new column and calculate Julian date \n  )\n\n# Check the multiple counts of animals per day \ncounts.per.day.jdate &lt;- sbou_dur |&gt; \n  group_by(animal.id, jdate, year) |&gt; \n  summarise(total = n(), unique = unique(jdate)) |&gt; \n  group_by(animal.id, total, year) |&gt; \n  summarise(total.j = n()) \n\n`summarise()` has grouped output by 'animal.id', 'jdate'. You can override\nusing the `.groups` argument.\n`summarise()` has grouped output by 'animal.id', 'total'. You can override\nusing the `.groups` argument.\n\n# lets plot this data to make more sense of what is happening\nggplot(counts.per.day.jdate, aes(x = total, y = total.j)) + \n  geom_point() +\n  geom_vline(xintercept = 1, color = \"red\")+\n  facet_wrap(.~animal.id + year)\n\n\n\n# lets select the first fix per day \nsbou_sub &lt;- sbou_dur |&gt; \n    group_by(animal.id, jdate) |&gt; \n    slice_sample( n = 1) \n\nWe can write out the standardized fix rate file for future use in analysis.\n\n# lets write this out as .csv and .Gpkg\nwrite_csv(sbou_sub, \"clean_data/scott_herd_subset.csv\")\n\nAlternatively we can convert to a sf object and export. Note this is required as we previously remove the spatial components to undertake the summaries above.\n\nsbou_sub_sf &lt;- st_as_sf(sbou_sub, coords = c(\"location.long\", \"location.lat\"), crs = 4326, remove = FALSE) |&gt; \n  st_transform(3005)\n\nExport as .gpkg\n\n# # export as .gpkg\nwrite_sf(sbou_sub_sf, \"clean_data/scott_herd_subset.gpkg\")\n\n\n# plot months of the year. \nggplot(sbou, aes(factor(month), fill = factor(year))) +\n  geom_bar(position = \"dodge\") +\n  facet_wrap(~animal.id)+\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))"
  },
  {
    "objectID": "03-intro-telemetry-data.html#your-turn",
    "href": "03-intro-telemetry-data.html#your-turn",
    "title": "Introduction to Telemetry Data",
    "section": "Your Turn",
    "text": "Your Turn\n\nUse the Burnt Pine herd to answer the following questions:\n\n\nhow many animals in this herd?\nhow many records are in this herd per animal?\nwhat is the sex ratio of collared animals?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbou_sf = read_sf(\"clean_data/caribou.gpkg\")\n\nbp &lt;- bou_sf |&gt; \n  filter(herd == \"Burnt Pine\")\n\nno_animals = unique(bp$animal.id)\n\nno_records_per_animal &lt;- bp |&gt; \n  group_by(animal.id) |&gt; \n  summarise(count = n())\n\nno_animals_sex &lt;- bp |&gt; \n  group_by(animal.sex) |&gt; \n  summarise(count = n())"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2023 r-telemetry-workshop-nov-2023 authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "08-rsf-prep.html",
    "href": "08-rsf-prep.html",
    "title": "Generate random sample points for RSF",
    "section": "",
    "text": "In this module we use our prepared spatial data to generate the input data files to conduct a resource selection function analysis. This includes:\n\ngenerate a set of “background points” from our study area.\nextract spatial information for presence and background point locations\nexport as table for future use\ngenerate summary statistics"
  },
  {
    "objectID": "08-rsf-prep.html#overview",
    "href": "08-rsf-prep.html#overview",
    "title": "Generate random sample points for RSF",
    "section": "",
    "text": "In this module we use our prepared spatial data to generate the input data files to conduct a resource selection function analysis. This includes:\n\ngenerate a set of “background points” from our study area.\nextract spatial information for presence and background point locations\nexport as table for future use\ngenerate summary statistics"
  },
  {
    "objectID": "08-rsf-prep.html#background-resource-selection-functions",
    "href": "08-rsf-prep.html#background-resource-selection-functions",
    "title": "Generate random sample points for RSF",
    "section": "Background: Resource Selection Functions",
    "text": "Background: Resource Selection Functions\nNow that we have a cleaned and standardized data set for the Scott Herd Caribou’s we can prepare the data for further analysis.\nResource Selection Functions are a common method used to assess what are the driving patterns of animal habitat preference. This process uses information or covariates (i.e. landscape attribute features) for locations where animals are present and compares them to all possible locations. In this way we can gather information on what conditions (i.e. landscape, aspect, distance from road, etc) characterize higher habitat use and selection for.\nIn addition to preparing data for future analysis, understanding the association with telemetry point locations and landscape information can provide meaningful summary statistics, i.e. proportion of locations within a specific BEC zone."
  },
  {
    "objectID": "08-rsf-prep.html#generate-background-location-points.",
    "href": "08-rsf-prep.html#generate-background-location-points.",
    "title": "Generate random sample points for RSF",
    "section": "1. Generate background location points.",
    "text": "1. Generate background location points.\nWe will generate a simple set of “background points” based on the geographic distribution of the study area. Note you can limit the area in which these points are drawn from using more sophisticated methods, such as restricting points within a kernal density or home range estimate.\nLets start by reading in the libraries we will use.\n\nlibrary(dplyr)\nlibrary(terra)\nlibrary(sf)\nlibrary(mapview)\nlibrary(ggplot2)\n\nNext, read in our point locations and rater template.\n\n# read in the aoi template \ntemplate &lt;- rast(\"clean_data/template.tif\")\n\n# read in points \nbou_pts &lt;- st_read(\"clean_data/scott_herd_subset.gpkg\") \n\nReading layer `scott_herd_subset' from data source \n  `/Users/andy/dev/r-telemetry-workshop-nov-2023/clean_data/scott_herd_subset.gpkg' \n  using driver `GPKG'\nSimple feature collection with 2906 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1145474 ymin: 1190327 xmax: 1185596 ymax: 1227451\nProjected CRS: NAD83 / BC Albers\n\n# Lets keep only the important columns and add a \"presence/background\" column. \nbou_pts &lt;- bou_pts |&gt;\n  dplyr::select(animal.id, jdate)|&gt;\n  mutate(pres_bkg = 1)\n\nWe can use the spatSample function to generate random points for our given study area. This function has many more options which can be reviewed by using : ?spatSample in the console.\nLets generate a set of points the same length as our “presence” locations using a “random” method.\n\n# Generate random points for RSF use areas.\nset.seed(123)\navail_points &lt;- spatSample(template, size = 2906, as.points = TRUE, na.rm = TRUE, method = \"random\")\n\navail_points &lt;- st_as_sf(avail_points)\n\n# lets rename the column to make it clear these are background points \navail_points &lt;- avail_points |&gt;\n  rename(\"pres_bkg\" = aoi )\n\nWe can do a quick review of the points using mapview\n\nmapview(avail_points) +\nmapview(bou_pts, color = \"red\", cex= 3)\n\n\n\n\n\n\nWe can now combine our caribou locations and “background” locations into a single data set. We will retain the spatial information to allow us to easily extract the values in the corresponding raster stack\n\nhead(avail_points)\n\nSimple feature collection with 6 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1155662 ymin: 1195888 xmax: 1185438 ymax: 1225412\nProjected CRS: NAD83 / BC Albers\n  pres_bkg                geometry\n1        0 POINT (1155662 1195888)\n2        0 POINT (1185438 1225412)\n3        0 POINT (1166262 1200912)\n4        0 POINT (1179562 1198038)\n5        0 POINT (1179038 1219712)\n6        0 POINT (1162338 1221188)\n\nhead(bou_pts)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1164599 ymin: 1211860 xmax: 1165761 ymax: 1213029\nProjected CRS: NAD83 / BC Albers\n  animal.id jdate                    geom pres_bkg\n1 SC_car168 15789 POINT (1164855 1211860)        1\n2 SC_car168 15790 POINT (1164599 1211888)        1\n3 SC_car168 15791 POINT (1165702 1212694)        1\n4 SC_car168 15792 POINT (1165754 1212486)        1\n5 SC_car168 15793 POINT (1165668 1213029)        1\n6 SC_car168 15794 POINT (1165761 1212577)        1\n\nallpts &lt;- bind_rows(bou_pts, avail_points) \n\nhead(allpts)\n\nSimple feature collection with 6 features and 3 fields\nActive geometry column: geom\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1164599 ymin: 1211860 xmax: 1165761 ymax: 1213029\nProjected CRS: NAD83 / BC Albers\n  animal.id jdate pres_bkg                    geom    geometry\n1 SC_car168 15789        1 POINT (1164855 1211860) POINT EMPTY\n2 SC_car168 15790        1 POINT (1164599 1211888) POINT EMPTY\n3 SC_car168 15791        1 POINT (1165702 1212694) POINT EMPTY\n4 SC_car168 15792        1 POINT (1165754 1212486) POINT EMPTY\n5 SC_car168 15793        1 POINT (1165668 1213029) POINT EMPTY\n6 SC_car168 15794        1 POINT (1165761 1212577) POINT EMPTY\n\n# hmmmmm...........that looks weird what happened? \n\n# note we have slightly different column headers \"geom\" vs \"geometry\" \nst_geometry(avail_points) = \"geom\" \n\nallpts &lt;- bind_rows(bou_pts, avail_points)"
  },
  {
    "objectID": "08-rsf-prep.html#extract-point-values",
    "href": "08-rsf-prep.html#extract-point-values",
    "title": "Generate random sample points for RSF",
    "section": "Extract point values",
    "text": "Extract point values\nNext, we can read in our prepared raster stack as an .rds object. We can now use the extract function from the terra package to extract information for all layers in the raster stack for each of our points.\n\n# read in the raster stack \nrstack &lt;- readRDS(\"clean_data/covars.RDS\")\n\n# extract all values in the raster stack for each location in the bou_pts file. \natts &lt;- terra::extract(rstack, allpts)\n\nhead(atts)\n\n  ID MAP_LABEL conif age_class cc_class HARVEST_YEAR WATERBODY_TYPE\n1  1   ESSFwc3  &lt;NA&gt;        NA       NA           NA           &lt;NA&gt;\n2  2   ESSFwc3    TC         9        5           NA           &lt;NA&gt;\n3  3   ESSFwcp    TC         8        3           NA           &lt;NA&gt;\n4  4   ESSFwcp    TC         8        3           NA           &lt;NA&gt;\n5  5   ESSFwcp  &lt;NA&gt;         8        1           NA           &lt;NA&gt;\n6  6   ESSFwcp    TC         8        3           NA           &lt;NA&gt;\n  STREAM_ORDER ROAD_SURFACE     layer focal_sum elevation     slope     aspect\n1           NA         &lt;NA&gt; 1.5773045        NA  1411.423 14.082280   2.045941\n2           NA         &lt;NA&gt; 1.6258846        NA  1392.466 13.354155 320.360718\n3           NA         &lt;NA&gt; 0.9053469        NA  1601.210  8.430601 138.105759\n4           NA         &lt;NA&gt; 1.0990066        NA  1575.208  5.037906  95.403023\n5           NA         &lt;NA&gt; 0.6147004        NA  1634.010  9.453105  94.128174\n6           NA         &lt;NA&gt; 1.0157410        NA  1583.300  6.978374 175.275101\n       TRI\n1 4.809753\n2 4.420059\n3 2.718521\n4 1.784927\n5 3.197327\n6 2.330856\n\n# remove unused columns \n# Could show how to use st_write() and explain the cbind(st_coordinates()) bit\nbou_full_pts &lt;- cbind(allpts, atts) |&gt;\n  select(-ID)\n\nExport as geo package or as csv table.\n\n# spatial file \nwrite_sf(bou_full_pts, \"clean_data/allpts_att.gpkg\")\n\n# write out as csv, keeping the XY values\nbou_table &lt;- st_coordinates(bou_full_pts) |&gt; \n  cbind(bou_full_pts) |&gt; \n  st_drop_geometry()\n\nwrite.csv(bou_table, \"clean_data/allpts_att.csv\")"
  },
  {
    "objectID": "08-rsf-prep.html#summarise-attributed-point-data",
    "href": "08-rsf-prep.html#summarise-attributed-point-data",
    "title": "Generate random sample points for RSF",
    "section": "3. Summarise attributed point data",
    "text": "3. Summarise attributed point data\nWe can use our attributed point data to provide valuable summaries of landscape feature. Lets looks at aspect and BEC zones.\n\n# Aspect\nggplot(bou_table, aes(aspect)) +\n  geom_histogram(binwidth = 20) +\n  facet_wrap(~pres_bkg)\n\nWarning: Removed 8 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n# BEC zones \nggplot(bou_table, aes(MAP_LABEL, group = pres_bkg, fill = factor(pres_bkg))) +\n  geom_bar(position = \"dodge\", show.legend = TRUE)"
  },
  {
    "objectID": "08-rsf-prep.html#your-turn",
    "href": "08-rsf-prep.html#your-turn",
    "title": "Generate random sample points for RSF",
    "section": "Your turn",
    "text": "Your turn\n\nGenerate a second set of background points used a method other than “random”.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\navail_pts_regular &lt;- spatSample(template, size = 50, as.points = TRUE, na.rm = TRUE, method = \"regular\")\nmapview(avail_pts_regular)\n\n\n\n\n\n\n\n\n\n\nUse the bou_table we created above to explore other landscape patters. Build a ggplot to show the differences between the presence and background points.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Here are a few example using elevation and distance to water. \n# elevation \nggplot(bou_table, aes(elevation)) +\n  geom_histogram(binwidth = 20) +\n  facet_wrap(~pres_bkg)\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n# distance to water (layer)\nggplot(bou_table, aes(layer)) +\n  geom_histogram() +\n  facet_wrap(~pres_bkg)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`)."
  },
  {
    "objectID": "bonus-movebank.html",
    "href": "bonus-movebank.html",
    "title": "Downloading telemetry data from Movebank",
    "section": "",
    "text": "In this module we will introduce the move2 r package and demonstrate how you can download telemetry data directly from the movebank web repository via an API."
  },
  {
    "objectID": "bonus-movebank.html#overview",
    "href": "bonus-movebank.html#overview",
    "title": "Downloading telemetry data from Movebank",
    "section": "",
    "text": "In this module we will introduce the move2 r package and demonstrate how you can download telemetry data directly from the movebank web repository via an API."
  },
  {
    "objectID": "bonus-movebank.html#background",
    "href": "bonus-movebank.html#background",
    "title": "Downloading telemetry data from Movebank",
    "section": "Background",
    "text": "Background\nMovebank is a free, online database of animal movement data hosted by the Max Planck Institute for Ornithology. Data owners can manage their data and have the option to share it with colleagues or the public. If the public or a registered user has permission to see a study, the study can be downloaded as a .csv file and imported directly into R using the move2 package\nNote move2 this is an updated version of the move package\n\n#install.packages(\"move2\")\nlibrary(\"move2\")\n\nlibrary(dplyr, quietly = TRUE)\n\n\nYou will need to create a log in on the movebank platform to be able to download and view data. This can be done at Movebank.\nSet up your credentials within the move2 package. To access any data through R you will also need to supply the credentials you used to create your movebank account to the move2 package. This package uses the keyring package to safely store your credentials. You only need to run the following code once, after which the credentials will be remembered for following R sessions.\n\nNOTE : make sure to keep your credential safe, and do not committ these to a common code platform such as github.\n\nmovebank_store_credentials(\"myUserName\", \"myPassword\")\n\n\n#movebank_store_credentials(\"gcperkins\", \"*****\")\n\n# to delete your credentials run this line\n#movebank_remove_credentials()\n\n# you can check by running this \nkeyring::key_list()"
  },
  {
    "objectID": "bonus-movebank.html#extra-challenge",
    "href": "bonus-movebank.html#extra-challenge",
    "title": "Downloading telemetry data from Movebank",
    "section": "Extra challenge!",
    "text": "Extra challenge!\nIf you are interested in viewing the entire Caribou data set you can find it on movebank (study_id = 216040785)\n\n# download the study info details\nbou &lt;- movebank_download_study_info(study_id = 216040785)\n\n# download the data \nbou_data &lt;- movebank_download_study(216040785, attributes = NULL)\n\n# download deployment data \nbou_ref_data &lt;- movebank_download_deployment(216040785)\n\nReferences:\n\nMovebank.\nmove2 r package vignette - “Downloading data from movebank”"
  },
  {
    "objectID": "00-intro-setup.html",
    "href": "00-intro-setup.html",
    "title": "Introduction and System Setup",
    "section": "",
    "text": "This two-day workshop will introduce you to the basics of working with telemetry data in R.\nWe will providing a data set to use throughout the workshop, but we encourage you to bring along your own data for the hackathon session (Friday AM).\n\n\nThe basics of spatial data in R\nHow to read in, clean, and QA your telemetry data\nHow to create useful summaries of your data\nData visualization techniques for spatial and telemetry data (graphs and maps)\nHow to use the bcmaps and bcdata packages to get vector and raster data from official BC government sources.\nHow to perform spatial operations to compile and generate landcape covariates\nHow to extract spatial information for telemetry and random location points\n\n\nMethodological details on technical telemetry parameters i.e. DOP estimates\nAdvanced modeling techniques\n\n\n\nSetup and troubleshooting (8:30 - 9:00)\nWorking with spatial data in R (9:00 - 10:30)\n\nBREAK (10:30 - 11:00)\nTelemetry data in R; clean, QA, and prepare data (Part I; 11:00 - 12:30)\n\nLUNCH (12:30 - 1:30)\nTelemetry data in R; clean, QA, and prepare data (Part II; 1:30 - 3:00)\n\nBREAK (3:00 - 3:30)\nVisualizing spatial data in R (3:30 - 4:30)\n\n\nSetup and troubleshooting (8:30 - 9:00)\nRetrieving spatial data from the B.C. Data Catalogue (9:00 - 10:30)\n\nBREAK (10:30 - 11:00)\nGetting and working with raster data (Part I; 11:00 - 12:30)\n\nLUNCH (12:30 - 1:30)\nPreparing base data for telemetry analysis (Part II; 1:30 - 3:00)\n\nBREAK (3:00 - 3:30)\nPreparing data for Resource Selection Function (RSF) analysis (3:30 - 4:30)"
  },
  {
    "objectID": "00-intro-setup.html#overview",
    "href": "00-intro-setup.html#overview",
    "title": "Introduction and System Setup",
    "section": "",
    "text": "This two-day workshop will introduce you to the basics of working with telemetry data in R.\nWe will providing a data set to use throughout the workshop, but we encourage you to bring along your own data for the hackathon session (Friday AM).\n\n\nThe basics of spatial data in R\nHow to read in, clean, and QA your telemetry data\nHow to create useful summaries of your data\nData visualization techniques for spatial and telemetry data (graphs and maps)\nHow to use the bcmaps and bcdata packages to get vector and raster data from official BC government sources.\nHow to perform spatial operations to compile and generate landcape covariates\nHow to extract spatial information for telemetry and random location points\n\n\nMethodological details on technical telemetry parameters i.e. DOP estimates\nAdvanced modeling techniques\n\n\n\nSetup and troubleshooting (8:30 - 9:00)\nWorking with spatial data in R (9:00 - 10:30)\n\nBREAK (10:30 - 11:00)\nTelemetry data in R; clean, QA, and prepare data (Part I; 11:00 - 12:30)\n\nLUNCH (12:30 - 1:30)\nTelemetry data in R; clean, QA, and prepare data (Part II; 1:30 - 3:00)\n\nBREAK (3:00 - 3:30)\nVisualizing spatial data in R (3:30 - 4:30)\n\n\nSetup and troubleshooting (8:30 - 9:00)\nRetrieving spatial data from the B.C. Data Catalogue (9:00 - 10:30)\n\nBREAK (10:30 - 11:00)\nGetting and working with raster data (Part I; 11:00 - 12:30)\n\nLUNCH (12:30 - 1:30)\nPreparing base data for telemetry analysis (Part II; 1:30 - 3:00)\n\nBREAK (3:00 - 3:30)\nPreparing data for Resource Selection Function (RSF) analysis (3:30 - 4:30)"
  },
  {
    "objectID": "00-intro-setup.html#computing-requirements",
    "href": "00-intro-setup.html#computing-requirements",
    "title": "Introduction and System Setup",
    "section": "Computing requirements",
    "text": "Computing requirements\nYou will require the following software installed and configured for the workshop. Please have this set up and ready to go before we start.\nYou will need:\n\nA laptop computer, preferably with administrative privileges\nR and RStudio\nSeveral R packages\n\nQGIS (optional)\n\nInstall R and RStudio\nYou will need:\n\nR version &gt;= 4.2.0\nRStudio &gt;= 2023.03.1\n\nInstall R\nDownload and install R for your operating system from https://cloud.r-project.org/.\nWindows with no admin rights:\nIf you do not have administrator rights, the installer will default to installing somewhere in your user folder (e.g., C:/Users/username/AppData/Local/Programs/). If you prefer, you can change the location to another folder that you have write access to, just make sure it is on your C:/ drive.\nInstall R Studio\nDownload and install RStudio Desktop from https://posit.co/download/rstudio-desktop/. This page should automatically offer you the version suitable for your operating system, but you can scroll down to find versions for all operating systems.\nWindows with admin rights:\nDownload the .zip archive for Windows under “Zip/Tarballs”. Create a folder called RStudio in a location on your C:/ drive, where you have write access (e.g. C:/Users/username/AppData/Local/Programs/RStudio), and extract the zip file into this folder. Find the RStudio program in this folder: it is named rstudio.exe, but the file extension will typically be hidden, so look for rstudio. Right-click this file to create a shortcut and drag it to your desktop/task bar. Use the shortcut to open RStudio.\nInstall packages\nIn R, install the necessary packages by running:\n\ninstall.packages(\n  c(\"tidyverse\", \"sf\", \"terra\", \"mapview\", \"bcdata\", \"bcmaps\", \"readxl\", \n    \"ggplot2\", \"usethis\", \"lubridate\", \"ggdensity\",\"ggspatial\", \"prettymapr\")\n)\n\nQGIS (optional)\nQGIS is a free and open-source geographic information system (GIS) that allows you to create, edit, visualize, analyze and publish geospatial information.\nDownload and install from the QGIS website."
  },
  {
    "objectID": "00-intro-setup.html#workshop-materials-and-data",
    "href": "00-intro-setup.html#workshop-materials-and-data",
    "title": "Introduction and System Setup",
    "section": "Workshop materials and data",
    "text": "Workshop materials and data\nWe have created an empty RStudio project containing the workshop data.\nThe easiest way to get started to run:\n\nusethis::use_course(\n  \"https://github.com/ninoxconsulting/r-telemetry-workshop-nov-2023/raw/main/r-telemetry-workshop.zip\"\n  )\n\nThis will download the workshop materials, save them in a logical spot on your computer, and open the project in a new RStudio session.\nAlternatively, you can download the zip file from here, decompress the zip file somewhere convenient on your C:/ drive. Then, navigate to the newly created folder (called r-telemetry-workshop), and double click on the r-telemetry-workshop.Rproj file to open the project in RStudio."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Telemetry Workshop, November 2023",
    "section": "",
    "text": "Welcome!\nWe are happy to be working with you to step through the process to use R to clean, summarise and analyse your telemetry data. This course will be for basic to medium R coders, but never fear, we will have material to follow along with and to take home so don’t worry!\nEvent details\n\nWhere: Cedar Boardroom, 2000 South Ospika Blvd, Prince George.\nWhen: November 15th, 16th, 17th 2023\nTimes: The course will be two days (8:30 - 4:30pm, 15th and 16th November) followed by a half day hackathon (8:30 - 12:30, 17th November). We will have breaks for coffee and lunch. Please bring your own food and snacks.\nWhat to bring: Bring your laptop with installed versions of R and Rstudio. For setup instructions go to setup instructions. Help will be available for the first 30 minutes before the course starts (8:30 - 9:00am) to help trouble shoot any computer problems\n\nWhat is a hackathon? A hackathon is an opportunity to work with your peers on common problems. This might be talking through options, paired coding or splitting up to separate tasks. We will provide more input during the course to help you best use this time.\nIf you have a data set you would like to work on, please bring it with you and you can use this for the hackathon.\nThank-you\nThank-you to Ministry of Forests, Government of British Columbia for sponsoring this workshop."
  },
  {
    "objectID": "bonus-kde.html",
    "href": "bonus-kde.html",
    "title": "Generating Home Range Estimates",
    "section": "",
    "text": "In this module we will create home range estimates using minimum convex polygons and kernel density estimates (kde) exploring a range of methods available.\n\n\nlibrary(sf)\nlibrary(mapview)\nlibrary(adehabitatHR)\nlibrary(sp)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# read in sf object \nbou &lt;- read_sf(\"clean_data/caribou.gpkg\") \n\n# add seasons \nbou &lt;- bou |&gt; \n  mutate(season = case_when(\n            month %in% c(4,5) ~ \"spring\",\n            month %in% c(6,7,8) ~ \"summer\",\n            month %in% c(9,10,11) ~ \"fall\",\n            month %in% c(12,1,2,3) ~ \"winter\")) \n\n# add coordinates\ntdf &lt;- st_coordinates(bou) |&gt; \n    cbind(bou) |&gt;\n    dplyr::select(location.long, location.lat, herd, animal.id, season) |&gt;  \n    st_drop_geometry()\n\nLets begin by checking we have enough points to be confident in our home range estimate. The total number of raw fixes will vary depending between individuals by season and years. In this example we will use a minimum of 50 unique locations as recommended for home range calculations (Seaman 1999, Kernohan 2001).\nNote: telemetry data is yet to be subset by a specific time interval and currently includes all raw fixes.\n\nid_pts &lt;- tdf |&gt;\n      group_by(animal.id)|&gt;\n      summarise(count = n())\n\nseason_pts &lt;- tdf |&gt;\n      group_by(herd, season)|&gt;\n      summarise(count = n())\n\n\nMinimum convex polygons (mcp) can be used to provide a quick estimate of data distribution. This provides a very broad brush approach and does not include density estimates.\n2.1 Using the sf package.\n\nmcp &lt;- st_convex_hull(st_union(bou))\nplot(mcp)\nplot(bou$geom, col = \"blue\", add = T)\n\n\n\n\nThis is not very representative so what other methods can we use?\n2.2 Using the sp package.\nAlternatively we can use the adhabitatHR package, which contains many ecology specific tools and functions. The downside of using this package is the reliance on old spatial package (sp) . The sp package has largely been superseded by the sf package. Unfortunately this is the reality of a coding language which is constantly being updated and improved!\nThe sp package uses a specific format called SpatialPointsDataFrame. To use these functions we need to convert from sf to sp formats.\n\n#  Create SpatialPointsDataFrame object\n  tdfgeo &lt;- bou |&gt;  \n    dplyr::select(animal.id) |&gt;\n    as(\"Spatial\")\n\n  # Calculate MCPs \n  c.mcp &lt;- mcp(tdfgeo, percent = 100)\n  \n  # convert this to a sf object \n  c.mcp_sf &lt;- st_as_sf(c.mcp)\n  \n  # plot with mapview\n  mapview::mapview(c.mcp_sf)\n\n\n\n\n\n\nThe advantage of using the adhabitatHR package is that it provides many more outputs and enables us to customize the information we want. For example we can extract multiple home ranges based on varying levels of specificity; i.e. percent = 75%.\n\n  # lets look at 75% inclusion\n  c.mcp.75 &lt;- mcp(tdfgeo, percent = 75)\n\n   # Plot\n  plot(tdfgeo, col = as.factor(tdfgeo$animal.id), pch = 16)\n  plot(c.mcp , col = alpha(1:5, 0.5), add = TRUE)\n  plot(c.mcp.75, col = alpha(1:5, 0.5), add = TRUE)\n\n\n\n  # Calculate the MCP by including 50 to 100 percent of points\n  hrarea &lt;- mcp.area(tdfgeo, percent = seq(50, 100, by = 10))\n\n\n\n\n\nKernel Density Estimates (kde) are a popular method to estimate the distribution of data, hence are used to estimate home ranges. Calculations for KDE require consideration of the mathematical method to describe the density function. A detailed description of the parameters can be found here.\nKDEs are very sensitive to input parameters, specifically the bandwidth (h) which determines the smoothing parameter. H parameters can be estimated using three methods:\n\na reference bandwidth (h = σ × n ^−1/6), however this is generally an overestimate of the range, and is not suitable for multi-modal distributions.\nLeast Square Cross Validation (LSCV), which minimizes the difference in volume between the true UD and the estimates UD.\nA subjective visual choice for the smoothing parameter, based on successive trials (Silverman, 1986; Wand & Jones 1995).\n\nOther parameters include:\n\nKernel Type: The type of kernel is limited to Gaussian (bivariate normal), quadratic or normal. We used the bivariate normal model as default.\nGrid/extent: These determines the area or extent over which the home range will be estimated. This is a mix of fine scale and time consuming processing and faster blocky resolution over a continuous surface. As a rule of thumb, Geospatial Modelling Environment program (GME) formally Hawth’s tools suggest: take the square root of the x or y variance value (whichever is smaller) and divide by 5 or 10 (I usually round to the nearest big number - so 36.7 becomes 40). Before using this rule of thumb value calculate how many cells this will result in for the output (take the width and height of you input points, divide by the cell size, and multiply the resulting numbers together). If you get a value somewhere between 1-20 million, then you have a reasonable value.\n\nTo create home ranges for each unique id we used a bivariate normal kernel with a variety of h (smoothing parameters). We then interpreted visually to determine size to use based on successive trials. This is supported by the literature (Hemson et al. 2005; Calenge et al. 2011).\nIt is also possible to run KDE with set barriers and boundaries.\n\n\ntdfgeo &lt;- bou |&gt;  \n  dplyr::select(animal.id) |&gt;\n  as(\"Spatial\")\n\n# define the parameters (h, kern, grid, extent) \nkde_href  &lt;- kernelUD(tdfgeo, h = \"href\", kern = c(\"bivnorm\"), grid = 500, extent = 2)\n\nkde_href\n\n********** Utilization distribution of several Animals ************\n\nType: probability density\nSmoothing parameter estimated with a  href smoothing parameter\nThis object is a list with one component per animal.\nEach component is an object of class estUD\nSee estUD-class for more information\n\n\nFrom this object (Utilization distribution of several Animals) we can extract the vertices or polygons which define the percentage we wish to include.\n\nver95 &lt;- getverticeshr(kde_href,95) # get vertices for home range\nver95_sf &lt;- st_as_sf(ver95)         # convert to sf object \n  \nver75 &lt;- getverticeshr(kde_href,75)\nver75_sf &lt;- st_as_sf(ver75 )\n  \nver50 &lt;- getverticeshr(kde_href,50)\nver50_sf&lt;- st_as_sf(ver50)\n \n# plot the outputs \nmapview(ver50_sf, zcol = \"id\") \n\n\n\n\n\nmapview (ver75_sf, zcol = \"id\") \n\n\n\n\n\nmapview (ver95_sf, zcol = \"id\")\n\n\n\n\n\nplot(st_geometry(ver95_sf),col = \"yellow\") \nplot(st_geometry(ver75_sf),col = \"blue\", add = TRUE)\nplot(st_geometry(ver50_sf),col = \"purple\", add = TRUE)\nplot(tdfgeo, pch = 1, size = 0.5, add = TRUE)     # Add points \n\n\n\n\n\n\nkde_lscv  &lt;- kernelUD(tdfgeo, h = \"LSCV\", kern = c(\"bivnorm\"), grid = 500, extent = 2)\n\nver95ls &lt;- getverticeshr(kde_lscv,95) # get vertices for home range\nver95ls_sf &lt;- st_as_sf(ver95ls) \n\nver50ls &lt;- getverticeshr(kde_lscv,50)\nver50ls_sf &lt;- st_as_sf(ver50ls) \n\n# plot the outputs \nmapview(ver50ls_sf, zcol = \"id\") \n\n\n\n\n\nmapview (ver95ls_sf, zcol = \"id\")\n\n\n\n\n\n\n\nTo test the sensitivity of the h value we can test a bivariate normal kernel with a variety of smoothing parameters (h = 1000, 2000, 4000).\nWe can then interpreted visually to determine size to use based on successive trials. This approach is supported by literature (Hemson et al. 2005; Calenge et al. 2011).\n\nkde_h1000  &lt;- kernelUD(tdfgeo, h = 1000, kern = c(\"bivnorm\"), grid = 500,extent = 2)\nkde_h500  &lt;- kernelUD(tdfgeo, h = 500, kern = c(\"bivnorm\"), grid = 500,extent = 2)\nkde_h3000  &lt;- kernelUD(tdfgeo, h = 3000, kern = c(\"bivnorm\"), grid = 500,extent = 2)\n\nLets extract the vertices and compare the outputs by building a plot.\n\n# kde - href = 1000\nver95_1000 &lt;- getverticeshr(kde_h1000, 95) # get vertices for home range\nver95_1000_sf &lt;- st_as_sf(ver95_1000) |&gt;     mutate(h = 1000) # convert to sf object \n\n# kde - href = 500\nver95_500 &lt;- getverticeshr(kde_h500, 95) # get vertices for home range\nver95_500_sf &lt;- st_as_sf(ver95_500)  |&gt; \n  mutate(h = 500) # convert to sf object \n  \n# kde - href = 3000\nver95_3000 &lt;- getverticeshr(kde_h3000, 95) # get vertices for home range\nver95_3000_sf &lt;- st_as_sf(ver95_3000)  |&gt; \n  mutate(h = 3000) # convert to sf object \n  \n# bind all data together \nall_verts &lt;- bind_rows(ver95_1000_sf,  ver95_500_sf,  ver95_3000_sf)\n  \n# lets plot the output \nggplot(data = all_verts) +\ngeom_sf(\n    aes(colour = id), \n    alpha = 0.1\n  ) + \n  scale_colour_viridis_d() + \n  facet_wrap(vars(h)) +\n  theme_bw()"
  },
  {
    "objectID": "bonus-kde.html#overview",
    "href": "bonus-kde.html#overview",
    "title": "Generating Home Range Estimates",
    "section": "",
    "text": "In this module we will create home range estimates using minimum convex polygons and kernel density estimates (kde) exploring a range of methods available.\n\n\nlibrary(sf)\nlibrary(mapview)\nlibrary(adehabitatHR)\nlibrary(sp)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n# read in sf object \nbou &lt;- read_sf(\"clean_data/caribou.gpkg\") \n\n# add seasons \nbou &lt;- bou |&gt; \n  mutate(season = case_when(\n            month %in% c(4,5) ~ \"spring\",\n            month %in% c(6,7,8) ~ \"summer\",\n            month %in% c(9,10,11) ~ \"fall\",\n            month %in% c(12,1,2,3) ~ \"winter\")) \n\n# add coordinates\ntdf &lt;- st_coordinates(bou) |&gt; \n    cbind(bou) |&gt;\n    dplyr::select(location.long, location.lat, herd, animal.id, season) |&gt;  \n    st_drop_geometry()\n\nLets begin by checking we have enough points to be confident in our home range estimate. The total number of raw fixes will vary depending between individuals by season and years. In this example we will use a minimum of 50 unique locations as recommended for home range calculations (Seaman 1999, Kernohan 2001).\nNote: telemetry data is yet to be subset by a specific time interval and currently includes all raw fixes.\n\nid_pts &lt;- tdf |&gt;\n      group_by(animal.id)|&gt;\n      summarise(count = n())\n\nseason_pts &lt;- tdf |&gt;\n      group_by(herd, season)|&gt;\n      summarise(count = n())\n\n\nMinimum convex polygons (mcp) can be used to provide a quick estimate of data distribution. This provides a very broad brush approach and does not include density estimates.\n2.1 Using the sf package.\n\nmcp &lt;- st_convex_hull(st_union(bou))\nplot(mcp)\nplot(bou$geom, col = \"blue\", add = T)\n\n\n\n\nThis is not very representative so what other methods can we use?\n2.2 Using the sp package.\nAlternatively we can use the adhabitatHR package, which contains many ecology specific tools and functions. The downside of using this package is the reliance on old spatial package (sp) . The sp package has largely been superseded by the sf package. Unfortunately this is the reality of a coding language which is constantly being updated and improved!\nThe sp package uses a specific format called SpatialPointsDataFrame. To use these functions we need to convert from sf to sp formats.\n\n#  Create SpatialPointsDataFrame object\n  tdfgeo &lt;- bou |&gt;  \n    dplyr::select(animal.id) |&gt;\n    as(\"Spatial\")\n\n  # Calculate MCPs \n  c.mcp &lt;- mcp(tdfgeo, percent = 100)\n  \n  # convert this to a sf object \n  c.mcp_sf &lt;- st_as_sf(c.mcp)\n  \n  # plot with mapview\n  mapview::mapview(c.mcp_sf)\n\n\n\n\n\n\nThe advantage of using the adhabitatHR package is that it provides many more outputs and enables us to customize the information we want. For example we can extract multiple home ranges based on varying levels of specificity; i.e. percent = 75%.\n\n  # lets look at 75% inclusion\n  c.mcp.75 &lt;- mcp(tdfgeo, percent = 75)\n\n   # Plot\n  plot(tdfgeo, col = as.factor(tdfgeo$animal.id), pch = 16)\n  plot(c.mcp , col = alpha(1:5, 0.5), add = TRUE)\n  plot(c.mcp.75, col = alpha(1:5, 0.5), add = TRUE)\n\n\n\n  # Calculate the MCP by including 50 to 100 percent of points\n  hrarea &lt;- mcp.area(tdfgeo, percent = seq(50, 100, by = 10))\n\n\n\n\n\nKernel Density Estimates (kde) are a popular method to estimate the distribution of data, hence are used to estimate home ranges. Calculations for KDE require consideration of the mathematical method to describe the density function. A detailed description of the parameters can be found here.\nKDEs are very sensitive to input parameters, specifically the bandwidth (h) which determines the smoothing parameter. H parameters can be estimated using three methods:\n\na reference bandwidth (h = σ × n ^−1/6), however this is generally an overestimate of the range, and is not suitable for multi-modal distributions.\nLeast Square Cross Validation (LSCV), which minimizes the difference in volume between the true UD and the estimates UD.\nA subjective visual choice for the smoothing parameter, based on successive trials (Silverman, 1986; Wand & Jones 1995).\n\nOther parameters include:\n\nKernel Type: The type of kernel is limited to Gaussian (bivariate normal), quadratic or normal. We used the bivariate normal model as default.\nGrid/extent: These determines the area or extent over which the home range will be estimated. This is a mix of fine scale and time consuming processing and faster blocky resolution over a continuous surface. As a rule of thumb, Geospatial Modelling Environment program (GME) formally Hawth’s tools suggest: take the square root of the x or y variance value (whichever is smaller) and divide by 5 or 10 (I usually round to the nearest big number - so 36.7 becomes 40). Before using this rule of thumb value calculate how many cells this will result in for the output (take the width and height of you input points, divide by the cell size, and multiply the resulting numbers together). If you get a value somewhere between 1-20 million, then you have a reasonable value.\n\nTo create home ranges for each unique id we used a bivariate normal kernel with a variety of h (smoothing parameters). We then interpreted visually to determine size to use based on successive trials. This is supported by the literature (Hemson et al. 2005; Calenge et al. 2011).\nIt is also possible to run KDE with set barriers and boundaries.\n\n\ntdfgeo &lt;- bou |&gt;  \n  dplyr::select(animal.id) |&gt;\n  as(\"Spatial\")\n\n# define the parameters (h, kern, grid, extent) \nkde_href  &lt;- kernelUD(tdfgeo, h = \"href\", kern = c(\"bivnorm\"), grid = 500, extent = 2)\n\nkde_href\n\n********** Utilization distribution of several Animals ************\n\nType: probability density\nSmoothing parameter estimated with a  href smoothing parameter\nThis object is a list with one component per animal.\nEach component is an object of class estUD\nSee estUD-class for more information\n\n\nFrom this object (Utilization distribution of several Animals) we can extract the vertices or polygons which define the percentage we wish to include.\n\nver95 &lt;- getverticeshr(kde_href,95) # get vertices for home range\nver95_sf &lt;- st_as_sf(ver95)         # convert to sf object \n  \nver75 &lt;- getverticeshr(kde_href,75)\nver75_sf &lt;- st_as_sf(ver75 )\n  \nver50 &lt;- getverticeshr(kde_href,50)\nver50_sf&lt;- st_as_sf(ver50)\n \n# plot the outputs \nmapview(ver50_sf, zcol = \"id\") \n\n\n\n\n\nmapview (ver75_sf, zcol = \"id\") \n\n\n\n\n\nmapview (ver95_sf, zcol = \"id\")\n\n\n\n\n\nplot(st_geometry(ver95_sf),col = \"yellow\") \nplot(st_geometry(ver75_sf),col = \"blue\", add = TRUE)\nplot(st_geometry(ver50_sf),col = \"purple\", add = TRUE)\nplot(tdfgeo, pch = 1, size = 0.5, add = TRUE)     # Add points \n\n\n\n\n\n\nkde_lscv  &lt;- kernelUD(tdfgeo, h = \"LSCV\", kern = c(\"bivnorm\"), grid = 500, extent = 2)\n\nver95ls &lt;- getverticeshr(kde_lscv,95) # get vertices for home range\nver95ls_sf &lt;- st_as_sf(ver95ls) \n\nver50ls &lt;- getverticeshr(kde_lscv,50)\nver50ls_sf &lt;- st_as_sf(ver50ls) \n\n# plot the outputs \nmapview(ver50ls_sf, zcol = \"id\") \n\n\n\n\n\nmapview (ver95ls_sf, zcol = \"id\")\n\n\n\n\n\n\n\nTo test the sensitivity of the h value we can test a bivariate normal kernel with a variety of smoothing parameters (h = 1000, 2000, 4000).\nWe can then interpreted visually to determine size to use based on successive trials. This approach is supported by literature (Hemson et al. 2005; Calenge et al. 2011).\n\nkde_h1000  &lt;- kernelUD(tdfgeo, h = 1000, kern = c(\"bivnorm\"), grid = 500,extent = 2)\nkde_h500  &lt;- kernelUD(tdfgeo, h = 500, kern = c(\"bivnorm\"), grid = 500,extent = 2)\nkde_h3000  &lt;- kernelUD(tdfgeo, h = 3000, kern = c(\"bivnorm\"), grid = 500,extent = 2)\n\nLets extract the vertices and compare the outputs by building a plot.\n\n# kde - href = 1000\nver95_1000 &lt;- getverticeshr(kde_h1000, 95) # get vertices for home range\nver95_1000_sf &lt;- st_as_sf(ver95_1000) |&gt;     mutate(h = 1000) # convert to sf object \n\n# kde - href = 500\nver95_500 &lt;- getverticeshr(kde_h500, 95) # get vertices for home range\nver95_500_sf &lt;- st_as_sf(ver95_500)  |&gt; \n  mutate(h = 500) # convert to sf object \n  \n# kde - href = 3000\nver95_3000 &lt;- getverticeshr(kde_h3000, 95) # get vertices for home range\nver95_3000_sf &lt;- st_as_sf(ver95_3000)  |&gt; \n  mutate(h = 3000) # convert to sf object \n  \n# bind all data together \nall_verts &lt;- bind_rows(ver95_1000_sf,  ver95_500_sf,  ver95_3000_sf)\n  \n# lets plot the output \nggplot(data = all_verts) +\ngeom_sf(\n    aes(colour = id), \n    alpha = 0.1\n  ) + \n  scale_colour_viridis_d() + \n  facet_wrap(vars(h)) +\n  theme_bw()"
  },
  {
    "objectID": "bonus-kde.html#comparison-of-kde-parameters",
    "href": "bonus-kde.html#comparison-of-kde-parameters",
    "title": "Generating Home Range Estimates",
    "section": "Comparison of kde parameters",
    "text": "Comparison of kde parameters\nLets rerun the home range by season and compare the methods for the KDE.\n\ntdfseason &lt;- bou |&gt;  \n  dplyr::filter(herd == \"Burnt Pine\") |&gt;\n  dplyr::select(season) |&gt;\n  as(\"Spatial\")\n\n# href \nkde_href  &lt;- kernelUD(tdfseason, h = \"href\", kern = c(\"bivnorm\"), grid = 500, extent = 2)\n\n# custome h values \nkde_h1000  &lt;- kernelUD(tdfseason, h = 1000, kern = c(\"bivnorm\"), grid = 500,extent = 2)\n\n# lscv \nkde_lscv  &lt;- kernelUD(tdfseason, h = \"LSCV\", kern = c(\"bivnorm\"), grid = 500, extent = 2)\n \n\n# build function to get vertices \n\nget_verts &lt;- function(in_kde, percent = 95, fieldname){\n  \n  ver &lt;- getverticeshr(in_kde, percent) # get vertices for home range\n  ver_sf &lt;- st_as_sf(ver)  |&gt; \n    mutate(type = fieldname) \n  \n  return(ver_sf)\n}\n\n\n# genertate vertices for each KDE \n\nhref &lt;- get_verts(kde_href, percent = 95, fieldname = \"href\")\nh1000 &lt;- get_verts(kde_h1000, percent = 95, fieldname = \"h1000\")\nkde_lscv &lt;- get_verts(kde_lscv, percent = 95, fieldname = \"lscv\")\n\n# bind all data together \nall_seasons &lt;- bind_rows(href, h1000, kde_lscv)\n  \n# lets plot the output \nggplot(data = all_seasons) +\ngeom_sf(\n    aes(colour = id), \n    alpha = 0.1\n  ) + \n  scale_colour_viridis_d() + \n  coord_sf() +\n  facet_wrap(vars(type)) +\n  theme_bw()\n\n\n\n\nFrom this example we can see the large difference in home range estimates, depending on the type of method used and parameters selected.\nWhen generating home range estimates it is important to think about the application of use, and test various parameters to estimate the best fit.\nReferences\n\nPackages: Estimate Kernel home range Utilization Distribution Using adehabitatHR (Calenge et al. 2011)\n\nKDE\nhttps://www.ckwri.tamuk.edu/sites/default/files/publication/pdfs/2017/leonard_analyzing_wildlife_telemetry_data_in_r.pdf\nSeaman, D. E., Millspaugh, J. J., Kernohan, B. J., Brundige, G. C., Raedeke, K. J., & Gitzen, R. A. (1999). Effects of sample #size on kernel home range estimates. The journal of wildlife management, 739-747.\nKernohan, B. J., R. A. Gitzen, and J. J. Millspaugh. 2001. Analysis of animal space use and movements. Pages 125–166 in J. J. #Millspaugh and J. M. Marzluff, editors. Radio tracking and animal populations. Academic Press, San Diego, CA, USA\nHemson, G., Johnson, P., South, A., Kenward, R., Ripley, R., & MACDONALD, D. (2005). Are kernels the mustard? Data from global positioning system (GPS) collars suggests problems for kernel home‐range analyses with least‐squares cross‐validation. Journal of Animal Ecology, 74(3), 455-463"
  },
  {
    "objectID": "05-vector-bcdata.html",
    "href": "05-vector-bcdata.html",
    "title": "Getting B.C. Open Data with R",
    "section": "",
    "text": "Use an area of interest (AOI) to define study area\nExplore the B.C. Data Catalogue\nUse the bcdata package to query and download data"
  },
  {
    "objectID": "05-vector-bcdata.html#outline",
    "href": "05-vector-bcdata.html#outline",
    "title": "Getting B.C. Open Data with R",
    "section": "",
    "text": "Use an area of interest (AOI) to define study area\nExplore the B.C. Data Catalogue\nUse the bcdata package to query and download data"
  },
  {
    "objectID": "05-vector-bcdata.html#b.c.-data-catalogue",
    "href": "05-vector-bcdata.html#b.c.-data-catalogue",
    "title": "Getting B.C. Open Data with R",
    "section": "B.C. Data Catalogue",
    "text": "B.C. Data Catalogue\nThe B.C. Government makes a huge amount of open data available, both spatial and non-spatial, and documents it in the B.C. Data Catalogue.\nThe bcdata package allows you to interact with the catalogue, and download data, directly from within R.\n\n\n\n\nbcdc_browse()\n\nOpen the catalogue in your default browser\n\n\n\nbcdc_search()\n\nSearch records in the catalogue\n\n\n\nbcdc_get_record()\n\nPrint a catalogue record\n\n\n\nbcdc_get_data()\n\nGet catalogue data\n\n\n\nbcdc_describe_feature()\n\nDescribe the structure (columns and types) of a data set\n\n\n\nbcdc_query_geodata()\n\nGet & query catalogue geospatial data available through a Web Service ]\n\n\n\n\n\n\n\nWe are going to use bcdata to get some data for our telemetry analysis:\n\nBEC\nVRI\nForest Cutblocks\nLakes and wetlands\nStreams\nRoads\n\n\nlibrary(bcdata)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(mapview)"
  },
  {
    "objectID": "05-vector-bcdata.html#create-an-area-of-interest-aoi",
    "href": "05-vector-bcdata.html#create-an-area-of-interest-aoi",
    "title": "Getting B.C. Open Data with R",
    "section": "Create an Area of Interest (AOI)",
    "text": "Create an Area of Interest (AOI)\nWe’ll read in the caribou data created in the previous module, and create an area of interest (a bounding box) around it, so we can use that to spatially subset the covariate data we will be using.\n\nscott &lt;- read_sf(\"clean_data/scott_herd_subset.gpkg\")\n\nscott_bbox &lt;- st_bbox(scott)\n\n## Round to nearest 100m outside the box to align with raster grids\nscott_bbox[\"xmin\"] &lt;- floor(scott_bbox[\"xmin\"] / 100) * 100\nscott_bbox[\"ymin\"] &lt;- floor(scott_bbox[\"ymin\"] / 100) * 100\nscott_bbox[\"xmax\"] &lt;- ceiling(scott_bbox[\"xmax\"] / 100) * 100\nscott_bbox[\"ymax\"] &lt;- ceiling(scott_bbox[\"ymax\"] / 100) * 100\n\nscott_aoi &lt;- st_as_sfc(scott_bbox)\n\n\nwrite_sf(scott_aoi, file.path(\"clean_data\", \"scott_aoi.gpkg\"))\n\nFirst, let’s open the B.C. Data Catalogue in our browser:\n\nbcdc_browse()\n\nIf you search for “BEC”, one of the first hits should be the BEC Map record. If you click on the “Share” (  ) button you will get a url: https://catalogue.data.gov.bc.ca/dataset/f358a53b-ffde-4830-a325-a5a03ff672c3. The last bit of the url (f358a53b-ffde-4830-a325-a5a03ff672c3) is the unique identifier for the record, and we can use that ID to query the dataset with bcdata.\nLet’s find out about the record:\n\nbcdc_get_record(\"f358a53b-ffde-4830-a325-a5a03ff672c3\")\n\nB.C. Data Catalogue Record: BEC Map\nName: bec-map (ID: f358a53b-ffde-4830-a325-a5a03ff672c3)\nPermalink:\n https://catalogue.data.gov.bc.ca/dataset/f358a53b-ffde-4830-a325-a5a03ff672c3\nLicence: Open Government Licence - British Columbia\nDescription: The current and most detailed version of the approved\n corporate provincial digital Biogeoclimatic Ecosystem Classification\n (BEC) Zone/Subzone/Variant/Phase map (version 12, September 2, 2021).\n Use this version when performing GIS analysis regardless of scale.\n This mapping is deliberately extended across the ocean, lakes,\n glaciers, etc to facilitate intersection with a terrestrial landcover\n layer of your choice\nAvailable Resources (1):\n 1. WMS getCapabilities request (wms)\nAccess the full 'Resources' data frame using:\n bcdc_tidy_resources('f358a53b-ffde-4830-a325-a5a03ff672c3')\nQuery and filter this data using:\n bcdc_query_geodata('f358a53b-ffde-4830-a325-a5a03ff672c3')"
  },
  {
    "objectID": "05-vector-bcdata.html#bec",
    "href": "05-vector-bcdata.html#bec",
    "title": "Getting B.C. Open Data with R",
    "section": "BEC",
    "text": "BEC\nbcdc_query_geodata() by itself does not download the data - it retrieves the first few records and shows them to us, as well as some helpful information about the data:\n\nbcdc_query_geodata(\"f358a53b-ffde-4830-a325-a5a03ff672c3\")\n\nQuerying 'bec-map' record\n• Using collect() on this object will return 15666 features and 20 fields\n• Accessing this record requires pagination and will make 16 separate\n• requests to the WFS. See ?bcdc_options\n• At most six rows of the record are printed here\n────────────────────────────────────────────────────────────────────────────────\nSimple feature collection with 6 features and 20 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 575312.5 ymin: 1553588 xmax: 780612.5 ymax: 1668388\nProjected CRS: NAD83 / BC Albers\n# A tibble: 6 × 21\n  id          FEATURE_CLASS_SKEY ZONE  SUBZONE VARIANT PHASE NATURAL_DISTURBANCE\n  &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;              \n1 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n2 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n3 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n4 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n5 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n6 WHSE_FORES…                435 SWB   uns     &lt;NA&gt;    &lt;NA&gt;  NDT2               \n# ℹ 14 more variables: MAP_LABEL &lt;chr&gt;, BGC_LABEL &lt;chr&gt;, ZONE_NAME &lt;chr&gt;,\n#   SUBZONE_NAME &lt;chr&gt;, VARIANT_NAME &lt;chr&gt;, PHASE_NAME &lt;chr&gt;,\n#   NATURAL_DISTURBANCE_NAME &lt;chr&gt;, FEATURE_AREA_SQM &lt;int&gt;,\n#   FEATURE_LENGTH_M &lt;int&gt;, FEATURE_AREA &lt;int&gt;, FEATURE_LENGTH &lt;int&gt;,\n#   OBJECTID &lt;int&gt;, SE_ANNO_CAD_DATA &lt;chr&gt;, geometry &lt;POLYGON [m]&gt;\n\n\nYou can use dplyr verbs filter() and select() to cut down the amount of data you need to download from the web service. filter() can take logical predicates such as ==, &gt;, %in% etc., as well as geometry predicates such as INTERSECTS(), OVERLAPS(), WITHIN() etc. See this vignette for details.\nOnce your query is complete you call collect() to tell the server to execute the query and send you the data:\n\nbec &lt;- bcdc_query_geodata(\"f358a53b-ffde-4830-a325-a5a03ff672c3\") |&gt;\n    filter(INTERSECTS(scott_aoi)) |&gt; \n    select(MAP_LABEL) |&gt; \n    collect()\n\nmapview(bec, zcol = \"MAP_LABEL\") + mapview(st_as_sf(scott_aoi))\n\n\n\n\n\n\nYou can see that filtering using the INTERSECTS() function does a good job of only downloading the features that intersect the AOI, but it doesn’t actually clip them to the AOI. We can do that with sf::st_intersection(). Also, there are some columns that are “sticky” - even if you don’t select them in a select() statement before you call collect, they come along anyway. We can do a final select() once the data is downloaded:\n\nbec &lt;- st_intersection(bec, scott_aoi) |&gt; \n  select(MAP_LABEL)\n\nmapview(bec)\n\n\n\n\n\n\nAnd now we can write our BEC data to a file to use later in the analysis.\n\nwrite_sf(bec, file.path(\"clean_data\", \"bec.gpkg\"))"
  },
  {
    "objectID": "05-vector-bcdata.html#vri",
    "href": "05-vector-bcdata.html#vri",
    "title": "Getting B.C. Open Data with R",
    "section": "VRI",
    "text": "VRI\nSee a list of VRI codes.\n\nvri &lt;- bcdc_query_geodata(\"2ebb35d8-c82f-4a17-9c96-612ac3532d55\") |&gt; \n  filter(INTERSECTS(scott_aoi)) |&gt; \n  select(PROJ_AGE_CLASS_CD_1, BCLCS_LEVEL_4, CROWN_CLOSURE_CLASS_CD) |&gt;  \n  collect() |&gt; \n  st_intersection(scott_aoi)\n\nWe want to split this into separate files for different variables:\n\nConiferous-leading stands\nStands with age greater than 40 yrs (age class &gt;=3)\nCrown closure\n\n\n# Tree coniferous leading - select coniferous leading vri plots\nvri_conif &lt;- vri |&gt;  \n    mutate(conif = BCLCS_LEVEL_4) |&gt; \n    filter(conif == \"TC\") |&gt; \n    select(conif)\n\n\nwrite_sf(vri_conif, file.path(\"clean_data\",\"vri_conif.gpkg\"))\n\n\n# Age class greater than 40 years\nvri_ageclass &lt;- vri |&gt; \n    mutate(age_class = as.numeric(PROJ_AGE_CLASS_CD_1)) |&gt; \n    filter(age_class &gt;= 3) |&gt; \n    select(age_class)\n\n\nwrite_sf(vri_ageclass, file.path(\"clean_data\", \"vri_ageclass.gpkg\"))\n\n\n# Crown closure class \nvri_cc &lt;- vri |&gt; \n    mutate(cc_class = as.numeric(CROWN_CLOSURE_CLASS_CD)) |&gt; \n    select(cc_class)\n\n\nwrite_sf(vri_cc, file.path(\"clean_data\", \"vri_cc.gpkg\"))"
  },
  {
    "objectID": "05-vector-bcdata.html#cutblocks",
    "href": "05-vector-bcdata.html#cutblocks",
    "title": "Getting B.C. Open Data with R",
    "section": "Cutblocks",
    "text": "Cutblocks\nTo get the cutblocks, we filter to our AOI, and also choose those blocks that have a harvest year in the last 30 years.\nWe can get information about the columns in a given data set with bcdc_describe_feature():\n\nbcdc_describe_feature(\"b1b647a6-f271-42e0-9cd0-89ec24bce9f7\")\n\n# A tibble: 13 × 5\n   col_name                sticky remote_col_type local_col_type column_comments\n   &lt;chr&gt;                   &lt;lgl&gt;  &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;          \n 1 id                      TRUE   xsd:string      character      &lt;NA&gt;           \n 2 VEG_CONSOLIDATED_CUT_B… TRUE   xsd:decimal     numeric        VEG_CONSOLIDAT…\n 3 OPENING_ID              FALSE  xsd:decimal     numeric        OPENING_ID is …\n 4 HARVEST_YEAR            FALSE  xsd:decimal     numeric        HARVEST_YEAR i…\n 5 DISTURBANCE_START_DATE  FALSE  xsd:date        date           DISTURBANCE_ST…\n 6 DISTURBANCE_END_DATE    FALSE  xsd:date        date           DISTURBANCE_EN…\n 7 DATA_SOURCE             FALSE  xsd:string      character      DATA_SOURCE is…\n 8 AREA_HA                 FALSE  xsd:decimal     numeric        AREA_HA is the…\n 9 FEATURE_AREA_SQM        FALSE  xsd:decimal     numeric        FEATURE_AREA_S…\n10 FEATURE_LENGTH_M        FALSE  xsd:decimal     numeric        FEATURE_LENGTH…\n11 SHAPE                   FALSE  gml:GeometryPr… sfc geometry   SHAPE is the c…\n12 OBJECTID                TRUE   xsd:decimal     numeric        OBJECTID is a …\n13 SE_ANNO_CAD_DATA        FALSE  xsd:hexBinary   numeric        SE_ANNO_CAD_DA…\n\ncutblocks &lt;- bcdc_query_geodata(\"b1b647a6-f271-42e0-9cd0-89ec24bce9f7\") |&gt;\n  filter(\n    INTERSECTS(scott_aoi), \n    HARVEST_YEAR &gt;= 1993) |&gt;\n  select(HARVEST_YEAR) |&gt;\n  collect() |&gt; \n  st_intersection(scott_aoi)\n\nmapview(cutblocks, zcol = \"HARVEST_YEAR\")\n\n\n\n\n\n\n\nwrite_sf(cutblocks, file.path(\"clean_data\", \"cutblocks.gpkg\"))"
  },
  {
    "objectID": "05-vector-bcdata.html#water-bodies",
    "href": "05-vector-bcdata.html#water-bodies",
    "title": "Getting B.C. Open Data with R",
    "section": "Water bodies",
    "text": "Water bodies\nSearching the catalogue for the BC Freshwater Atlas\nWe can search the catalogue for data using keywords, with bcdc_search(). Control the number of results returned with n.\n\nbcdc_search(\"freshwater atlas\", n = 20)\n\nList of B.C. Data Catalogue Records\nNumber of records: 20\nTitles:\n1: Freshwater Atlas Glaciers (multiple, html, pdf, wms, kml)\n ID: 8f2aee65-9f4c-4f72-b54c-0937dbf3e6f7\n Name: freshwater-atlas-glaciers\n2: Freshwater Atlas Watersheds (multiple, html, pdf, wms, kml, fgdb)\n ID: 3ee497c4-57d7-47f8-b030-2e0c03f8462a\n Name: freshwater-atlas-watersheds\n3: Freshwater Atlas Obstructions (multiple, html, pdf, wms, kml)\n ID: 64797286-3ca5-4202-9064-a7f790321e9e\n Name: freshwater-atlas-obstructions\n4: Freshwater Atlas Rivers (multiple, fgdb, pdf, wms, kml)\n ID: f7dac054-efbf-402f-ab62-6fc4b32a619e\n Name: freshwater-atlas-rivers\n5: Freshwater Atlas Lakes (multiple, html, pdf, wms, kml)\n ID: cb1e3aba-d3fe-4de1-a2d4-b8b6650fb1f6\n Name: freshwater-atlas-lakes\n6: Freshwater Atlas Coastlines (multiple, html, pdf, wms, kml)\n ID: 87b1d6a7-d4d1-4c25-a879-233becdbffed\n Name: freshwater-atlas-coastlines\n7: Freshwater Atlas Wetlands (multiple, html, pdf, wms, kml)\n ID: 93b413d8-1840-4770-9629-641d74bd1cc6\n Name: freshwater-atlas-wetlands\n8: Freshwater Atlas Islands (multiple, html, pdf, wms, kml)\n ID: 4483aeea-df26-4b83-a565-934c769e74de\n Name: freshwater-atlas-islands\n9: Freshwater Atlas Watershed Boundaries (multiple, html, pdf, wms,\n kml, fgdb)\n ID: ab758580-809d-4e11-bb2c-df02ac5465c9\n Name: freshwater-atlas-watershed-boundaries\n10: Freshwater Atlas Manmade Waterbodies (multiple, html, pdf, wms,\n kml)\n ID: 055fd71e-b771-4d47-a863-8a54f91a954c\n Name: freshwater-atlas-manmade-waterbodies\n11: Freshwater Atlas Linear Boundaries (multiple, html, pdf, wms, kml,\n fgdb)\n ID: 2af1388e-d5f7-46dc-a6e2-f85415ddbd1c\n Name: freshwater-atlas-linear-boundaries\n12: Freshwater Atlas Watershed Groups (multiple, html, pdf, wms, kml)\n ID: 51f20b1a-ab75-42de-809d-bf415a0f9c62\n Name: freshwater-atlas-watershed-groups\n13: Freshwater Atlas Named Watersheds (multiple, html, pdf, wms, kml)\n ID: ea63ea04-eab0-4b83-8729-f8a93ac688a1\n Name: freshwater-atlas-named-watersheds\n14: Freshwater Atlas Assessment Watersheds (multiple, html, pdf, wms,\n kml)\n ID: 97d8ef37-b8d2-4c3b-b772-6b25c1db13d0\n Name: freshwater-atlas-assessment-watersheds\n15: Freshwater Atlas Stream Directions (multiple, html, pdf, wms, kml)\n ID: d7165359-52ef-41d0-b762-c53e3468ff3f\n Name: freshwater-atlas-stream-directions\n16: Freshwater Atlas Stream Network (multiple, html, pdf, wms, kml,\n fgdb)\n ID: 92344413-8035-4c08-b996-65a9b3f62fca\n Name: freshwater-atlas-stream-network\n17: Freshwater Atlas Edge Type Codes (multiple, html, pdf)\n ID: 509cbf74-7ee7-44d3-a88d-4e088ea67325\n Name: freshwater-atlas-edge-type-codes\n18: Freshwater Atlas Waterbody Type Codes (multiple, html, pdf)\n ID: ade4f36a-1fd4-4583-8253-2b2a1bbe34ff\n Name: freshwater-atlas-waterbody-type-codes\n19: Freshwater Atlas Watershed Type Codes (multiple, html, pdf)\n ID: f7efa3ea-bf1c-4c4f-bb33-ba841aa076c0\n Name: freshwater-atlas-watershed-type-codes\n20: Freshwater Atlas Named Point Features (multiple, html, pdf, wms,\n kml)\n ID: db43a358-273c-4c2e-8a5c-cc28eaaffaa7\n Name: freshwater-atlas-named-point-features\n\nAccess a single record by calling `bcdc_get_record(ID)` with the ID\n from the desired record.\n\n\n\nlakes &lt;- bcdc_query_geodata(\"cb1e3aba-d3fe-4de1-a2d4-b8b6650fb1f6\") |&gt;\n  filter(INTERSECTS(scott_aoi)) |&gt;\n  select(id, WATERBODY_TYPE, AREA_HA) |&gt;\n  collect()\n\nwetlands &lt;- bcdc_query_geodata(\"93b413d8-1840-4770-9629-641d74bd1cc6\") |&gt;\n  filter(INTERSECTS(scott_aoi)) |&gt;\n  select(id, WATERBODY_TYPE, AREA_HA) |&gt;\n  collect()\n\n# Combine the data sets into one, select only the columns we want, and \n# clip to aoi\nwater &lt;- bind_rows(lakes, wetlands) |&gt; \n  select(id, WATERBODY_TYPE, AREA_HA) |&gt; \n  st_intersection(scott_aoi)\n\n\nwrite_sf(water, file.path(\"clean_data\", \"water.gpkg\"))"
  },
  {
    "objectID": "05-vector-bcdata.html#stream-index",
    "href": "05-vector-bcdata.html#stream-index",
    "title": "Getting B.C. Open Data with R",
    "section": "Stream Index",
    "text": "Stream Index\n\nstreams_cols &lt;- bcdc_describe_feature(\"92344413-8035-4c08-b996-65a9b3f62fca\")\nprint(streams_cols, n = 20)\n\n# A tibble: 28 × 5\n   col_name                sticky remote_col_type local_col_type column_comments\n   &lt;chr&gt;                   &lt;lgl&gt;  &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;          \n 1 id                      TRUE   xsd:string      character       &lt;NA&gt;          \n 2 LINEAR_FEATURE_ID       TRUE   xsd:decimal     numeric        \"A unique nume…\n 3 WATERSHED_GROUP_ID      TRUE   xsd:decimal     numeric        \"An automatica…\n 4 EDGE_TYPE               TRUE   xsd:decimal     numeric        \"A 4 digit num…\n 5 BLUE_LINE_KEY           FALSE  xsd:decimal     numeric        \"Uniquely iden…\n 6 WATERSHED_KEY           FALSE  xsd:decimal     numeric        \"A key that id…\n 7 FWA_WATERSHED_CODE      FALSE  xsd:string      character      \"A 143 charact…\n 8 LOCAL_WATERSHED_CODE    FALSE  xsd:string      character      \"The 143 chara…\n 9 WATERSHED_GROUP_CODE    TRUE   xsd:string      character      \"The watershed…\n10 DOWNSTREAM_ROUTE_MEASU… FALSE  xsd:decimal     numeric        \"The distance,…\n11 LENGTH_METRE            TRUE   xsd:decimal     numeric        \"The length in…\n12 FEATURE_SOURCE          TRUE   xsd:string      character      \"The source of…\n13 GNIS_ID                 FALSE  xsd:decimal     numeric        \"The BCGNIS  (…\n14 GNIS_NAME               FALSE  xsd:string      character      \"The BCGNIS  (…\n15 LEFT_RIGHT_TRIBUTARY    FALSE  xsd:string      character      \"Describes whi…\n16 STREAM_ORDER            TRUE   xsd:decimal     numeric        \"The calculate…\n17 STREAM_MAGNITUDE        TRUE   xsd:decimal     numeric        \"The calculate…\n18 WATERBODY_KEY           FALSE  xsd:decimal     numeric        \"The waterbody…\n19 BLUE_LINE_KEY_50K       FALSE  xsd:decimal     numeric        \"The best matc…\n20 WATERSHED_CODE_50K      FALSE  xsd:string      character      \"The hierarchi…\n# ℹ 8 more rows\n\n\nLet’s use the STREAM_ORDER column to just get streams of order 3 and 4 and still intersect with our AOI.\n\nstreams &lt;- bcdc_query_geodata(\"92344413-8035-4c08-b996-65a9b3f62fca\") |&gt;\n  filter(\n    INTERSECTS(scott_aoi), \n    STREAM_ORDER &gt;= 3\n  ) |&gt;\n  select(id, STREAM_ORDER) |&gt;\n  collect() |&gt; \n  select(id, STREAM_ORDER) |&gt;\n  st_zm() |&gt; \n  st_intersection(scott_aoi)\n\nmapview(streams)\n\n\n\n\n\n\n\nwrite_sf(streams, file.path(\"clean_data\", \"streams.gpkg\"))"
  },
  {
    "objectID": "05-vector-bcdata.html#roads",
    "href": "05-vector-bcdata.html#roads",
    "title": "Getting B.C. Open Data with R",
    "section": "Roads",
    "text": "Roads\n\nroads &lt;- bcdc_query_geodata(\"bb060417-b6e6-4548-b837-f9060d94743e\") |&gt; \n  filter(INTERSECTS(scott_aoi))  |&gt; \n  select(id, ROAD_CLASS, ROAD_SURFACE) |&gt; \n  collect() |&gt; \n  select(ROAD_SURFACE, ROAD_CLASS) |&gt; \n  st_intersection(scott_aoi) |&gt; # clip roads so all inside aoi\n  st_cast(\"MULTILINESTRING\")\n\n\nwrite_sf(roads, file.path(\"clean_data\", \"roads.gpkg\"))"
  },
  {
    "objectID": "07-processing-vector-data.html",
    "href": "07-processing-vector-data.html",
    "title": "Processing Base Vector Data",
    "section": "",
    "text": "In this module we will post-process vector data we prepared earlier using the terra package. This includes;\n\ngenerate a road impact raster\ncalculate a distance to water raster\nconvert vectors to rasters\ncombine all rasters into a raster stack and export"
  },
  {
    "objectID": "07-processing-vector-data.html#overview",
    "href": "07-processing-vector-data.html#overview",
    "title": "Processing Base Vector Data",
    "section": "",
    "text": "In this module we will post-process vector data we prepared earlier using the terra package. This includes;\n\ngenerate a road impact raster\ncalculate a distance to water raster\nconvert vectors to rasters\ncombine all rasters into a raster stack and export"
  },
  {
    "objectID": "07-processing-vector-data.html#road-influence",
    "href": "07-processing-vector-data.html#road-influence",
    "title": "Processing Base Vector Data",
    "section": "1. Road influence",
    "text": "1. Road influence\nTo assemble meaningful information for modelling, we often need to post-process vector data. This could include manipulating the data into categories (i.e. high, medium, low) or exploring spatial patterns in more detail (i.e. distance from a road)\nIn this example we will generate a raster to quantifying the impact of road type and spatial orientation.\n\n# load in the libraries needed\nlibrary(bcdata)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(terra)\n\n# read in raster template\ntemplate &lt;- rast(\"clean_data/template.tif\")\n\n# read in roads\nroads &lt;- read_sf(\"clean_data/roads.gpkg\")\n\nWe can assume the impact of road type (ROAD_SURFACE) might have a different impact on Caribou avoidance or predator use.\nWe can use this column to assign values to reflect this. In this example we assign a higher value to roads that are likely to be traveled at a higher speed and would have more frequent use.\n\n# assign a value to the roads based on estimated speed of travel or use\nroads &lt;- roads |&gt;\n  mutate(rd_value = case_when(\n            ROAD_SURFACE == \"loose\" ~ 25,\n            ROAD_SURFACE == \"overgrown\" ~ 5,\n            ROAD_SURFACE == \"rough\" ~ 10,\n            ROAD_SURFACE == \"unknown\" ~ 7.5)) \n\nNow we can convert the vector to a raster using the template\n\n# convert roads to a raster \nrroads &lt;- rasterize(roads, template, field = \"rd_value\" )\n\nplot(rroads)\n\n\n\n\nSecondly, we can assume the impact of the road will have a wider influence than a single pixel. To capture this influence we use a moving window analysis.\nMoving windows or focal calculations estimate a new value for each pixel based on the values of the surrounding pixels (neighbours) and the function we supply. In this example we will use a window of 9, and a calculation based on the sum of these 9 pixels.\n\n# create a moving window \nrrdens &lt;- focal(rroads, w=9, fun=\"sum\", na.rm=TRUE)\n\nplot(rrdens)\n\n\n\n\n\nwriteRaster(rrdens, \"clean_data/road_density.tif\", overwrite = TRUE)"
  },
  {
    "objectID": "07-processing-vector-data.html#calculate-distance-from-water",
    "href": "07-processing-vector-data.html#calculate-distance-from-water",
    "title": "Processing Base Vector Data",
    "section": "2. Calculate distance from water",
    "text": "2. Calculate distance from water\nWe might also be interested in understanding the relationship between Caribou locations and distance to a particular feature or habitat type.\nIn this case we will explore the distance to water bodies within the study areas.\n\n# read in water \nwater &lt;- read_sf(\"clean_data/water.gpkg\")\n\n# calculate the distance to water for each pixel in the raster\n# be patient - this might take some time. \nwater_dis &lt;- distance(template, water, unit = \"km\")\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\nplot(water_dis)\nplot(water$geom, add = T)\n\n\n\n# what sort of values are we seeing?\nrange(water_dis)\n\nclass       : SpatRaster \ndimensions  : 1484, 1596, 2  (nrow, ncol, nlyr)\nresolution  : 25, 25  (x, y)\nextent      : 1145700, 1185600, 1190400, 1227500  (xmin, xmax, ymin, ymax)\ncoord. ref. : NAD83 / BC Albers (EPSG:3005) \nsource(s)   : memory\nnames       : range_min, range_max \nmin values  :  0.000000,  0.000000 \nmax values  :  6.090321,  6.090321"
  },
  {
    "objectID": "07-processing-vector-data.html#convert-base-vector-data-to-rasters",
    "href": "07-processing-vector-data.html#convert-base-vector-data-to-rasters",
    "title": "Processing Base Vector Data",
    "section": "3. Convert base vector data to rasters",
    "text": "3. Convert base vector data to rasters\nTo prepare rasters for modelling and further analysis, we can convert all our vector data into one standard raster stack.\nNote: it is not always required to convert to a raster stack, but this is helpful for any future predictions.\n\n# read in our prepared vector data sets\nbec &lt;- read_sf(\"clean_data/bec.gpkg\")\nvri_conif &lt;- read_sf(\"clean_data/vri_conif.gpkg\")\nvri_ageclass &lt;- read_sf(\"clean_data/vri_ageclass.gpkg\")\nvri_cc &lt;- read_sf(\"clean_data/vri_cc.gpkg\")\ncutblocks &lt;- read_sf(\"clean_data/cutblocks.gpkg\")\nwater &lt;- read_sf(\"clean_data/water.gpkg\")\nstreams &lt;- read_sf(\"clean_data/streams.gpkg\")\nroads &lt;- read_sf(\"clean_data/roads.gpkg\")\n\n# convert to rasters\nrbec &lt;- rasterize(bec, template, field = \"MAP_LABEL\")\nrvri_conif &lt;- rasterize(vri_conif, template, field = \"conif\")\nrvri_ageclass &lt;- rasterize(vri_ageclass, template, field = \"age_class\")\nrvri_cc &lt;- rasterize(vri_cc, template, field = \"cc_class\" )\nrcutblocks &lt;- rasterize(cutblocks, template, field = \"HARVEST_YEAR\")\nrwater &lt;- rasterize(water, template, field = \"WATERBODY_TYPE\" )\nrstreams &lt;- rasterize(streams, template, field = \"STREAM_ORDER\" )\nrroads &lt;- rasterize(roads, template, field = \"ROAD_SURFACE\" )\n\n# stack into a set of rasters\nvect_stack &lt;- c(rbec, rvri_conif, rvri_ageclass, rvri_cc,\n                rcutblocks, rwater, rstreams, rroads, \n                water_dis, rrdens)\n\nplot(vect_stack)\n\n\n\n\nLets combine these with the terrain rasters we created earlier.\n\nrslope &lt;- rast(\"clean_data/slope.tif\")\nraspect &lt;- rast(\"clean_data/aspect.tif\")\nrtri  &lt;- rast(\"clean_data/tri.tif\")\nrtrim_3005 &lt;- rast(\"clean_data/dem.tif\")\n\n# create a raster stack\nrast_stack &lt;- c(rtrim_3005, rslope, raspect, rtri)"
  },
  {
    "objectID": "07-processing-vector-data.html#combine-raster-layers-into-a-stack",
    "href": "07-processing-vector-data.html#combine-raster-layers-into-a-stack",
    "title": "Processing Base Vector Data",
    "section": "4. Combine raster layers into a stack",
    "text": "4. Combine raster layers into a stack\n\nall_stack &lt;- c(vect_stack, rast_stack)\n\nplot(all_stack)\n\n\n\n# we can write this out as a tif (raster object) \n#writeRaster(all_stack, \"clean_data/rstack.tif\", overwrite = T)\n\nOr we can write this out to a very small R object\n\nsaveRDS(all_stack, \"clean_data/covars.RDS\") # much faster"
  },
  {
    "objectID": "07-processing-vector-data.html#your-turn",
    "href": "07-processing-vector-data.html#your-turn",
    "title": "Processing Base Vector Data",
    "section": "Your turn",
    "text": "Your turn\n\nGenerate a road density later using a different function (i.e. not sum), and or different scale of window (w). Hint: use the ?focal to see other options available.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# One example would be to generate the \"max\" value of the moving window. Note the w parameter is also reduced in the example. \nrrdensmax &lt;- focal(rroads, w=3, fun=\"max\", na.rm=TRUE)\n\n\n\n\n\nSelect another linear feature (roads, steams) and generate a raster to capture the distance to that feature for the study area.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\n#This solution generates a distance from roads covariate\nroads &lt;- read_sf(\"clean_data/roads.gpkg\") # read in roads\n\nroad_dis &lt;- distance(template, roads, unit = \"km\") # calculate distance - be patient this might take time. \n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\nplot(road_dis)"
  },
  {
    "objectID": "01-intro-motivation.html#welcome",
    "href": "01-intro-motivation.html#welcome",
    "title": "Introduction and Motivation",
    "section": "Welcome",
    "text": "Welcome\n\n\nWho we are:\nGenevieve Perkins\n\nninoxconsulting.ca\n\nAndy Teucher\n\nandyteucher.ca\n\n\nWho are you?\n\nName, where you are from, and what you want to get out of the course"
  },
  {
    "objectID": "01-intro-motivation.html#structure-of-the-course",
    "href": "01-intro-motivation.html#structure-of-the-course",
    "title": "Introduction and Motivation",
    "section": "Structure of the course",
    "text": "Structure of the course\n\nDay 1 & 2: structured content, live coding\nDay 3 (half day): hackathon\n\nWrite your questions on the whiteboard\n\n\nTake-homes\n\nCourse website\nBonus materials\nSurvey"
  },
  {
    "objectID": "01-intro-motivation.html#agenda---day-1",
    "href": "01-intro-motivation.html#agenda---day-1",
    "title": "Introduction and Motivation",
    "section": "Agenda - Day 1",
    "text": "Agenda - Day 1\n\n\nSetup and troubleshooting (8:30 - 9:00)\nWorking with spatial data in R (9:00 - 10:30)\nBREAK (10:30 - 11:00)\nTelemetry data in R; clean, QA, and prepare data (Part I; 11:00 - 12:30)\nLUNCH (12:30 - 1:30)\nTelemetry data in R; clean, QA, and prepare data (Part II; 1:30 - 3:00)\nBREAK (3:00 - 3:30)\nVisualizing spatial data in R (3:30 - 4:30)"
  },
  {
    "objectID": "01-intro-motivation.html#agenda---day-2",
    "href": "01-intro-motivation.html#agenda---day-2",
    "title": "Introduction and Motivation",
    "section": "Agenda - Day 2",
    "text": "Agenda - Day 2\n\n\nSetup and troubleshooting (8:30 - 9:00)\nRetrieving spatial data from the B.C. Data Catalogue (9:00 - 10:30)\nBREAK (10:30 - 11:00)\nGetting and working with raster data (Part I; 11:00 - 12:30)\nLUNCH (12:30 - 1:30)\nPreparing base data for telemetry analysis (Part II; 1:30 - 3:00)\nBREAK (3:00 - 3:30)\nPreparing data for Resource Selection Function (RSF) analysis (3:30 - 4:30)"
  },
  {
    "objectID": "01-intro-motivation.html#code-of-conduct",
    "href": "01-intro-motivation.html#code-of-conduct",
    "title": "Introduction and Motivation",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nIn the interest of ensuring a safe environment for all, all attendees, speakers and organisers of the workshop must agree to follow the workshop’s code of conduct.   Be kind to yourself! Everyone has different background and experiences - stop us, ask questions!"
  },
  {
    "objectID": "01-intro-motivation.html#housekeeping",
    "href": "01-intro-motivation.html#housekeeping",
    "title": "Introduction and Motivation",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nWiFi:\n\nBCNGN\nDogwood\n\nWashrooms:\n\nDown the hall & turn left\n\nEtherpad:\n\nhttps://etherpad.andyteucher.ca/p/r-telemetry-pg"
  },
  {
    "objectID": "01-intro-motivation.html#motivation",
    "href": "01-intro-motivation.html#motivation",
    "title": "Introduction and Motivation",
    "section": "Motivation",
    "text": "Motivation\nThe Data Science Workflow"
  },
  {
    "objectID": "01-intro-motivation.html#motivation-1",
    "href": "01-intro-motivation.html#motivation-1",
    "title": "Introduction and Motivation",
    "section": "Motivation",
    "text": "Motivation\nSpatial Data Science with a GUI"
  },
  {
    "objectID": "01-intro-motivation.html#motivation-2",
    "href": "01-intro-motivation.html#motivation-2",
    "title": "Introduction and Motivation",
    "section": "Motivation",
    "text": "Motivation\nSpatial Data Science with R\n\n\n\nWorkshop Home"
  }
]